# -*- coding: utf-8 -*-
"""FLP&AP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qM1lJA2J2PIm6INePUHVGjkLaG72u5eU

# **Flight Price and Type Prediction using a Customized Cascaded Algorithm**

### **GPU**
"""

from IPython.display import HTML, clear_output
from subprocess import getoutput
s = getoutput('nvidia-smi')
if 'K80' in s:gpu = 'K80'
elif 'T4' in s:gpu = 'T4'
elif 'P100' in s:gpu = 'P100'
elif 'P4' in s:gpu = 'P4'
display(HTML(f"<h1>{gpu}</h1>"))

"""### **Libraries**"""

import pandas as pd # Library to process the dataframe
import numpy as np # Library to handle with numpy arrays
import warnings # Library that handles all the types of warnings during execution
import matplotlib.pyplot as plt# Library that handles ploting of  the graphs
from sklearn.model_selection import train_test_split
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
warnings.filterwarnings("ignore") # Ignore all the warnings

"""### **Data Preprocessing**"""

train_df=pd.read_excel('Data_Train.xlsx')
train_df=train_df.dropna(how='any')
train_df

train_df.info()

train_df.describe()

train_df["Airline"].unique()

test_df=pd.read_excel('Test_set.xlsx')
test_df=test_df.dropna(how='any')
test_df

test_df.info()

test_df.describe()

test_df["Airline"].unique()

"""### **Data Visualization**"""

sns.catplot(y = "Price", x = "Airline", data = train_df.sort_values("Price", ascending = False), kind="boxen", height = 8, aspect = 3)
plt.show()

sns.catplot(y = "Price", x = "Source", data = train_df.sort_values("Price", ascending = False), kind="violin", height = 8, aspect = 3)
plt.show()

sns.catplot(y = "Price", x = "Destination", data = train_df.sort_values("Price", ascending = False), kind="box", height = 8, aspect = 3)
plt.show()

train_df['Duration'] = train_df['Duration'].str.replace("h", '*60').str.replace(' ','+').str.replace('m','*1').apply(eval)
test_df['Duration'] = test_df['Duration'].str.replace("h", '*60').str.replace(' ','+').str.replace('m','*1').apply(eval)

train_df

test_df

train_df["Journey_day"] = train_df['Date_of_Journey'].str.split('/').str[0].astype(int)
train_df["Journey_month"] = train_df['Date_of_Journey'].str.split('/').str[1].astype(int)
train_df.drop(["Date_of_Journey"], axis = 1, inplace = True)

train_df

test_df["Journey_day"] = test_df['Date_of_Journey'].str.split('/').str[0].astype(int)
test_df["Journey_month"] = test_df['Date_of_Journey'].str.split('/').str[1].astype(int)
test_df.drop(["Date_of_Journey"], axis = 1, inplace = True)

test_df

train_df["Dep_hour"] = pd.to_datetime(train_df["Dep_Time"]).dt.hour
train_df["Dep_min"] = pd.to_datetime(train_df["Dep_Time"]).dt.minute
train_df.drop(["Dep_Time"], axis = 1, inplace = True)

train_df

test_df["Dep_hour"] = pd.to_datetime(test_df["Dep_Time"]).dt.hour
test_df["Dep_min"] = pd.to_datetime(test_df["Dep_Time"]).dt.minute
test_df.drop(["Dep_Time"], axis = 1, inplace = True)

test_df

train_df["Arrival_hour"] = pd.to_datetime(train_df.Arrival_Time).dt.hour
train_df["Arrival_min"] = pd.to_datetime(train_df.Arrival_Time).dt.minute
train_df.drop(["Arrival_Time"], axis = 1, inplace = True)

train_df

test_df["Arrival_hour"] = pd.to_datetime(test_df.Arrival_Time).dt.hour
test_df["Arrival_min"] = pd.to_datetime(test_df.Arrival_Time).dt.minute
test_df.drop(["Arrival_Time"], axis = 1, inplace = True)

test_df

train_df['Estimated Price'] = train_df['Price']
train_df['Airline Type'] = train_df['Airline']
train_df=train_df.drop(columns=['Airline','Price'])
train_df

test_df['Airline Type'] = test_df['Airline']
test_df=test_df.drop(columns=['Airline'])
test_df

"""## **Regression Training and Assesment**

## **Prediction of Airline Price using MLP Regressor with Stacked Autoencoder**
"""

train_df.to_csv('Train.csv')
train_df=train_df.drop(columns=['Airline Type'])
test_df.to_csv('Test.csv')
test_df=test_df.drop(columns=['Airline Type'])

train_df

test_df

def LE(df):
  le1= LabelEncoder()
  le2= LabelEncoder()
  le3= LabelEncoder()
  le4= LabelEncoder()
  le5= LabelEncoder()
  df['Source']=le1.fit_transform(df['Source'])
  df['Destination']=le2.fit_transform(df['Destination'])
  df['Route']=le3.fit_transform(df['Route'])
  df['Total_Stops']=le4.fit_transform(df['Total_Stops'])
  df['Additional_Info']=le5.fit_transform(df['Additional_Info'])
  return df

train_df=LE(train_df)

train_df

test_df=LE(test_df)

test_df

X=train_df.iloc[:,:-1].values
y=train_df.iloc[:,-1].values

X

y=y.reshape(len(y),1)

y

sc = StandardScaler()
y=sc.fit_transform(y)
y

X.shape

y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25 ,random_state = 1)

X_val=test_df.iloc[:,:].values

"""### **Feature Engineering using Stacked Autoencoder**"""

from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from sklearn.metrics import r2_score
from tensorflow.keras.models import load_model
import pickle

def Mod1(X_train, X_test, y_train, y_test,n_inputs):
    learning_rate = 1e-5
    input_shape=Input(shape=(n_inputs,))
    # Encoding layers
    e1 = layers.Dense(n_inputs, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(input_shape)
    e2 = layers.Dense(10, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e1)
    e3 = layers.Dense(8, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e2)
    e4 = layers.Dense(6, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e3)
    e5 = layers.Dense(4, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e4)
    #Decoding layers
    d1 = layers.Dense(4, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e3)
    d2 = layers.Dense(6, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d1)
    d3 = layers.Dense(8, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d2)
    d4 = layers.Dense(10, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d3)
    d5 = layers.Dense(n_inputs, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d4)
    # output layer
    output = layers.Dense(n_inputs, activation='linear',activity_regularizer=regularizers.l1(learning_rate))(d5)
    autoencoder = Model(inputs=input_shape, outputs=output)
    autoencoder.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history=autoencoder.fit(X_train, X_train,epochs=2500,batch_size=2500,shuffle=True,validation_data=(X_test,X_test))
    plt.plot(history.history['accuracy'], label='Encoder-1:train')
    plt.plot(history.history['val_accuracy'], label='Encoder-1:test')
    plt.title("Autoencoder-1 Results")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE1-4.png")
    autoencoder_1_input = autoencoder.predict(X_train)
    autoencoder_1_input = np.concatenate((autoencoder_1_input , X_train))
    autoencoder_1 = Model(inputs=input_shape, outputs=output)
    autoencoder_1.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history1=autoencoder_1.fit(autoencoder_1_input,autoencoder_1_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_1_input,autoencoder_1_input))
    plt.plot(history1.history['accuracy'], label='Encoder-2:train')
    plt.plot(history1.history['val_accuracy'], label='Encoder-2:test')
    plt.title("Autoencoder-2 Results")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE2-4.png")
    autoencoder_2_input = autoencoder_1.predict(autoencoder_1_input)
    autoencoder_2_input = np.concatenate((autoencoder_2_input, autoencoder_1_input))
    autoencoder_2 = Model(inputs=input_shape, outputs=output)
    autoencoder_2.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history2=autoencoder_2.fit(autoencoder_2_input,autoencoder_2_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_2_input,autoencoder_2_input))
    plt.plot(history2.history['accuracy'], label='Encoder-3:train')
    plt.plot(history2.history['val_accuracy'], label='Encoder-3:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE3-4.png")
    autoencoder_3_input = autoencoder_2.predict(autoencoder_2_input)
    autoencoder_3_input = np.concatenate((autoencoder_3_input, autoencoder_2_input))
    autoencoder_3 = Model(inputs=input_shape, outputs=output)
    autoencoder_3.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history3=autoencoder_3.fit(autoencoder_3_input,autoencoder_3_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_3_input,autoencoder_3_input))
    plt.plot(history3.history['accuracy'], label='Encoder-4:train')
    plt.plot(history3.history['val_accuracy'], label='Encoder-4:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE4-4.png")
    autoencoder_4_input = autoencoder_3.predict(autoencoder_3_input)
    autoencoder_4_input = np.concatenate((autoencoder_4_input, autoencoder_3_input))
    autoencoder_4 = Model(inputs=input_shape, outputs=output)
    autoencoder_4.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history4=autoencoder_4.fit(autoencoder_4_input,autoencoder_4_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_4_input,autoencoder_4_input))
    plt.plot(history4.history['accuracy'], label='Encoder-5:train')
    plt.plot(history4.history['val_accuracy'], label='Encoder-5:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE5-5.png")
    autoencoder_5_input = autoencoder_4.predict(autoencoder_4_input)
    autoencoder_5_input = np.concatenate((autoencoder_5_input, autoencoder_4_input))
    autoencoder_5 = Model(inputs=input_shape, outputs=output)
    autoencoder_5.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history5=autoencoder_5.fit(autoencoder_5_input,autoencoder_5_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_5_input,autoencoder_5_input))
    plt.plot(history5.history['accuracy'], label='Encoder-6:train')
    plt.plot(history5.history['val_accuracy'], label='Encoder-6:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE5-6.png")
    autoencoder_6_input = autoencoder_5.predict(autoencoder_5_input)
    autoencoder_6_input = np.concatenate((autoencoder_6_input, autoencoder_5_input))
    autoencoder_6 = Model(inputs=input_shape, outputs=output)
    autoencoder_6.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history6=autoencoder_5.fit(autoencoder_6_input,autoencoder_6_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_6_input,autoencoder_6_input))
    plt.plot(history6.history['accuracy'], label='Encoder-7:train')
    plt.plot(history6.history['val_accuracy'], label='Encoder-7:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE5-7.png")
    encoder = Model(inputs=input_shape, outputs=e3)
    encoder.save('STAE1-4-encoder.h5')

n_inputs=12
Mod1(X_train, X_test, y_train, y_test,n_inputs)

"""### **Passive Aggressive Regressor Model**"""

from sklearn.linear_model import PassiveAggressiveRegressor

model =  PassiveAggressiveRegressor(max_iter=100)
encoder = load_model('STAE1-4-encoder.h5')
print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)
X_train_encode = encoder.predict(X_train)
X_test_encode = encoder.predict(X_test)
X_val_encode = encoder.predict(X_val)
print(X_train_encode.shape)
print(X_test_encode.shape)
model.fit(X_train_encode,y_train)
filename = 'PAR.h5'
pickle.dump(model, open(filename, 'wb'))
print("Model saved succesfully!!!")
loaded_model = pickle.load(open(filename, 'rb'))
print("Loaded Model Sucessfully")
yhat = loaded_model.predict(X_test_encode)
print('Regression  Model Training Score: %.3f' % (r2_score(y_test, yhat)))

encoder = load_model('STAE1-4-encoder.h5')
filename = 'PAR.h5'
X_val_encode = encoder.predict(X_val)

loaded_model = pickle.load(open(filename, 'rb'))
print("Loaded Model Sucessfully")
yhat1 = loaded_model.predict(X_val_encode)

"""## **Model Validation**"""

sam=pd.read_excel('Sample_submission.xlsx')
y_val = sam.iloc[:,:].values

y_val

yhat1=sc.inverse_transform(yhat1.reshape(len(yhat1),1))
yhat1

"""### **Model Metrics**"""

from sklearn.metrics import explained_variance_score
from sklearn.metrics import max_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_log_error
from sklearn.metrics import median_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_poisson_deviance

print('\n Explained Variance score:',explained_variance_score(y_val, yhat1))
print('\n Max Score:',max_error(y_val, yhat1))
print('\n Mean absolute error Score:',mean_absolute_error(y_val, yhat1))
print('\n Mean Squared log Error Score:',mean_squared_log_error(y_val, yhat1))
print('\n Median absolute Score:',median_absolute_error(y_val, yhat1))
print('\n Mean absolute Percentage Error Score:',mean_absolute_percentage_error(y_val, yhat1))
print('\n R2 Score:',r2_score(y_val, yhat1))

"""## **Classification Training and Assesment**

## **Prediction of Airline Type using MLP Classifier with Stacked Autoencoder**

**Note: To predict Airline Type , we first require the airline price based on which we predict/validate the same.Hence this model is termed as a Cascaded Model / Pipeline Model**
"""

train_df=pd.read_csv('Train.csv')
test_df=pd.read_csv('Test.csv')

train_df['Airline']= train_df['Airline Type']
train_df=train_df.drop(columns=['Unnamed: 0','Airline Type'])
train_df

test_df['Price'] = np.round(yhat1). astype(int)
test_df['Airline']= test_df['Airline Type']
test_df = test_df.drop(columns=['Unnamed: 0','Airline Type'])
test_df

def LE1(df):
  le1= LabelEncoder()
  le2= LabelEncoder()
  le3= LabelEncoder()
  le4= LabelEncoder()
  le5= LabelEncoder()
  le6 =LabelEncoder()
  df['Source']=le1.fit_transform(df['Source'])
  df['Destination']=le2.fit_transform(df['Destination'])
  df['Route']=le3.fit_transform(df['Route'])
  df['Total_Stops']=le4.fit_transform(df['Total_Stops'])
  df['Additional_Info']=le5.fit_transform(df['Additional_Info'])
  df['Airline']= le6.fit_transform(df['Airline'])
  return df

train_df= LE1(train_df)

train_df

test_df=LE1(test_df)

test_df

X=train_df.iloc[:,:-1].values
y=train_df.iloc[:,-1].values
X_val=test_df.iloc[:,:-1].values
y_val=test_df.iloc[:,-1].values

X

y=y.reshape(len(y),1)

y

X[:,12:13]

sc = StandardScaler()
X[:,12:13]=sc.fit_transform(X[:,12:13])
X

X.shape

y.shape

X_val

y_val=y_val.reshape(len(y_val),1)

y_val

X_val[:,12:13]

sc = StandardScaler()
X_val[:,12:13]=sc.fit_transform(X_val[:,12:13])
X_val

X_val.shape

y_val.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25 ,random_state = 1)

"""### **Feature Engineering Using Stacked Autoencoder**"""

def Mod(X_train, X_test, y_train, y_test,n_inputs):
    learning_rate = 1e-5
    input_shape=Input(shape=(n_inputs,))
    # Encoding layers
    e1 = layers.Dense(n_inputs, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(input_shape)
    e2 = layers.Dense(9, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e1)
    e3 = layers.Dense(7, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e2)
    e4 = layers.Dense(5, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e3)
    e5 = layers.Dense(3, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e4)
    #Decoding layers
    d1 = layers.Dense(3, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e3)
    d2 = layers.Dense(5, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d1)
    d3 = layers.Dense(7, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d2)
    d4 = layers.Dense(9, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d3)
    d5 = layers.Dense(n_inputs, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d4)
    # output layer
    output = layers.Dense(n_inputs, activation='linear',activity_regularizer=regularizers.l1(learning_rate))(d5)
    autoencoder = Model(inputs=input_shape, outputs=output)
    autoencoder.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history=autoencoder.fit(X_train, X_train,epochs=2500,batch_size=2500,shuffle=True,validation_data=(X_test,X_test))
    plt.plot(history.history['accuracy'], label='Encoder-1:train')
    plt.plot(history.history['val_accuracy'], label='Encoder-1:test')
    plt.title("Autoencoder-1 Results")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE1-41.png")
    autoencoder_1_input = autoencoder.predict(X_train)
    autoencoder_1_input = np.concatenate((autoencoder_1_input , X_train))
    autoencoder_1 = Model(inputs=input_shape, outputs=output)
    autoencoder_1.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history1=autoencoder_1.fit(autoencoder_1_input,autoencoder_1_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_1_input,autoencoder_1_input))
    plt.plot(history1.history['accuracy'], label='Encoder-2:train')
    plt.plot(history1.history['val_accuracy'], label='Encoder-2:test')
    plt.title("Autoencoder-2 Results")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE2-42.png")
    autoencoder_2_input = autoencoder_1.predict(autoencoder_1_input)
    autoencoder_2_input = np.concatenate((autoencoder_2_input, autoencoder_1_input))
    autoencoder_2 = Model(inputs=input_shape, outputs=output)
    autoencoder_2.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history2=autoencoder_2.fit(autoencoder_2_input,autoencoder_2_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_2_input,autoencoder_2_input))
    plt.plot(history2.history['accuracy'], label='Encoder-3:train')
    plt.plot(history2.history['val_accuracy'], label='Encoder-3:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE3-43.png")
    autoencoder_3_input = autoencoder_2.predict(autoencoder_2_input)
    autoencoder_3_input = np.concatenate((autoencoder_3_input, autoencoder_2_input))
    autoencoder_3 = Model(inputs=input_shape, outputs=output)
    autoencoder_3.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history3=autoencoder_3.fit(autoencoder_3_input,autoencoder_3_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_3_input,autoencoder_3_input))
    plt.plot(history3.history['accuracy'], label='Encoder-4:train')
    plt.plot(history3.history['val_accuracy'], label='Encoder-4:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE4-44.png")
    autoencoder_4_input = autoencoder_3.predict(autoencoder_3_input)
    autoencoder_4_input = np.concatenate((autoencoder_4_input, autoencoder_3_input))
    autoencoder_4 = Model(inputs=input_shape, outputs=output)
    autoencoder_4.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history4=autoencoder_4.fit(autoencoder_4_input,autoencoder_4_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_4_input,autoencoder_4_input))
    plt.plot(history4.history['accuracy'], label='Encoder-5:train')
    plt.plot(history4.history['val_accuracy'], label='Encoder-5:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE5-45.png")
    autoencoder_5_input = autoencoder_4.predict(autoencoder_4_input)
    autoencoder_5_input = np.concatenate((autoencoder_5_input, autoencoder_4_input))
    autoencoder_5 = Model(inputs=input_shape, outputs=output)
    autoencoder_5.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history5=autoencoder_5.fit(autoencoder_5_input,autoencoder_5_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_5_input,autoencoder_5_input))
    plt.plot(history5.history['accuracy'], label='Encoder-6:train')
    plt.plot(history5.history['val_accuracy'], label='Encoder-6:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE5-46.png")
    autoencoder_6_input = autoencoder_5.predict(autoencoder_5_input)
    autoencoder_6_input = np.concatenate((autoencoder_6_input, autoencoder_5_input))
    autoencoder_6 = Model(inputs=input_shape, outputs=output)
    autoencoder_6.compile(metrics=['accuracy'],optimizer='adam', loss='mse')
    history6=autoencoder_5.fit(autoencoder_6_input,autoencoder_6_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_6_input,autoencoder_6_input))
    plt.plot(history6.history['accuracy'], label='Encoder-7:train')
    plt.plot(history6.history['val_accuracy'], label='Encoder-7:test')
    plt.title("Stacked Autoencoder Results (Layers:5)")
    plt.ylabel("Accuracy--->")
    plt.xlabel("Epochs--->")
    plt.legend()
    plt.savefig("STAE5-47.png")
    encoder = Model(inputs=input_shape, outputs=e3)
    encoder.save('STAE2-4-encoder.h5')

n_inputs=13
Mod(X_train,X_test,y_train,y_test,n_inputs)

"""### **Super Learner Classifier Model**"""

from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from xgboost import XGBClassifier
from tensorflow.keras.models import load_model
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import RidgeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import StackingClassifier
import pickle

models = list()
meta = RandomForestClassifier()
models.append(('LogR',LogisticRegression(solver='liblinear')))
# models.append(('KNN',KNeighborsClassifier()))
models.append(('PC',Perceptron()))
models.append(('RIC',RidgeClassifier()))
models.append(('PAC',PassiveAggressiveClassifier()))
models.append(('BC',BaggingClassifier(meta,n_estimators=150)))
models.append(('RC',RandomForestClassifier(n_estimators=150)))
models.append(('EXTC',ExtraTreesClassifier(n_estimators=150)))
models.append(('XGBC',XGBClassifier()))
models.append(('MLPC',MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=250)))

# meta model
meta = XGBClassifier()

model =  StackingClassifier(estimators=models, final_estimator=meta, cv=25)
encoder = load_model('STAE2-4-encoder.h5')
print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)
X_train_encode = encoder.predict(X_train)
print(X_train_encode.shape)
print(X_test_encode.shape)
model.fit(X_train_encode,y_train)
filename = 'SLMC.h5'
pickle.dump(model, open(filename, 'wb'))
print("Model saved succesfully!!!")

filename = 'SLMC.h5'
encoder = load_model('STAE2-4-encoder.h5')
X_test_encode = encoder.predict(X_test)
X_val_encode = encoder.predict(X_val)

loaded_model = pickle.load(open(filename, 'rb'))
print("Loaded Model Sucessfully")

yhat = loaded_model.predict(X_test_encode)
print('Classifier Learner: %.3f' % (accuracy_score(y_test, yhat) * 100))

loaded_model = pickle.load(open(filename, 'rb'))
print("Loaded Model Sucessfully")
yhat1 = loaded_model.predict(X_val_encode)

y_val

yhat1

"""### **Model Metrics**"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import average_precision_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import f1_score
from sklearn.metrics import hamming_loss
from sklearn.metrics import jaccard_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix

print("\n Score Metrics:")
print('\n Accuracy score:', round(accuracy_score(y_test, yhat) * 100))
print('\n Balanced Accuracy score' ,round(balanced_accuracy_score(y_test, yhat) * 100))
print('\n Cohen Kappa score' ,round(cohen_kappa_score(y_test, yhat) * 100))
print('\n F1 Score (Macro):' ,round(f1_score(y_test, yhat,average='macro') * 100))
print('\n F1 Score (Micro):' ,round(f1_score(y_test, yhat,average='micro') * 100))
print('\n F1 Score (Weighted):' ,round(f1_score(y_test, yhat,average='weighted') * 100))
print('\n Jaccard Score (Macro):' ,round(jaccard_score(y_test, yhat,average='macro') * 100))
print('\n Jaccard Score (Micro):' ,round(jaccard_score(y_test, yhat,average='micro') * 100))
print('\n Jaccard Score (Weighted):' ,round(jaccard_score(y_test, yhat,average='weighted') * 100))
print("\n Loss Metrics:")
print('\n Hamming Loss :' ,hamming_loss(y_test, yhat))
print("\n Confusion matrix")
print('\n')
print(confusion_matrix(y_test,yhat))
print('\n')