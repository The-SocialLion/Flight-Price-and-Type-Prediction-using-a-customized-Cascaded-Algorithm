{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCf4Ti2RrY3P"
      },
      "source": [
        "# **Flight Price and Type Prediction using a Customized Cascaded Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFpUVh0or_Yq"
      },
      "source": [
        "### **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "5MuvGHo7r6As",
        "outputId": "747d806c-e748-4476-d6fe-902cb19a7272"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h1>T4</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import HTML, clear_output\n",
        "from subprocess import getoutput\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'K80' in s:gpu = 'K80'\n",
        "elif 'T4' in s:gpu = 'T4'\n",
        "elif 'P100' in s:gpu = 'P100'\n",
        "elif 'P4' in s:gpu = 'P4'\n",
        "display(HTML(f\"<h1>{gpu}</h1>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6oGvCwbsBxV"
      },
      "source": [
        "### **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ls9mN7y6rTmZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # Library to process the dataframe\n",
        "import numpy as np # Library to handle with numpy arrays\n",
        "import warnings # Library that handles all the types of warnings during execution\n",
        "import matplotlib.pyplot as plt# Library that handles ploting of  the graphs\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "warnings.filterwarnings(\"ignore\") # Ignore all the warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENLtmn9HsO1I"
      },
      "source": [
        "### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Tov7viURsLLg",
        "outputId": "b7a49f40-d4c7-4cc6-d055-eda9811b7725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e5cf11af-113d-43f4-a1e8-cad0d324e0d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Date_of_Journey</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Dep_Time</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>24/03/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>22:20</td>\n",
              "      <td>01:10 22 Mar</td>\n",
              "      <td>2h 50m</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>3897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Air India</td>\n",
              "      <td>1/05/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → IXR → BBI → BLR</td>\n",
              "      <td>05:50</td>\n",
              "      <td>13:15</td>\n",
              "      <td>7h 25m</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>7662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>9/06/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → LKO → BOM → COK</td>\n",
              "      <td>09:25</td>\n",
              "      <td>04:25 10 Jun</td>\n",
              "      <td>19h</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>13882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>12/05/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → NAG → BLR</td>\n",
              "      <td>18:05</td>\n",
              "      <td>23:30</td>\n",
              "      <td>5h 25m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>01/03/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → NAG → DEL</td>\n",
              "      <td>16:50</td>\n",
              "      <td>21:35</td>\n",
              "      <td>4h 45m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>13302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>9/04/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>19:55</td>\n",
              "      <td>22:25</td>\n",
              "      <td>2h 30m</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>Air India</td>\n",
              "      <td>27/04/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>20:45</td>\n",
              "      <td>23:20</td>\n",
              "      <td>2h 35m</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>27/04/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>08:20</td>\n",
              "      <td>11:20</td>\n",
              "      <td>3h</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>7229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>01/03/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>11:30</td>\n",
              "      <td>14:10</td>\n",
              "      <td>2h 40m</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10682</th>\n",
              "      <td>Air India</td>\n",
              "      <td>9/05/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → GOI → BOM → COK</td>\n",
              "      <td>10:55</td>\n",
              "      <td>19:15</td>\n",
              "      <td>8h 20m</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>11753</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5cf11af-113d-43f4-a1e8-cad0d324e0d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e5cf11af-113d-43f4-a1e8-cad0d324e0d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e5cf11af-113d-43f4-a1e8-cad0d324e0d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           Airline Date_of_Journey    Source  ... Total_Stops Additional_Info  Price\n",
              "0           IndiGo      24/03/2019  Banglore  ...    non-stop         No info   3897\n",
              "1        Air India       1/05/2019   Kolkata  ...     2 stops         No info   7662\n",
              "2      Jet Airways       9/06/2019     Delhi  ...     2 stops         No info  13882\n",
              "3           IndiGo      12/05/2019   Kolkata  ...      1 stop         No info   6218\n",
              "4           IndiGo      01/03/2019  Banglore  ...      1 stop         No info  13302\n",
              "...            ...             ...       ...  ...         ...             ...    ...\n",
              "10678     Air Asia       9/04/2019   Kolkata  ...    non-stop         No info   4107\n",
              "10679    Air India      27/04/2019   Kolkata  ...    non-stop         No info   4145\n",
              "10680  Jet Airways      27/04/2019  Banglore  ...    non-stop         No info   7229\n",
              "10681      Vistara      01/03/2019  Banglore  ...    non-stop         No info  12648\n",
              "10682    Air India       9/05/2019     Delhi  ...     2 stops         No info  11753\n",
              "\n",
              "[10682 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "train_df=pd.read_excel('Data_Train.xlsx')\n",
        "train_df=train_df.dropna(how='any')\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUpJ30UksRzE",
        "outputId": "21f35c73-e0db-45aa-90c8-8590d9a1011c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 10682 entries, 0 to 10682\n",
            "Data columns (total 11 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Airline          10682 non-null  object\n",
            " 1   Date_of_Journey  10682 non-null  object\n",
            " 2   Source           10682 non-null  object\n",
            " 3   Destination      10682 non-null  object\n",
            " 4   Route            10682 non-null  object\n",
            " 5   Dep_Time         10682 non-null  object\n",
            " 6   Arrival_Time     10682 non-null  object\n",
            " 7   Duration         10682 non-null  object\n",
            " 8   Total_Stops      10682 non-null  object\n",
            " 9   Additional_Info  10682 non-null  object\n",
            " 10  Price            10682 non-null  int64 \n",
            "dtypes: int64(1), object(10)\n",
            "memory usage: 1001.4+ KB\n"
          ]
        }
      ],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "fe56730mzJvJ",
        "outputId": "d998bf28-e24d-4c23-ea7f-11d745bc66af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e1c3df45-6fbd-4fab-acae-057125ad891f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10682.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>9087.214567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4611.548810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1759.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5277.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8372.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>12373.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>79512.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1c3df45-6fbd-4fab-acae-057125ad891f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1c3df45-6fbd-4fab-acae-057125ad891f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1c3df45-6fbd-4fab-acae-057125ad891f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              Price\n",
              "count  10682.000000\n",
              "mean    9087.214567\n",
              "std     4611.548810\n",
              "min     1759.000000\n",
              "25%     5277.000000\n",
              "50%     8372.000000\n",
              "75%    12373.000000\n",
              "max    79512.000000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhjDyFTPzNca",
        "outputId": "2b201164-13c6-44ff-9499-cbaecad29db7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['IndiGo', 'Air India', 'Jet Airways', 'SpiceJet',\n",
              "       'Multiple carriers', 'GoAir', 'Vistara', 'Air Asia',\n",
              "       'Vistara Premium economy', 'Jet Airways Business',\n",
              "       'Multiple carriers Premium economy', 'Trujet'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_df[\"Airline\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DE5W8lg9zboM",
        "outputId": "d86558ac-9aa4-4b36-e3ec-2cbdb4eac997"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-35196c9b-34b9-42bd-b6e4-619b8dbe25fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Date_of_Journey</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Dep_Time</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>6/06/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>17:30</td>\n",
              "      <td>04:25 07 Jun</td>\n",
              "      <td>10h 55m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>12/05/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → MAA → BLR</td>\n",
              "      <td>06:20</td>\n",
              "      <td>10:20</td>\n",
              "      <td>4h</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>21/05/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>19:15</td>\n",
              "      <td>19:00 22 May</td>\n",
              "      <td>23h 45m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>In-flight meal not included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>21/05/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>08:00</td>\n",
              "      <td>21:00</td>\n",
              "      <td>13h</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>24/06/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>23:55</td>\n",
              "      <td>02:45 25 Jun</td>\n",
              "      <td>2h 50m</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>Air India</td>\n",
              "      <td>6/06/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → DEL → BLR</td>\n",
              "      <td>20:30</td>\n",
              "      <td>20:25 07 Jun</td>\n",
              "      <td>23h 55m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>27/03/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>14:20</td>\n",
              "      <td>16:55</td>\n",
              "      <td>2h 35m</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>6/03/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>21:50</td>\n",
              "      <td>04:25 07 Mar</td>\n",
              "      <td>6h 35m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>Air India</td>\n",
              "      <td>6/03/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>04:00</td>\n",
              "      <td>19:15</td>\n",
              "      <td>15h 15m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>15/06/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>04:55</td>\n",
              "      <td>19:15</td>\n",
              "      <td>14h 20m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35196c9b-34b9-42bd-b6e4-619b8dbe25fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-35196c9b-34b9-42bd-b6e4-619b8dbe25fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-35196c9b-34b9-42bd-b6e4-619b8dbe25fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                Airline  ...              Additional_Info\n",
              "0           Jet Airways  ...                      No info\n",
              "1                IndiGo  ...                      No info\n",
              "2           Jet Airways  ...  In-flight meal not included\n",
              "3     Multiple carriers  ...                      No info\n",
              "4              Air Asia  ...                      No info\n",
              "...                 ...  ...                          ...\n",
              "2666          Air India  ...                      No info\n",
              "2667             IndiGo  ...                      No info\n",
              "2668        Jet Airways  ...                      No info\n",
              "2669          Air India  ...                      No info\n",
              "2670  Multiple carriers  ...                      No info\n",
              "\n",
              "[2671 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test_df=pd.read_excel('Test_set.xlsx')\n",
        "test_df=test_df.dropna(how='any')\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRUE7UBZzVF2",
        "outputId": "524381a8-5786-4a2b-9bad-310e9d135b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2671 entries, 0 to 2670\n",
            "Data columns (total 10 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Airline          2671 non-null   object\n",
            " 1   Date_of_Journey  2671 non-null   object\n",
            " 2   Source           2671 non-null   object\n",
            " 3   Destination      2671 non-null   object\n",
            " 4   Route            2671 non-null   object\n",
            " 5   Dep_Time         2671 non-null   object\n",
            " 6   Arrival_Time     2671 non-null   object\n",
            " 7   Duration         2671 non-null   object\n",
            " 8   Total_Stops      2671 non-null   object\n",
            " 9   Additional_Info  2671 non-null   object\n",
            "dtypes: object(10)\n",
            "memory usage: 229.5+ KB\n"
          ]
        }
      ],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "gyuXPxUkzqSc",
        "outputId": "e10eb5fd-2402-4101-e382-241ea9bdd918"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5697c9ca-3956-49d8-bcec-71b357152314\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Date_of_Journey</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Dep_Time</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "      <td>2671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>11</td>\n",
              "      <td>44</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>100</td>\n",
              "      <td>199</td>\n",
              "      <td>704</td>\n",
              "      <td>320</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>9/05/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>10:00</td>\n",
              "      <td>19:00</td>\n",
              "      <td>2h 50m</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>897</td>\n",
              "      <td>144</td>\n",
              "      <td>1145</td>\n",
              "      <td>1145</td>\n",
              "      <td>624</td>\n",
              "      <td>62</td>\n",
              "      <td>113</td>\n",
              "      <td>122</td>\n",
              "      <td>1431</td>\n",
              "      <td>2148</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5697c9ca-3956-49d8-bcec-71b357152314')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5697c9ca-3956-49d8-bcec-71b357152314 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5697c9ca-3956-49d8-bcec-71b357152314');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            Airline Date_of_Journey Source  ... Duration Total_Stops Additional_Info\n",
              "count          2671            2671   2671  ...     2671        2671            2671\n",
              "unique           11              44      5  ...      320           5               6\n",
              "top     Jet Airways       9/05/2019  Delhi  ...   2h 50m      1 stop         No info\n",
              "freq            897             144   1145  ...      122        1431            2148\n",
              "\n",
              "[4 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "test_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te-Lh9NlzwGD",
        "outputId": "3d6b3ef9-e103-440c-911d-d5838a510da8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Jet Airways', 'IndiGo', 'Multiple carriers', 'Air Asia',\n",
              "       'Air India', 'Vistara', 'SpiceJet', 'Vistara Premium economy',\n",
              "       'GoAir', 'Multiple carriers Premium economy',\n",
              "       'Jet Airways Business'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_df[\"Airline\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS1Sepncz4Vs"
      },
      "source": [
        "### **Data Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "3AiBiD8DzyYq",
        "outputId": "aa422fac-669c-46f1-c40a-354e6089d409"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.catplot(y = \"Price\", x = \"Airline\", data = train_df.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 8, aspect = 3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "vY3pLnyxz68k",
        "outputId": "7a7e1cd4-df15-4756-dc56-f70454d8d569"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.catplot(y = \"Price\", x = \"Source\", data = train_df.sort_values(\"Price\", ascending = False), kind=\"violin\", height = 8, aspect = 3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "QrZySoHxz-yt",
        "outputId": "7b9e1cc7-c039-4bcf-f696-758e6bf2f4d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.catplot(y = \"Price\", x = \"Destination\", data = train_df.sort_values(\"Price\", ascending = False), kind=\"box\", height = 8, aspect = 3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f32S2U8Y0I3I"
      },
      "outputs": [],
      "source": [
        "train_df['Duration'] = train_df['Duration'].str.replace(\"h\", '*60').str.replace(' ','+').str.replace('m','*1').apply(eval)\n",
        "test_df['Duration'] = test_df['Duration'].str.replace(\"h\", '*60').str.replace(' ','+').str.replace('m','*1').apply(eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oqqfDt2Q0Tqg",
        "outputId": "9eb30e8d-0796-4761-d75b-4583bf9185e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-574b512d-8269-48e3-822e-45da9b1bbd5f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Date_of_Journey</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Dep_Time</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>24/03/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>22:20</td>\n",
              "      <td>01:10 22 Mar</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>3897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Air India</td>\n",
              "      <td>1/05/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → IXR → BBI → BLR</td>\n",
              "      <td>05:50</td>\n",
              "      <td>13:15</td>\n",
              "      <td>445</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>7662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>9/06/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → LKO → BOM → COK</td>\n",
              "      <td>09:25</td>\n",
              "      <td>04:25 10 Jun</td>\n",
              "      <td>1140</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>13882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>12/05/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → NAG → BLR</td>\n",
              "      <td>18:05</td>\n",
              "      <td>23:30</td>\n",
              "      <td>325</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>01/03/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → NAG → DEL</td>\n",
              "      <td>16:50</td>\n",
              "      <td>21:35</td>\n",
              "      <td>285</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>13302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>9/04/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>19:55</td>\n",
              "      <td>22:25</td>\n",
              "      <td>150</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>Air India</td>\n",
              "      <td>27/04/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>20:45</td>\n",
              "      <td>23:20</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>27/04/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>08:20</td>\n",
              "      <td>11:20</td>\n",
              "      <td>180</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>7229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>01/03/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>11:30</td>\n",
              "      <td>14:10</td>\n",
              "      <td>160</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10682</th>\n",
              "      <td>Air India</td>\n",
              "      <td>9/05/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → GOI → BOM → COK</td>\n",
              "      <td>10:55</td>\n",
              "      <td>19:15</td>\n",
              "      <td>500</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>11753</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-574b512d-8269-48e3-822e-45da9b1bbd5f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-574b512d-8269-48e3-822e-45da9b1bbd5f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-574b512d-8269-48e3-822e-45da9b1bbd5f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           Airline Date_of_Journey    Source  ... Total_Stops Additional_Info  Price\n",
              "0           IndiGo      24/03/2019  Banglore  ...    non-stop         No info   3897\n",
              "1        Air India       1/05/2019   Kolkata  ...     2 stops         No info   7662\n",
              "2      Jet Airways       9/06/2019     Delhi  ...     2 stops         No info  13882\n",
              "3           IndiGo      12/05/2019   Kolkata  ...      1 stop         No info   6218\n",
              "4           IndiGo      01/03/2019  Banglore  ...      1 stop         No info  13302\n",
              "...            ...             ...       ...  ...         ...             ...    ...\n",
              "10678     Air Asia       9/04/2019   Kolkata  ...    non-stop         No info   4107\n",
              "10679    Air India      27/04/2019   Kolkata  ...    non-stop         No info   4145\n",
              "10680  Jet Airways      27/04/2019  Banglore  ...    non-stop         No info   7229\n",
              "10681      Vistara      01/03/2019  Banglore  ...    non-stop         No info  12648\n",
              "10682    Air India       9/05/2019     Delhi  ...     2 stops         No info  11753\n",
              "\n",
              "[10682 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "I8P4fiqV0VQX",
        "outputId": "0a13c1c2-68f3-4cf0-b925-aec434fe454e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-40d07bba-91f9-474e-90d4-b19aa9fc0bd5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Date_of_Journey</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Dep_Time</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>6/06/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>17:30</td>\n",
              "      <td>04:25 07 Jun</td>\n",
              "      <td>655</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>12/05/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → MAA → BLR</td>\n",
              "      <td>06:20</td>\n",
              "      <td>10:20</td>\n",
              "      <td>240</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>21/05/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>19:15</td>\n",
              "      <td>19:00 22 May</td>\n",
              "      <td>1425</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>In-flight meal not included</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>21/05/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>08:00</td>\n",
              "      <td>21:00</td>\n",
              "      <td>780</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>24/06/2019</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>23:55</td>\n",
              "      <td>02:45 25 Jun</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>Air India</td>\n",
              "      <td>6/06/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → DEL → BLR</td>\n",
              "      <td>20:30</td>\n",
              "      <td>20:25 07 Jun</td>\n",
              "      <td>1435</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>27/03/2019</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>14:20</td>\n",
              "      <td>16:55</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>6/03/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>21:50</td>\n",
              "      <td>04:25 07 Mar</td>\n",
              "      <td>395</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>Air India</td>\n",
              "      <td>6/03/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>04:00</td>\n",
              "      <td>19:15</td>\n",
              "      <td>915</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>15/06/2019</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>04:55</td>\n",
              "      <td>19:15</td>\n",
              "      <td>860</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40d07bba-91f9-474e-90d4-b19aa9fc0bd5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40d07bba-91f9-474e-90d4-b19aa9fc0bd5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40d07bba-91f9-474e-90d4-b19aa9fc0bd5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                Airline  ...              Additional_Info\n",
              "0           Jet Airways  ...                      No info\n",
              "1                IndiGo  ...                      No info\n",
              "2           Jet Airways  ...  In-flight meal not included\n",
              "3     Multiple carriers  ...                      No info\n",
              "4              Air Asia  ...                      No info\n",
              "...                 ...  ...                          ...\n",
              "2666          Air India  ...                      No info\n",
              "2667             IndiGo  ...                      No info\n",
              "2668        Jet Airways  ...                      No info\n",
              "2669          Air India  ...                      No info\n",
              "2670  Multiple carriers  ...                      No info\n",
              "\n",
              "[2671 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dbfeAqLy0YR4"
      },
      "outputs": [],
      "source": [
        "train_df[\"Journey_day\"] = train_df['Date_of_Journey'].str.split('/').str[0].astype(int)\n",
        "train_df[\"Journey_month\"] = train_df['Date_of_Journey'].str.split('/').str[1].astype(int)\n",
        "train_df.drop([\"Date_of_Journey\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aifveFax0cJx",
        "outputId": "780c40ad-82f7-4f34-fde6-57964a750212"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0d008f2d-eba2-479b-a3ce-ff428fc4c834\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Dep_Time</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Price</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>22:20</td>\n",
              "      <td>01:10 22 Mar</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>3897</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → IXR → BBI → BLR</td>\n",
              "      <td>05:50</td>\n",
              "      <td>13:15</td>\n",
              "      <td>445</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>7662</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → LKO → BOM → COK</td>\n",
              "      <td>09:25</td>\n",
              "      <td>04:25 10 Jun</td>\n",
              "      <td>1140</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>13882</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → NAG → BLR</td>\n",
              "      <td>18:05</td>\n",
              "      <td>23:30</td>\n",
              "      <td>325</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6218</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → NAG → DEL</td>\n",
              "      <td>16:50</td>\n",
              "      <td>21:35</td>\n",
              "      <td>285</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>13302</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>19:55</td>\n",
              "      <td>22:25</td>\n",
              "      <td>150</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4107</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>20:45</td>\n",
              "      <td>23:20</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4145</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>08:20</td>\n",
              "      <td>11:20</td>\n",
              "      <td>180</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>7229</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>11:30</td>\n",
              "      <td>14:10</td>\n",
              "      <td>160</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12648</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10682</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → GOI → BOM → COK</td>\n",
              "      <td>10:55</td>\n",
              "      <td>19:15</td>\n",
              "      <td>500</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>11753</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d008f2d-eba2-479b-a3ce-ff428fc4c834')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d008f2d-eba2-479b-a3ce-ff428fc4c834 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d008f2d-eba2-479b-a3ce-ff428fc4c834');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           Airline    Source Destination  ...  Price Journey_day Journey_month\n",
              "0           IndiGo  Banglore   New Delhi  ...   3897          24             3\n",
              "1        Air India   Kolkata    Banglore  ...   7662           1             5\n",
              "2      Jet Airways     Delhi      Cochin  ...  13882           9             6\n",
              "3           IndiGo   Kolkata    Banglore  ...   6218          12             5\n",
              "4           IndiGo  Banglore   New Delhi  ...  13302           1             3\n",
              "...            ...       ...         ...  ...    ...         ...           ...\n",
              "10678     Air Asia   Kolkata    Banglore  ...   4107           9             4\n",
              "10679    Air India   Kolkata    Banglore  ...   4145          27             4\n",
              "10680  Jet Airways  Banglore       Delhi  ...   7229          27             4\n",
              "10681      Vistara  Banglore   New Delhi  ...  12648           1             3\n",
              "10682    Air India     Delhi      Cochin  ...  11753           9             5\n",
              "\n",
              "[10682 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2LyeWaxv0iBT"
      },
      "outputs": [],
      "source": [
        "test_df[\"Journey_day\"] = test_df['Date_of_Journey'].str.split('/').str[0].astype(int)\n",
        "test_df[\"Journey_month\"] = test_df['Date_of_Journey'].str.split('/').str[1].astype(int)\n",
        "test_df.drop([\"Date_of_Journey\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "qfPgzBNO0dUQ",
        "outputId": "7f5a2473-a83d-4fa4-d612-96793b331423"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5b5266c9-9f32-48be-9519-15b69adaf17b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Dep_Time</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>17:30</td>\n",
              "      <td>04:25 07 Jun</td>\n",
              "      <td>655</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → MAA → BLR</td>\n",
              "      <td>06:20</td>\n",
              "      <td>10:20</td>\n",
              "      <td>240</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>19:15</td>\n",
              "      <td>19:00 22 May</td>\n",
              "      <td>1425</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>In-flight meal not included</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>08:00</td>\n",
              "      <td>21:00</td>\n",
              "      <td>780</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>23:55</td>\n",
              "      <td>02:45 25 Jun</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → DEL → BLR</td>\n",
              "      <td>20:30</td>\n",
              "      <td>20:25 07 Jun</td>\n",
              "      <td>1435</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>14:20</td>\n",
              "      <td>16:55</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>21:50</td>\n",
              "      <td>04:25 07 Mar</td>\n",
              "      <td>395</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>04:00</td>\n",
              "      <td>19:15</td>\n",
              "      <td>915</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>04:55</td>\n",
              "      <td>19:15</td>\n",
              "      <td>860</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b5266c9-9f32-48be-9519-15b69adaf17b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5b5266c9-9f32-48be-9519-15b69adaf17b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5b5266c9-9f32-48be-9519-15b69adaf17b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                Airline    Source  ... Journey_day Journey_month\n",
              "0           Jet Airways     Delhi  ...           6             6\n",
              "1                IndiGo   Kolkata  ...          12             5\n",
              "2           Jet Airways     Delhi  ...          21             5\n",
              "3     Multiple carriers     Delhi  ...          21             5\n",
              "4              Air Asia  Banglore  ...          24             6\n",
              "...                 ...       ...  ...         ...           ...\n",
              "2666          Air India   Kolkata  ...           6             6\n",
              "2667             IndiGo   Kolkata  ...          27             3\n",
              "2668        Jet Airways     Delhi  ...           6             3\n",
              "2669          Air India     Delhi  ...           6             3\n",
              "2670  Multiple carriers     Delhi  ...          15             6\n",
              "\n",
              "[2671 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ByqySEZP0tmd"
      },
      "outputs": [],
      "source": [
        "train_df[\"Dep_hour\"] = pd.to_datetime(train_df[\"Dep_Time\"]).dt.hour\n",
        "train_df[\"Dep_min\"] = pd.to_datetime(train_df[\"Dep_Time\"]).dt.minute\n",
        "train_df.drop([\"Dep_Time\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ohWa1lle0z58",
        "outputId": "d071862c-23fe-4b41-d283-6bab333cb0de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c41fe891-cd2f-433d-9402-a30c5a83606e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Price</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>01:10 22 Mar</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>3897</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → IXR → BBI → BLR</td>\n",
              "      <td>13:15</td>\n",
              "      <td>445</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>7662</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → LKO → BOM → COK</td>\n",
              "      <td>04:25 10 Jun</td>\n",
              "      <td>1140</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>13882</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → NAG → BLR</td>\n",
              "      <td>23:30</td>\n",
              "      <td>325</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6218</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → NAG → DEL</td>\n",
              "      <td>21:35</td>\n",
              "      <td>285</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>13302</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>22:25</td>\n",
              "      <td>150</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4107</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>23:20</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4145</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>11:20</td>\n",
              "      <td>180</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>7229</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>14:10</td>\n",
              "      <td>160</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12648</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10682</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → GOI → BOM → COK</td>\n",
              "      <td>19:15</td>\n",
              "      <td>500</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>11753</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c41fe891-cd2f-433d-9402-a30c5a83606e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c41fe891-cd2f-433d-9402-a30c5a83606e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c41fe891-cd2f-433d-9402-a30c5a83606e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           Airline    Source Destination  ... Journey_month Dep_hour  Dep_min\n",
              "0           IndiGo  Banglore   New Delhi  ...             3       22       20\n",
              "1        Air India   Kolkata    Banglore  ...             5        5       50\n",
              "2      Jet Airways     Delhi      Cochin  ...             6        9       25\n",
              "3           IndiGo   Kolkata    Banglore  ...             5       18        5\n",
              "4           IndiGo  Banglore   New Delhi  ...             3       16       50\n",
              "...            ...       ...         ...  ...           ...      ...      ...\n",
              "10678     Air Asia   Kolkata    Banglore  ...             4       19       55\n",
              "10679    Air India   Kolkata    Banglore  ...             4       20       45\n",
              "10680  Jet Airways  Banglore       Delhi  ...             4        8       20\n",
              "10681      Vistara  Banglore   New Delhi  ...             3       11       30\n",
              "10682    Air India     Delhi      Cochin  ...             5       10       55\n",
              "\n",
              "[10682 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9_L8-1wD00iG"
      },
      "outputs": [],
      "source": [
        "test_df[\"Dep_hour\"] = pd.to_datetime(test_df[\"Dep_Time\"]).dt.hour\n",
        "test_df[\"Dep_min\"] = pd.to_datetime(test_df[\"Dep_Time\"]).dt.minute\n",
        "test_df.drop([\"Dep_Time\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "AghsVkC008v8",
        "outputId": "07319ea5-1d39-4c17-c2c3-addc56bf3e55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c5d78b5c-3377-4e8e-a191-c5e9fc83c248\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>04:25 07 Jun</td>\n",
              "      <td>655</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → MAA → BLR</td>\n",
              "      <td>10:20</td>\n",
              "      <td>240</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>19:00 22 May</td>\n",
              "      <td>1425</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>In-flight meal not included</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>21:00</td>\n",
              "      <td>780</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>02:45 25 Jun</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → DEL → BLR</td>\n",
              "      <td>20:25 07 Jun</td>\n",
              "      <td>1435</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>16:55</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>04:25 07 Mar</td>\n",
              "      <td>395</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>19:15</td>\n",
              "      <td>915</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>19:15</td>\n",
              "      <td>860</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5d78b5c-3377-4e8e-a191-c5e9fc83c248')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5d78b5c-3377-4e8e-a191-c5e9fc83c248 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5d78b5c-3377-4e8e-a191-c5e9fc83c248');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                Airline    Source Destination  ... Journey_month Dep_hour  Dep_min\n",
              "0           Jet Airways     Delhi      Cochin  ...             6       17       30\n",
              "1                IndiGo   Kolkata    Banglore  ...             5        6       20\n",
              "2           Jet Airways     Delhi      Cochin  ...             5       19       15\n",
              "3     Multiple carriers     Delhi      Cochin  ...             5        8        0\n",
              "4              Air Asia  Banglore       Delhi  ...             6       23       55\n",
              "...                 ...       ...         ...  ...           ...      ...      ...\n",
              "2666          Air India   Kolkata    Banglore  ...             6       20       30\n",
              "2667             IndiGo   Kolkata    Banglore  ...             3       14       20\n",
              "2668        Jet Airways     Delhi      Cochin  ...             3       21       50\n",
              "2669          Air India     Delhi      Cochin  ...             3        4        0\n",
              "2670  Multiple carriers     Delhi      Cochin  ...             6        4       55\n",
              "\n",
              "[2671 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oZtKio-60_Tn"
      },
      "outputs": [],
      "source": [
        "train_df[\"Arrival_hour\"] = pd.to_datetime(train_df.Arrival_Time).dt.hour\n",
        "train_df[\"Arrival_min\"] = pd.to_datetime(train_df.Arrival_Time).dt.minute\n",
        "train_df.drop([\"Arrival_Time\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hvnEP5Kv1Fjc",
        "outputId": "09d3da4e-8b7d-4af0-cd1d-2eba591c4773"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-67716877-d24f-44a3-9d02-092d01130902\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Price</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>3897</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → IXR → BBI → BLR</td>\n",
              "      <td>445</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>7662</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → LKO → BOM → COK</td>\n",
              "      <td>1140</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>13882</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → NAG → BLR</td>\n",
              "      <td>325</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6218</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → NAG → DEL</td>\n",
              "      <td>285</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>13302</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>50</td>\n",
              "      <td>21</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>150</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4107</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>4145</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>45</td>\n",
              "      <td>23</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>180</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>7229</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>Vistara</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>160</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12648</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10682</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → GOI → BOM → COK</td>\n",
              "      <td>500</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>11753</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67716877-d24f-44a3-9d02-092d01130902')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67716877-d24f-44a3-9d02-092d01130902 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67716877-d24f-44a3-9d02-092d01130902');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           Airline    Source Destination  ... Dep_min  Arrival_hour Arrival_min\n",
              "0           IndiGo  Banglore   New Delhi  ...      20             1          10\n",
              "1        Air India   Kolkata    Banglore  ...      50            13          15\n",
              "2      Jet Airways     Delhi      Cochin  ...      25             4          25\n",
              "3           IndiGo   Kolkata    Banglore  ...       5            23          30\n",
              "4           IndiGo  Banglore   New Delhi  ...      50            21          35\n",
              "...            ...       ...         ...  ...     ...           ...         ...\n",
              "10678     Air Asia   Kolkata    Banglore  ...      55            22          25\n",
              "10679    Air India   Kolkata    Banglore  ...      45            23          20\n",
              "10680  Jet Airways  Banglore       Delhi  ...      20            11          20\n",
              "10681      Vistara  Banglore   New Delhi  ...      30            14          10\n",
              "10682    Air India     Delhi      Cochin  ...      55            19          15\n",
              "\n",
              "[10682 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sqcQVfkr1G4w"
      },
      "outputs": [],
      "source": [
        "test_df[\"Arrival_hour\"] = pd.to_datetime(test_df.Arrival_Time).dt.hour\n",
        "test_df[\"Arrival_min\"] = pd.to_datetime(test_df.Arrival_Time).dt.minute\n",
        "test_df.drop([\"Arrival_Time\"], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "_Vuj7RvX1NJQ",
        "outputId": "1abbf318-b131-496e-a749-41944ac29286"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1093f650-579d-46f8-86cc-aab2f8371ea0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Airline</th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>655</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → MAA → BLR</td>\n",
              "      <td>240</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>1425</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>In-flight meal not included</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>780</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Air Asia</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → DEL → BLR</td>\n",
              "      <td>1435</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>IndiGo</td>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Jet Airways</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>395</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>50</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>Air India</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>915</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>Multiple carriers</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>860</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1093f650-579d-46f8-86cc-aab2f8371ea0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1093f650-579d-46f8-86cc-aab2f8371ea0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1093f650-579d-46f8-86cc-aab2f8371ea0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                Airline    Source  ... Arrival_hour Arrival_min\n",
              "0           Jet Airways     Delhi  ...            4          25\n",
              "1                IndiGo   Kolkata  ...           10          20\n",
              "2           Jet Airways     Delhi  ...           19           0\n",
              "3     Multiple carriers     Delhi  ...           21           0\n",
              "4              Air Asia  Banglore  ...            2          45\n",
              "...                 ...       ...  ...          ...         ...\n",
              "2666          Air India   Kolkata  ...           20          25\n",
              "2667             IndiGo   Kolkata  ...           16          55\n",
              "2668        Jet Airways     Delhi  ...            4          25\n",
              "2669          Air India     Delhi  ...           19          15\n",
              "2670  Multiple carriers     Delhi  ...           19          15\n",
              "\n",
              "[2671 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "1ibk53At1aoC",
        "outputId": "a505d4ce-d0d2-4244-8243-34fa0b873399"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6dbe138c-c83a-4feb-b311-c1121bc9b522\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "      <th>Estimated Price</th>\n",
              "      <th>Airline Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3897</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → IXR → BBI → BLR</td>\n",
              "      <td>445</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>7662</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → LKO → BOM → COK</td>\n",
              "      <td>1140</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>13882</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → NAG → BLR</td>\n",
              "      <td>325</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>30</td>\n",
              "      <td>6218</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → NAG → DEL</td>\n",
              "      <td>285</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>50</td>\n",
              "      <td>21</td>\n",
              "      <td>35</td>\n",
              "      <td>13302</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>150</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>4107</td>\n",
              "      <td>Air Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>45</td>\n",
              "      <td>23</td>\n",
              "      <td>20</td>\n",
              "      <td>4145</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>180</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>7229</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>160</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>12648</td>\n",
              "      <td>Vistara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10682</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → GOI → BOM → COK</td>\n",
              "      <td>500</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>11753</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6dbe138c-c83a-4feb-b311-c1121bc9b522')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6dbe138c-c83a-4feb-b311-c1121bc9b522 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6dbe138c-c83a-4feb-b311-c1121bc9b522');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Source Destination  ... Estimated Price  Airline Type\n",
              "0      Banglore   New Delhi  ...            3897        IndiGo\n",
              "1       Kolkata    Banglore  ...            7662     Air India\n",
              "2         Delhi      Cochin  ...           13882   Jet Airways\n",
              "3       Kolkata    Banglore  ...            6218        IndiGo\n",
              "4      Banglore   New Delhi  ...           13302        IndiGo\n",
              "...         ...         ...  ...             ...           ...\n",
              "10678   Kolkata    Banglore  ...            4107      Air Asia\n",
              "10679   Kolkata    Banglore  ...            4145     Air India\n",
              "10680  Banglore       Delhi  ...            7229   Jet Airways\n",
              "10681  Banglore   New Delhi  ...           12648       Vistara\n",
              "10682     Delhi      Cochin  ...           11753     Air India\n",
              "\n",
              "[10682 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "train_df['Estimated Price'] = train_df['Price']\n",
        "train_df['Airline Type'] = train_df['Airline']\n",
        "train_df=train_df.drop(columns=['Airline','Price'])\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "s-VxAjcD25ra",
        "outputId": "a0c1a985-5cdf-45f4-f550-c59aa5bc9b38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c0fd42ff-0a81-4793-9956-731c4758beee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "      <th>Airline Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>655</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → MAA → BLR</td>\n",
              "      <td>240</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>1425</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>In-flight meal not included</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>780</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>Multiple carriers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "      <td>Air Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → DEL → BLR</td>\n",
              "      <td>1435</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>55</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>395</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>50</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>915</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>860</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>Multiple carriers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0fd42ff-0a81-4793-9956-731c4758beee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0fd42ff-0a81-4793-9956-731c4758beee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0fd42ff-0a81-4793-9956-731c4758beee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Source Destination  ... Arrival_min       Airline Type\n",
              "0        Delhi      Cochin  ...          25        Jet Airways\n",
              "1      Kolkata    Banglore  ...          20             IndiGo\n",
              "2        Delhi      Cochin  ...           0        Jet Airways\n",
              "3        Delhi      Cochin  ...           0  Multiple carriers\n",
              "4     Banglore       Delhi  ...          45           Air Asia\n",
              "...        ...         ...  ...         ...                ...\n",
              "2666   Kolkata    Banglore  ...          25          Air India\n",
              "2667   Kolkata    Banglore  ...          55             IndiGo\n",
              "2668     Delhi      Cochin  ...          25        Jet Airways\n",
              "2669     Delhi      Cochin  ...          15          Air India\n",
              "2670     Delhi      Cochin  ...          15  Multiple carriers\n",
              "\n",
              "[2671 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "test_df['Airline Type'] = test_df['Airline']\n",
        "test_df=test_df.drop(columns=['Airline'])\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Regression Training and Assesment**"
      ],
      "metadata": {
        "id": "yq3_TtWe7C8z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71UAVdsEee2C"
      },
      "source": [
        "## **Prediction of Airline Price using MLP Regressor with Stacked Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4_SXd09s3S4t"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv('Train.csv')\n",
        "train_df=train_df.drop(columns=['Airline Type'])\n",
        "test_df.to_csv('Test.csv')\n",
        "test_df=test_df.drop(columns=['Airline Type'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "OCYJcjHx3lvv",
        "outputId": "cc52d7f9-e4aa-4b07-9b99-ca82bcd4a348"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2c778dfc-940a-4a6c-86b7-3c23ab212ef4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "      <th>Estimated Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → IXR → BBI → BLR</td>\n",
              "      <td>445</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>7662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → LKO → BOM → COK</td>\n",
              "      <td>1140</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>13882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → NAG → BLR</td>\n",
              "      <td>325</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>30</td>\n",
              "      <td>6218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → NAG → DEL</td>\n",
              "      <td>285</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>50</td>\n",
              "      <td>21</td>\n",
              "      <td>35</td>\n",
              "      <td>13302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>150</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>4107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>45</td>\n",
              "      <td>23</td>\n",
              "      <td>20</td>\n",
              "      <td>4145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>180</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>7229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>160</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>12648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10682</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → GOI → BOM → COK</td>\n",
              "      <td>500</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>11753</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c778dfc-940a-4a6c-86b7-3c23ab212ef4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c778dfc-940a-4a6c-86b7-3c23ab212ef4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c778dfc-940a-4a6c-86b7-3c23ab212ef4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Source Destination  ... Arrival_min  Estimated Price\n",
              "0      Banglore   New Delhi  ...          10             3897\n",
              "1       Kolkata    Banglore  ...          15             7662\n",
              "2         Delhi      Cochin  ...          25            13882\n",
              "3       Kolkata    Banglore  ...          30             6218\n",
              "4      Banglore   New Delhi  ...          35            13302\n",
              "...         ...         ...  ...         ...              ...\n",
              "10678   Kolkata    Banglore  ...          25             4107\n",
              "10679   Kolkata    Banglore  ...          20             4145\n",
              "10680  Banglore       Delhi  ...          20             7229\n",
              "10681  Banglore   New Delhi  ...          10            12648\n",
              "10682     Delhi      Cochin  ...          15            11753\n",
              "\n",
              "[10682 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Ksdiw3ty3uLY",
        "outputId": "a9f74bb9-8c44-4116-9820-6ff2c3b91d48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5fc4223b-0685-4500-a830-7f51e8c6df71\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>655</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → MAA → BLR</td>\n",
              "      <td>240</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>1425</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>In-flight meal not included</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>780</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → DEL → BLR</td>\n",
              "      <td>1435</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>395</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>50</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>915</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>860</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5fc4223b-0685-4500-a830-7f51e8c6df71')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5fc4223b-0685-4500-a830-7f51e8c6df71 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5fc4223b-0685-4500-a830-7f51e8c6df71');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Source Destination            Route  ...  Dep_min Arrival_hour Arrival_min\n",
              "0        Delhi      Cochin  DEL → BOM → COK  ...       30            4          25\n",
              "1      Kolkata    Banglore  CCU → MAA → BLR  ...       20           10          20\n",
              "2        Delhi      Cochin  DEL → BOM → COK  ...       15           19           0\n",
              "3        Delhi      Cochin  DEL → BOM → COK  ...        0           21           0\n",
              "4     Banglore       Delhi        BLR → DEL  ...       55            2          45\n",
              "...        ...         ...              ...  ...      ...          ...         ...\n",
              "2666   Kolkata    Banglore  CCU → DEL → BLR  ...       30           20          25\n",
              "2667   Kolkata    Banglore        CCU → BLR  ...       20           16          55\n",
              "2668     Delhi      Cochin  DEL → BOM → COK  ...       50            4          25\n",
              "2669     Delhi      Cochin  DEL → BOM → COK  ...        0           19          15\n",
              "2670     Delhi      Cochin  DEL → BOM → COK  ...       55           19          15\n",
              "\n",
              "[2671 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dd--bchB34Bt"
      },
      "outputs": [],
      "source": [
        "def LE(df):\n",
        "  le1= LabelEncoder()\n",
        "  le2= LabelEncoder()\n",
        "  le3= LabelEncoder()\n",
        "  le4= LabelEncoder()\n",
        "  le5= LabelEncoder()\n",
        "  df['Source']=le1.fit_transform(df['Source'])\n",
        "  df['Destination']=le2.fit_transform(df['Destination'])\n",
        "  df['Route']=le3.fit_transform(df['Route'])\n",
        "  df['Total_Stops']=le4.fit_transform(df['Total_Stops'])\n",
        "  df['Additional_Info']=le5.fit_transform(df['Additional_Info'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bRE0sWaN3wc5"
      },
      "outputs": [],
      "source": [
        "train_df=LE(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "IAmbpHGf5TG5",
        "outputId": "f1ad87b5-d90d-4399-a156-a041cc4d473b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2c3b151d-d761-4f91-8283-0989a251a313\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "      <th>Estimated Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>170</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>84</td>\n",
              "      <td>445</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>7662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>118</td>\n",
              "      <td>1140</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>13882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>91</td>\n",
              "      <td>325</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>30</td>\n",
              "      <td>6218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>29</td>\n",
              "      <td>285</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>50</td>\n",
              "      <td>21</td>\n",
              "      <td>35</td>\n",
              "      <td>13302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>150</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>4107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>155</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>45</td>\n",
              "      <td>23</td>\n",
              "      <td>20</td>\n",
              "      <td>4145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>180</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>7229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>160</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>12648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10682</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>108</td>\n",
              "      <td>500</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>11753</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c3b151d-d761-4f91-8283-0989a251a313')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c3b151d-d761-4f91-8283-0989a251a313 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c3b151d-d761-4f91-8283-0989a251a313');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Source  Destination  Route  ...  Arrival_hour  Arrival_min  Estimated Price\n",
              "0           0            5     18  ...             1           10             3897\n",
              "1           3            0     84  ...            13           15             7662\n",
              "2           2            1    118  ...             4           25            13882\n",
              "3           3            0     91  ...            23           30             6218\n",
              "4           0            5     29  ...            21           35            13302\n",
              "...       ...          ...    ...  ...           ...          ...              ...\n",
              "10678       3            0     64  ...            22           25             4107\n",
              "10679       3            0     64  ...            23           20             4145\n",
              "10680       0            2     18  ...            11           20             7229\n",
              "10681       0            5     18  ...            14           10            12648\n",
              "10682       2            1    108  ...            19           15            11753\n",
              "\n",
              "[10682 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5iqlSVD15HNK"
      },
      "outputs": [],
      "source": [
        "test_df=LE(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "a5eKQiIs5nQc",
        "outputId": "d49c5cbb-d69b-4f17-d212-42c0349598db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-27ab49d1-60a9-4d46-858a-6df355d2a086\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>655</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>65</td>\n",
              "      <td>240</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>1425</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>780</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>170</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>1435</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>155</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>395</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>50</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>915</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>860</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27ab49d1-60a9-4d46-858a-6df355d2a086')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27ab49d1-60a9-4d46-858a-6df355d2a086 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27ab49d1-60a9-4d46-858a-6df355d2a086');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Source  Destination  Route  ...  Dep_min  Arrival_hour  Arrival_min\n",
              "0          2            1     76  ...       30             4           25\n",
              "1          3            0     65  ...       20            10           20\n",
              "2          2            1     76  ...       15            19            0\n",
              "3          2            1     76  ...        0            21            0\n",
              "4          0            2     16  ...       55             2           45\n",
              "...      ...          ...    ...  ...      ...           ...          ...\n",
              "2666       3            0     51  ...       30            20           25\n",
              "2667       3            0     43  ...       20            16           55\n",
              "2668       2            1     76  ...       50             4           25\n",
              "2669       2            1     76  ...        0            19           15\n",
              "2670       2            1     76  ...       55            19           15\n",
              "\n",
              "[2671 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GfTLkanS67Xt"
      },
      "outputs": [],
      "source": [
        "X=train_df.iloc[:,:-1].values\n",
        "y=train_df.iloc[:,-1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBHxhbx_64KW",
        "outputId": "007c3886-b4d8-4c6f-bd03-040486d89e70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   5,  18, ...,  20,   1,  10],\n",
              "       [  3,   0,  84, ...,  50,  13,  15],\n",
              "       [  2,   1, 118, ...,  25,   4,  25],\n",
              "       ...,\n",
              "       [  0,   2,  18, ...,  20,  11,  20],\n",
              "       [  0,   5,  18, ...,  30,  14,  10],\n",
              "       [  2,   1, 108, ...,  55,  19,  15]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cdGe8Dfg7UJY"
      },
      "outputs": [],
      "source": [
        "y=y.reshape(len(y),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRMIBtoA7XYS",
        "outputId": "6c22dc72-422c-4e4f-c301-fbc5a1a750fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3897],\n",
              "       [ 7662],\n",
              "       [13882],\n",
              "       ...,\n",
              "       [ 7229],\n",
              "       [12648],\n",
              "       [11753]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnbCYmNx7Yz1",
        "outputId": "e2b33d40-a2f2-4039-a0c8-c6de0af2f31c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.12553455],\n",
              "       [-0.30906781],\n",
              "       [ 1.03978296],\n",
              "       ...,\n",
              "       [-0.40296691],\n",
              "       [ 0.77218138],\n",
              "       [ 0.57809433]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "sc = StandardScaler()\n",
        "y=sc.fit_transform(y)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN5LG8T-7uaz",
        "outputId": "f5ae67ef-e265-4742-d494-08610afd3ade"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10682, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNXn5oqP7zYQ",
        "outputId": "7c5f7a5b-0011-4e55-ef1e-9b50ace1a3cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10682, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "3oeJolul71QI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25 ,random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZT4UGr0CCsXC"
      },
      "outputs": [],
      "source": [
        "X_val=test_df.iloc[:,:].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ZrqZZW6WV6"
      },
      "source": [
        "### **Feature Engineering using Stacked Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "StVuzFmB8aZc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcfFEsX56aB6"
      },
      "outputs": [],
      "source": [
        "def Mod1(X_train, X_test, y_train, y_test,n_inputs):\n",
        "    learning_rate = 1e-5\n",
        "    input_shape=Input(shape=(n_inputs,))\n",
        "    # Encoding layers\n",
        "    e1 = layers.Dense(n_inputs, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(input_shape)\n",
        "    e2 = layers.Dense(10, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e1)\n",
        "    e3 = layers.Dense(8, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e2)\n",
        "    e4 = layers.Dense(6, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e3)\n",
        "    e5 = layers.Dense(4, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e4)\n",
        "    #Decoding layers\n",
        "    d1 = layers.Dense(4, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e3)\n",
        "    d2 = layers.Dense(6, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d1)\n",
        "    d3 = layers.Dense(8, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d2)\n",
        "    d4 = layers.Dense(10, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d3)\n",
        "    d5 = layers.Dense(n_inputs, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d4)\n",
        "    # output layer\n",
        "    output = layers.Dense(n_inputs, activation='linear',activity_regularizer=regularizers.l1(learning_rate))(d5)\n",
        "    autoencoder = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history=autoencoder.fit(X_train, X_train,epochs=2500,batch_size=2500,shuffle=True,validation_data=(X_test,X_test))\n",
        "    plt.plot(history.history['accuracy'], label='Encoder-1:train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Encoder-1:test')\n",
        "    plt.title(\"Autoencoder-1 Results\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE1-4.png\")\n",
        "    autoencoder_1_input = autoencoder.predict(X_train)\n",
        "    autoencoder_1_input = np.concatenate((autoencoder_1_input , X_train))\n",
        "    autoencoder_1 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_1.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history1=autoencoder_1.fit(autoencoder_1_input,autoencoder_1_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_1_input,autoencoder_1_input))\n",
        "    plt.plot(history1.history['accuracy'], label='Encoder-2:train')\n",
        "    plt.plot(history1.history['val_accuracy'], label='Encoder-2:test')\n",
        "    plt.title(\"Autoencoder-2 Results\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE2-4.png\")\n",
        "    autoencoder_2_input = autoencoder_1.predict(autoencoder_1_input)\n",
        "    autoencoder_2_input = np.concatenate((autoencoder_2_input, autoencoder_1_input))\n",
        "    autoencoder_2 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_2.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history2=autoencoder_2.fit(autoencoder_2_input,autoencoder_2_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_2_input,autoencoder_2_input))\n",
        "    plt.plot(history2.history['accuracy'], label='Encoder-3:train')\n",
        "    plt.plot(history2.history['val_accuracy'], label='Encoder-3:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE3-4.png\")\n",
        "    autoencoder_3_input = autoencoder_2.predict(autoencoder_2_input)\n",
        "    autoencoder_3_input = np.concatenate((autoencoder_3_input, autoencoder_2_input))\n",
        "    autoencoder_3 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_3.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history3=autoencoder_3.fit(autoencoder_3_input,autoencoder_3_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_3_input,autoencoder_3_input))\n",
        "    plt.plot(history3.history['accuracy'], label='Encoder-4:train')\n",
        "    plt.plot(history3.history['val_accuracy'], label='Encoder-4:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE4-4.png\")\n",
        "    autoencoder_4_input = autoencoder_3.predict(autoencoder_3_input)\n",
        "    autoencoder_4_input = np.concatenate((autoencoder_4_input, autoencoder_3_input))\n",
        "    autoencoder_4 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_4.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history4=autoencoder_4.fit(autoencoder_4_input,autoencoder_4_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_4_input,autoencoder_4_input))\n",
        "    plt.plot(history4.history['accuracy'], label='Encoder-5:train')\n",
        "    plt.plot(history4.history['val_accuracy'], label='Encoder-5:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE5-5.png\")\n",
        "    autoencoder_5_input = autoencoder_4.predict(autoencoder_4_input)\n",
        "    autoencoder_5_input = np.concatenate((autoencoder_5_input, autoencoder_4_input))\n",
        "    autoencoder_5 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_5.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history5=autoencoder_5.fit(autoencoder_5_input,autoencoder_5_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_5_input,autoencoder_5_input))\n",
        "    plt.plot(history5.history['accuracy'], label='Encoder-6:train')\n",
        "    plt.plot(history5.history['val_accuracy'], label='Encoder-6:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE5-6.png\")\n",
        "    autoencoder_6_input = autoencoder_5.predict(autoencoder_5_input)\n",
        "    autoencoder_6_input = np.concatenate((autoencoder_6_input, autoencoder_5_input))\n",
        "    autoencoder_6 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_6.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history6=autoencoder_5.fit(autoencoder_6_input,autoencoder_6_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_6_input,autoencoder_6_input))\n",
        "    plt.plot(history6.history['accuracy'], label='Encoder-7:train')\n",
        "    plt.plot(history6.history['val_accuracy'], label='Encoder-7:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE5-7.png\")\n",
        "    encoder = Model(inputs=input_shape, outputs=e3)\n",
        "    encoder.save('STAE1-4-encoder.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gt0AOWOr7qiI",
        "outputId": "487151a4-1746-4ed2-d224-d0dc488a8172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6695 - accuracy: 1.0000 - val_loss: 0.6817 - val_accuracy: 1.0000\n",
            "Epoch 2/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6319 - accuracy: 1.0000 - val_loss: 0.6383 - val_accuracy: 1.0000\n",
            "Epoch 3/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6598 - accuracy: 1.0000 - val_loss: 0.6472 - val_accuracy: 1.0000\n",
            "Epoch 4/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6482 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 1.0000\n",
            "Epoch 5/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6622 - accuracy: 1.0000 - val_loss: 0.6200 - val_accuracy: 1.0000\n",
            "Epoch 6/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6501 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 1.0000\n",
            "Epoch 7/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6677 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 1.0000\n",
            "Epoch 8/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6794 - accuracy: 1.0000 - val_loss: 0.6392 - val_accuracy: 1.0000\n",
            "Epoch 9/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6400 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 1.0000\n",
            "Epoch 10/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6507 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 1.0000\n",
            "Epoch 11/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6675 - accuracy: 1.0000 - val_loss: 0.8485 - val_accuracy: 1.0000\n",
            "Epoch 12/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6506 - accuracy: 1.0000 - val_loss: 0.6220 - val_accuracy: 1.0000\n",
            "Epoch 13/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6838 - accuracy: 1.0000 - val_loss: 0.6159 - val_accuracy: 1.0000\n",
            "Epoch 14/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6672 - accuracy: 1.0000 - val_loss: 0.6395 - val_accuracy: 1.0000\n",
            "Epoch 15/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6248 - accuracy: 1.0000 - val_loss: 0.7545 - val_accuracy: 1.0000\n",
            "Epoch 16/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6553 - accuracy: 1.0000 - val_loss: 0.6116 - val_accuracy: 1.0000\n",
            "Epoch 17/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6613 - accuracy: 1.0000 - val_loss: 0.6372 - val_accuracy: 1.0000\n",
            "Epoch 18/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 0.6948 - val_accuracy: 1.0000\n",
            "Epoch 19/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6740 - accuracy: 1.0000 - val_loss: 0.6947 - val_accuracy: 1.0000\n",
            "Epoch 20/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6495 - accuracy: 1.0000 - val_loss: 0.6372 - val_accuracy: 1.0000\n",
            "Epoch 21/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6673 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 22/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6504 - accuracy: 1.0000 - val_loss: 0.6190 - val_accuracy: 1.0000\n",
            "Epoch 23/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6595 - accuracy: 1.0000 - val_loss: 0.6190 - val_accuracy: 1.0000\n",
            "Epoch 24/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6549 - accuracy: 1.0000 - val_loss: 0.6231 - val_accuracy: 1.0000\n",
            "Epoch 25/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6704 - accuracy: 1.0000 - val_loss: 0.6673 - val_accuracy: 1.0000\n",
            "Epoch 26/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6530 - accuracy: 1.0000 - val_loss: 0.6195 - val_accuracy: 1.0000\n",
            "Epoch 27/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6655 - accuracy: 1.0000 - val_loss: 0.6300 - val_accuracy: 1.0000\n",
            "Epoch 28/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6159 - val_accuracy: 1.0000\n",
            "Epoch 29/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 0.6591 - val_accuracy: 1.0000\n",
            "Epoch 30/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7031 - accuracy: 1.0000 - val_loss: 0.6213 - val_accuracy: 1.0000\n",
            "Epoch 31/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6233 - accuracy: 1.0000 - val_loss: 0.6317 - val_accuracy: 1.0000\n",
            "Epoch 32/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6601 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 1.0000\n",
            "Epoch 33/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6881 - val_accuracy: 1.0000\n",
            "Epoch 34/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6327 - val_accuracy: 1.0000\n",
            "Epoch 35/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6679 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 1.0000\n",
            "Epoch 36/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6601 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 1.0000\n",
            "Epoch 37/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6572 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 1.0000\n",
            "Epoch 38/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6631 - accuracy: 1.0000 - val_loss: 0.7084 - val_accuracy: 1.0000\n",
            "Epoch 39/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6416 - accuracy: 1.0000 - val_loss: 0.6358 - val_accuracy: 1.0000\n",
            "Epoch 40/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6485 - accuracy: 1.0000 - val_loss: 0.7184 - val_accuracy: 1.0000\n",
            "Epoch 41/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6755 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 1.0000\n",
            "Epoch 42/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6490 - accuracy: 1.0000 - val_loss: 0.6197 - val_accuracy: 1.0000\n",
            "Epoch 43/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6714 - accuracy: 1.0000 - val_loss: 0.6219 - val_accuracy: 1.0000\n",
            "Epoch 44/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6548 - accuracy: 1.0000 - val_loss: 0.6281 - val_accuracy: 1.0000\n",
            "Epoch 45/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6538 - accuracy: 1.0000 - val_loss: 0.6297 - val_accuracy: 1.0000\n",
            "Epoch 46/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6670 - accuracy: 1.0000 - val_loss: 0.6148 - val_accuracy: 1.0000\n",
            "Epoch 47/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6505 - accuracy: 1.0000 - val_loss: 0.7329 - val_accuracy: 1.0000\n",
            "Epoch 48/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6624 - accuracy: 1.0000 - val_loss: 0.6239 - val_accuracy: 1.0000\n",
            "Epoch 49/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6401 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 1.0000\n",
            "Epoch 50/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6603 - accuracy: 1.0000 - val_loss: 0.6525 - val_accuracy: 1.0000\n",
            "Epoch 51/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6747 - accuracy: 1.0000 - val_loss: 1.5199 - val_accuracy: 1.0000\n",
            "Epoch 52/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6558 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 1.0000\n",
            "Epoch 53/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6402 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 1.0000\n",
            "Epoch 54/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6456 - accuracy: 1.0000 - val_loss: 0.6277 - val_accuracy: 1.0000\n",
            "Epoch 55/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6709 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 1.0000\n",
            "Epoch 56/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6399 - accuracy: 1.0000 - val_loss: 0.6230 - val_accuracy: 1.0000\n",
            "Epoch 57/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6644 - accuracy: 1.0000 - val_loss: 0.6319 - val_accuracy: 1.0000\n",
            "Epoch 58/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6508 - accuracy: 1.0000 - val_loss: 0.7477 - val_accuracy: 1.0000\n",
            "Epoch 59/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6636 - accuracy: 1.0000 - val_loss: 0.6513 - val_accuracy: 1.0000\n",
            "Epoch 60/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6680 - accuracy: 1.0000 - val_loss: 0.7462 - val_accuracy: 1.0000\n",
            "Epoch 61/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6437 - accuracy: 1.0000 - val_loss: 0.7081 - val_accuracy: 1.0000\n",
            "Epoch 62/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6919 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 1.0000\n",
            "Epoch 63/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6200 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 1.0000\n",
            "Epoch 64/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6610 - accuracy: 1.0000 - val_loss: 1.0551 - val_accuracy: 1.0000\n",
            "Epoch 65/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6669 - accuracy: 1.0000 - val_loss: 1.2046 - val_accuracy: 1.0000\n",
            "Epoch 66/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6616 - accuracy: 1.0000 - val_loss: 0.6200 - val_accuracy: 1.0000\n",
            "Epoch 67/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6597 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 1.0000\n",
            "Epoch 68/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6485 - accuracy: 1.0000 - val_loss: 0.6141 - val_accuracy: 1.0000\n",
            "Epoch 69/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6576 - accuracy: 1.0000 - val_loss: 0.6379 - val_accuracy: 1.0000\n",
            "Epoch 70/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6473 - accuracy: 1.0000 - val_loss: 0.6452 - val_accuracy: 1.0000\n",
            "Epoch 71/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6740 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 1.0000\n",
            "Epoch 72/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6527 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 1.0000\n",
            "Epoch 73/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6427 - accuracy: 1.0000 - val_loss: 0.7128 - val_accuracy: 1.0000\n",
            "Epoch 74/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6675 - accuracy: 1.0000 - val_loss: 0.6876 - val_accuracy: 1.0000\n",
            "Epoch 75/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6830 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 1.0000\n",
            "Epoch 76/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6176 - accuracy: 1.0000 - val_loss: 0.6498 - val_accuracy: 1.0000\n",
            "Epoch 77/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6668 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 1.0000\n",
            "Epoch 78/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6717 - accuracy: 1.0000 - val_loss: 1.0044 - val_accuracy: 1.0000\n",
            "Epoch 79/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6257 - val_accuracy: 1.0000\n",
            "Epoch 80/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6578 - accuracy: 1.0000 - val_loss: 0.7605 - val_accuracy: 1.0000\n",
            "Epoch 81/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6618 - accuracy: 1.0000 - val_loss: 0.6414 - val_accuracy: 1.0000\n",
            "Epoch 82/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6905 - accuracy: 1.0000 - val_loss: 0.6237 - val_accuracy: 1.0000\n",
            "Epoch 83/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6136 - accuracy: 1.0000 - val_loss: 0.6230 - val_accuracy: 1.0000\n",
            "Epoch 84/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7017 - accuracy: 1.0000 - val_loss: 0.6103 - val_accuracy: 1.0000\n",
            "Epoch 85/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6404 - accuracy: 1.0000 - val_loss: 0.6169 - val_accuracy: 1.0000\n",
            "Epoch 86/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6731 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 1.0000\n",
            "Epoch 87/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.6288 - val_accuracy: 1.0000\n",
            "Epoch 88/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6675 - accuracy: 1.0000 - val_loss: 0.7167 - val_accuracy: 1.0000\n",
            "Epoch 89/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 0.8141 - val_accuracy: 1.0000\n",
            "Epoch 90/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6544 - accuracy: 1.0000 - val_loss: 0.6342 - val_accuracy: 1.0000\n",
            "Epoch 91/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6613 - accuracy: 1.0000 - val_loss: 0.6172 - val_accuracy: 1.0000\n",
            "Epoch 92/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6713 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 1.0000\n",
            "Epoch 93/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6472 - accuracy: 1.0000 - val_loss: 0.6342 - val_accuracy: 1.0000\n",
            "Epoch 94/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6630 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 1.0000\n",
            "Epoch 95/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6447 - accuracy: 1.0000 - val_loss: 0.6294 - val_accuracy: 1.0000\n",
            "Epoch 96/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6537 - accuracy: 1.0000 - val_loss: 0.6500 - val_accuracy: 1.0000\n",
            "Epoch 97/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6569 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 1.0000\n",
            "Epoch 98/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6517 - accuracy: 1.0000 - val_loss: 0.6290 - val_accuracy: 1.0000\n",
            "Epoch 99/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
            "Epoch 100/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6835 - accuracy: 1.0000 - val_loss: 0.6629 - val_accuracy: 1.0000\n",
            "Epoch 101/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.7272 - val_accuracy: 1.0000\n",
            "Epoch 102/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6724 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 1.0000\n",
            "Epoch 103/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6934 - accuracy: 1.0000 - val_loss: 0.6506 - val_accuracy: 1.0000\n",
            "Epoch 104/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6179 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 1.0000\n",
            "Epoch 105/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6855 - accuracy: 1.0000 - val_loss: 0.7283 - val_accuracy: 1.0000\n",
            "Epoch 106/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6528 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 1.0000\n",
            "Epoch 107/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6294 - accuracy: 1.0000 - val_loss: 0.6730 - val_accuracy: 1.0000\n",
            "Epoch 108/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6591 - accuracy: 1.0000 - val_loss: 0.6537 - val_accuracy: 1.0000\n",
            "Epoch 109/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6545 - accuracy: 1.0000 - val_loss: 0.6879 - val_accuracy: 1.0000\n",
            "Epoch 110/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6589 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 111/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6568 - accuracy: 1.0000 - val_loss: 0.6378 - val_accuracy: 1.0000\n",
            "Epoch 112/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6549 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 1.0000\n",
            "Epoch 113/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6388 - val_accuracy: 1.0000\n",
            "Epoch 114/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6479 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 1.0000\n",
            "Epoch 115/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6305 - val_accuracy: 1.0000\n",
            "Epoch 116/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6622 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 1.0000\n",
            "Epoch 117/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 1.0000 - val_loss: 0.6163 - val_accuracy: 1.0000\n",
            "Epoch 118/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6804 - accuracy: 1.0000 - val_loss: 0.8241 - val_accuracy: 1.0000\n",
            "Epoch 119/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6445 - accuracy: 1.0000 - val_loss: 0.6306 - val_accuracy: 1.0000\n",
            "Epoch 120/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6612 - accuracy: 1.0000 - val_loss: 0.6252 - val_accuracy: 1.0000\n",
            "Epoch 121/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6572 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 122/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 1.0000\n",
            "Epoch 123/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6286 - val_accuracy: 1.0000\n",
            "Epoch 124/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6551 - accuracy: 1.0000 - val_loss: 0.6332 - val_accuracy: 1.0000\n",
            "Epoch 125/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7396 - accuracy: 1.0000 - val_loss: 0.6727 - val_accuracy: 1.0000\n",
            "Epoch 126/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6110 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 1.0000\n",
            "Epoch 127/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6440 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 1.0000\n",
            "Epoch 128/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6511 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 129/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6688 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 1.0000\n",
            "Epoch 130/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6438 - accuracy: 1.0000 - val_loss: 0.6202 - val_accuracy: 1.0000\n",
            "Epoch 131/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6393 - accuracy: 1.0000 - val_loss: 0.6501 - val_accuracy: 1.0000\n",
            "Epoch 132/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6709 - accuracy: 1.0000 - val_loss: 0.6461 - val_accuracy: 1.0000\n",
            "Epoch 133/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6521 - accuracy: 1.0000 - val_loss: 0.6353 - val_accuracy: 1.0000\n",
            "Epoch 134/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6563 - accuracy: 1.0000 - val_loss: 0.6333 - val_accuracy: 1.0000\n",
            "Epoch 135/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6735 - accuracy: 1.0000 - val_loss: 0.6320 - val_accuracy: 1.0000\n",
            "Epoch 136/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6329 - accuracy: 1.0000 - val_loss: 0.6687 - val_accuracy: 1.0000\n",
            "Epoch 137/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6651 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 1.0000\n",
            "Epoch 138/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6737 - accuracy: 1.0000 - val_loss: 0.8525 - val_accuracy: 1.0000\n",
            "Epoch 139/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6286 - accuracy: 1.0000 - val_loss: 0.6172 - val_accuracy: 1.0000\n",
            "Epoch 140/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6649 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 141/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6639 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 142/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6351 - accuracy: 1.0000 - val_loss: 0.8309 - val_accuracy: 1.0000\n",
            "Epoch 143/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6458 - accuracy: 1.0000 - val_loss: 1.1347 - val_accuracy: 1.0000\n",
            "Epoch 144/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6860 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 1.0000\n",
            "Epoch 145/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6627 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 1.0000\n",
            "Epoch 146/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6246 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 1.0000\n",
            "Epoch 147/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6843 - accuracy: 1.0000 - val_loss: 0.6399 - val_accuracy: 1.0000\n",
            "Epoch 148/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6605 - accuracy: 1.0000 - val_loss: 0.6317 - val_accuracy: 1.0000\n",
            "Epoch 149/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6265 - accuracy: 1.0000 - val_loss: 0.7628 - val_accuracy: 1.0000\n",
            "Epoch 150/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6587 - accuracy: 1.0000 - val_loss: 0.6154 - val_accuracy: 1.0000\n",
            "Epoch 151/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7165 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 1.0000\n",
            "Epoch 152/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6435 - accuracy: 1.0000 - val_loss: 1.2887 - val_accuracy: 1.0000\n",
            "Epoch 153/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6330 - accuracy: 1.0000 - val_loss: 0.6112 - val_accuracy: 1.0000\n",
            "Epoch 154/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6471 - accuracy: 1.0000 - val_loss: 0.6978 - val_accuracy: 1.0000\n",
            "Epoch 155/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6592 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 1.0000\n",
            "Epoch 156/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6876 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 157/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6215 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 1.0000\n",
            "Epoch 158/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6542 - accuracy: 1.0000 - val_loss: 0.7369 - val_accuracy: 1.0000\n",
            "Epoch 159/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6565 - accuracy: 1.0000 - val_loss: 0.7617 - val_accuracy: 1.0000\n",
            "Epoch 160/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6155 - val_accuracy: 1.0000\n",
            "Epoch 161/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6700 - accuracy: 1.0000 - val_loss: 0.6112 - val_accuracy: 1.0000\n",
            "Epoch 162/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6435 - accuracy: 1.0000 - val_loss: 0.6551 - val_accuracy: 1.0000\n",
            "Epoch 163/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6496 - accuracy: 1.0000 - val_loss: 0.6230 - val_accuracy: 1.0000\n",
            "Epoch 164/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6706 - accuracy: 1.0000 - val_loss: 0.6149 - val_accuracy: 1.0000\n",
            "Epoch 165/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6313 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 1.0000\n",
            "Epoch 166/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6545 - accuracy: 1.0000 - val_loss: 0.6716 - val_accuracy: 1.0000\n",
            "Epoch 167/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6534 - accuracy: 1.0000 - val_loss: 0.6356 - val_accuracy: 1.0000\n",
            "Epoch 168/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6624 - accuracy: 1.0000 - val_loss: 0.7103 - val_accuracy: 1.0000\n",
            "Epoch 169/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6531 - accuracy: 1.0000 - val_loss: 0.6544 - val_accuracy: 1.0000\n",
            "Epoch 170/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6651 - accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 1.0000\n",
            "Epoch 171/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6427 - accuracy: 1.0000 - val_loss: 0.6221 - val_accuracy: 1.0000\n",
            "Epoch 172/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7000 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 1.0000\n",
            "Epoch 173/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6187 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 1.0000\n",
            "Epoch 174/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6807 - accuracy: 1.0000 - val_loss: 0.6238 - val_accuracy: 1.0000\n",
            "Epoch 175/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6314 - accuracy: 1.0000 - val_loss: 0.6913 - val_accuracy: 1.0000\n",
            "Epoch 176/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6428 - accuracy: 1.0000 - val_loss: 0.6120 - val_accuracy: 1.0000\n",
            "Epoch 177/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6554 - accuracy: 1.0000 - val_loss: 0.6095 - val_accuracy: 1.0000\n",
            "Epoch 178/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6576 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 1.0000\n",
            "Epoch 179/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6698 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 1.0000\n",
            "Epoch 180/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6528 - accuracy: 1.0000 - val_loss: 0.6296 - val_accuracy: 1.0000\n",
            "Epoch 181/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6564 - accuracy: 1.0000 - val_loss: 0.6276 - val_accuracy: 1.0000\n",
            "Epoch 182/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6536 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 1.0000\n",
            "Epoch 183/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6658 - accuracy: 1.0000 - val_loss: 0.6528 - val_accuracy: 1.0000\n",
            "Epoch 184/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 185/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6509 - accuracy: 1.0000 - val_loss: 0.6176 - val_accuracy: 1.0000\n",
            "Epoch 186/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6483 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 187/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6799 - accuracy: 1.0000 - val_loss: 0.7162 - val_accuracy: 1.0000\n",
            "Epoch 188/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6469 - accuracy: 1.0000 - val_loss: 0.6168 - val_accuracy: 1.0000\n",
            "Epoch 189/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6547 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 1.0000\n",
            "Epoch 190/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 1.0000 - val_loss: 0.6575 - val_accuracy: 1.0000\n",
            "Epoch 191/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6590 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 192/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6575 - accuracy: 1.0000 - val_loss: 0.6106 - val_accuracy: 1.0000\n",
            "Epoch 193/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6502 - accuracy: 1.0000 - val_loss: 0.6237 - val_accuracy: 1.0000\n",
            "Epoch 194/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6474 - accuracy: 1.0000 - val_loss: 0.6621 - val_accuracy: 1.0000\n",
            "Epoch 195/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6644 - accuracy: 1.0000 - val_loss: 0.7528 - val_accuracy: 1.0000\n",
            "Epoch 196/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6510 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 1.0000\n",
            "Epoch 197/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6599 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 1.0000\n",
            "Epoch 198/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6401 - accuracy: 1.0000 - val_loss: 0.6688 - val_accuracy: 1.0000\n",
            "Epoch 199/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6637 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 1.0000\n",
            "Epoch 200/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6593 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 201/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6512 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 1.0000\n",
            "Epoch 202/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6464 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 1.0000\n",
            "Epoch 203/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6768 - accuracy: 1.0000 - val_loss: 0.7233 - val_accuracy: 1.0000\n",
            "Epoch 204/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6246 - accuracy: 1.0000 - val_loss: 0.6300 - val_accuracy: 1.0000\n",
            "Epoch 205/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6673 - accuracy: 1.0000 - val_loss: 0.6584 - val_accuracy: 1.0000\n",
            "Epoch 206/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6534 - accuracy: 1.0000 - val_loss: 0.6753 - val_accuracy: 1.0000\n",
            "Epoch 207/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6294 - val_accuracy: 1.0000\n",
            "Epoch 208/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6604 - accuracy: 1.0000 - val_loss: 0.8713 - val_accuracy: 1.0000\n",
            "Epoch 209/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6618 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 1.0000\n",
            "Epoch 210/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6524 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 1.0000\n",
            "Epoch 211/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.7725 - val_accuracy: 1.0000\n",
            "Epoch 212/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6450 - accuracy: 1.0000 - val_loss: 0.6964 - val_accuracy: 1.0000\n",
            "Epoch 213/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6488 - accuracy: 1.0000 - val_loss: 0.6257 - val_accuracy: 1.0000\n",
            "Epoch 214/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6495 - accuracy: 1.0000 - val_loss: 0.9006 - val_accuracy: 1.0000\n",
            "Epoch 215/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6662 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 216/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6573 - accuracy: 1.0000 - val_loss: 0.7675 - val_accuracy: 1.0000\n",
            "Epoch 217/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6499 - accuracy: 1.0000 - val_loss: 0.6395 - val_accuracy: 1.0000\n",
            "Epoch 218/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6451 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 1.0000\n",
            "Epoch 219/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6546 - accuracy: 1.0000 - val_loss: 0.6263 - val_accuracy: 1.0000\n",
            "Epoch 220/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6813 - accuracy: 1.0000 - val_loss: 0.7042 - val_accuracy: 1.0000\n",
            "Epoch 221/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6228 - accuracy: 1.0000 - val_loss: 0.6353 - val_accuracy: 1.0000\n",
            "Epoch 222/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6581 - accuracy: 1.0000 - val_loss: 0.6262 - val_accuracy: 1.0000\n",
            "Epoch 223/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6659 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 224/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6392 - accuracy: 1.0000 - val_loss: 0.6220 - val_accuracy: 1.0000\n",
            "Epoch 225/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6597 - accuracy: 1.0000 - val_loss: 0.8317 - val_accuracy: 1.0000\n",
            "Epoch 226/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6381 - accuracy: 1.0000 - val_loss: 1.9191 - val_accuracy: 1.0000\n",
            "Epoch 227/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7147 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 1.0000\n",
            "Epoch 228/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6209 - accuracy: 1.0000 - val_loss: 0.6947 - val_accuracy: 1.0000\n",
            "Epoch 229/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6913 - val_accuracy: 1.0000\n",
            "Epoch 230/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6551 - accuracy: 1.0000 - val_loss: 0.6338 - val_accuracy: 1.0000\n",
            "Epoch 231/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6553 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 1.0000\n",
            "Epoch 232/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6772 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 1.0000\n",
            "Epoch 233/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 1.0000\n",
            "Epoch 234/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6566 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 1.0000\n",
            "Epoch 235/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6605 - accuracy: 1.0000 - val_loss: 0.6739 - val_accuracy: 1.0000\n",
            "Epoch 236/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6583 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 1.0000\n",
            "Epoch 237/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6488 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 1.0000\n",
            "Epoch 238/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6664 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 1.0000\n",
            "Epoch 239/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6585 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 1.0000\n",
            "Epoch 240/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6263 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 1.0000\n",
            "Epoch 241/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6562 - accuracy: 1.0000 - val_loss: 0.6313 - val_accuracy: 1.0000\n",
            "Epoch 242/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6626 - accuracy: 1.0000 - val_loss: 0.6300 - val_accuracy: 1.0000\n",
            "Epoch 243/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6591 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 244/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6378 - accuracy: 1.0000 - val_loss: 0.7170 - val_accuracy: 1.0000\n",
            "Epoch 245/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6643 - accuracy: 1.0000 - val_loss: 0.7203 - val_accuracy: 1.0000\n",
            "Epoch 246/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6761 - accuracy: 1.0000 - val_loss: 0.6198 - val_accuracy: 1.0000\n",
            "Epoch 247/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6327 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 1.0000\n",
            "Epoch 248/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6419 - accuracy: 1.0000 - val_loss: 0.6414 - val_accuracy: 1.0000\n",
            "Epoch 249/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6554 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 1.0000\n",
            "Epoch 250/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6690 - accuracy: 1.0000 - val_loss: 0.6443 - val_accuracy: 1.0000\n",
            "Epoch 251/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6588 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 1.0000\n",
            "Epoch 252/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6322 - accuracy: 1.0000 - val_loss: 0.8223 - val_accuracy: 1.0000\n",
            "Epoch 253/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6652 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 254/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6421 - val_accuracy: 1.0000\n",
            "Epoch 255/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6353 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 1.0000\n",
            "Epoch 256/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6787 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 1.0000\n",
            "Epoch 257/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6293 - accuracy: 1.0000 - val_loss: 0.9176 - val_accuracy: 1.0000\n",
            "Epoch 258/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6677 - accuracy: 1.0000 - val_loss: 0.6427 - val_accuracy: 1.0000\n",
            "Epoch 259/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6625 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 1.0000\n",
            "Epoch 260/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6394 - accuracy: 1.0000 - val_loss: 0.6241 - val_accuracy: 1.0000\n",
            "Epoch 261/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6415 - accuracy: 1.0000 - val_loss: 0.6528 - val_accuracy: 1.0000\n",
            "Epoch 262/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6489 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 1.0000\n",
            "Epoch 263/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6566 - accuracy: 1.0000 - val_loss: 0.6268 - val_accuracy: 1.0000\n",
            "Epoch 264/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6615 - accuracy: 1.0000 - val_loss: 1.2015 - val_accuracy: 1.0000\n",
            "Epoch 265/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6172 - val_accuracy: 1.0000\n",
            "Epoch 266/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6672 - accuracy: 1.0000 - val_loss: 0.7365 - val_accuracy: 1.0000\n",
            "Epoch 267/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.6406 - accuracy: 1.0000 - val_loss: 0.6218 - val_accuracy: 1.0000\n",
            "Epoch 268/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6491 - accuracy: 1.0000 - val_loss: 0.6829 - val_accuracy: 1.0000\n",
            "Epoch 269/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.6560 - accuracy: 1.0000 - val_loss: 0.6130 - val_accuracy: 1.0000\n",
            "Epoch 270/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6623 - accuracy: 1.0000 - val_loss: 0.6217 - val_accuracy: 1.0000\n",
            "Epoch 271/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 1.0000\n",
            "Epoch 272/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.6622 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
            "Epoch 273/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.6115 - val_accuracy: 1.0000\n",
            "Epoch 274/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6633 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
            "Epoch 275/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6921 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 1.0000\n",
            "Epoch 276/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6144 - accuracy: 1.0000 - val_loss: 0.6918 - val_accuracy: 1.0000\n",
            "Epoch 277/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7074 - accuracy: 1.0000 - val_loss: 0.6390 - val_accuracy: 1.0000\n",
            "Epoch 278/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6192 - accuracy: 1.0000 - val_loss: 0.6361 - val_accuracy: 1.0000\n",
            "Epoch 279/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6643 - accuracy: 1.0000 - val_loss: 0.6100 - val_accuracy: 1.0000\n",
            "Epoch 280/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6766 - accuracy: 1.0000 - val_loss: 0.6149 - val_accuracy: 1.0000\n",
            "Epoch 281/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6203 - accuracy: 1.0000 - val_loss: 0.7831 - val_accuracy: 1.0000\n",
            "Epoch 282/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6679 - accuracy: 1.0000 - val_loss: 0.6066 - val_accuracy: 1.0000\n",
            "Epoch 283/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6467 - accuracy: 1.0000 - val_loss: 0.6228 - val_accuracy: 1.0000\n",
            "Epoch 284/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6489 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 1.0000\n",
            "Epoch 285/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6486 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 1.0000\n",
            "Epoch 286/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6675 - accuracy: 1.0000 - val_loss: 0.7135 - val_accuracy: 1.0000\n",
            "Epoch 287/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6521 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 288/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6471 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 1.0000\n",
            "Epoch 289/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6462 - accuracy: 1.0000 - val_loss: 0.6100 - val_accuracy: 1.0000\n",
            "Epoch 290/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 1.0000\n",
            "Epoch 291/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6454 - accuracy: 1.0000 - val_loss: 0.6165 - val_accuracy: 1.0000\n",
            "Epoch 292/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6829 - accuracy: 1.0000 - val_loss: 0.7706 - val_accuracy: 1.0000\n",
            "Epoch 293/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6170 - accuracy: 1.0000 - val_loss: 0.6262 - val_accuracy: 1.0000\n",
            "Epoch 294/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6521 - accuracy: 1.0000 - val_loss: 0.6685 - val_accuracy: 1.0000\n",
            "Epoch 295/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 1.0000\n",
            "Epoch 296/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 1.0000\n",
            "Epoch 297/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6544 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 1.0000\n",
            "Epoch 298/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6576 - accuracy: 1.0000 - val_loss: 0.7403 - val_accuracy: 1.0000\n",
            "Epoch 299/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6403 - accuracy: 1.0000 - val_loss: 0.8356 - val_accuracy: 1.0000\n",
            "Epoch 300/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6805 - accuracy: 1.0000 - val_loss: 0.7484 - val_accuracy: 1.0000\n",
            "Epoch 301/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6322 - accuracy: 1.0000 - val_loss: 0.6445 - val_accuracy: 1.0000\n",
            "Epoch 302/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7055 - accuracy: 1.0000 - val_loss: 0.7601 - val_accuracy: 1.0000\n",
            "Epoch 303/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6152 - accuracy: 1.0000 - val_loss: 0.6107 - val_accuracy: 1.0000\n",
            "Epoch 304/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6429 - accuracy: 1.0000 - val_loss: 0.6172 - val_accuracy: 1.0000\n",
            "Epoch 305/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6450 - accuracy: 1.0000 - val_loss: 0.6842 - val_accuracy: 1.0000\n",
            "Epoch 306/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6528 - accuracy: 1.0000 - val_loss: 0.6235 - val_accuracy: 1.0000\n",
            "Epoch 307/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6534 - accuracy: 1.0000 - val_loss: 0.6263 - val_accuracy: 1.0000\n",
            "Epoch 308/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6597 - accuracy: 1.0000 - val_loss: 0.6117 - val_accuracy: 1.0000\n",
            "Epoch 309/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6784 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 1.0000\n",
            "Epoch 310/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6200 - accuracy: 1.0000 - val_loss: 0.7110 - val_accuracy: 1.0000\n",
            "Epoch 311/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6667 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 312/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6351 - accuracy: 1.0000 - val_loss: 0.6456 - val_accuracy: 1.0000\n",
            "Epoch 313/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6565 - accuracy: 1.0000 - val_loss: 0.6295 - val_accuracy: 1.0000\n",
            "Epoch 314/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6503 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 1.0000\n",
            "Epoch 315/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6727 - accuracy: 1.0000 - val_loss: 0.6100 - val_accuracy: 1.0000\n",
            "Epoch 316/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6283 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 1.0000\n",
            "Epoch 317/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6652 - accuracy: 1.0000 - val_loss: 0.6400 - val_accuracy: 1.0000\n",
            "Epoch 318/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6484 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 1.0000\n",
            "Epoch 319/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6799 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 320/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6138 - accuracy: 1.0000 - val_loss: 0.6649 - val_accuracy: 1.0000\n",
            "Epoch 321/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6543 - accuracy: 1.0000 - val_loss: 0.6141 - val_accuracy: 1.0000\n",
            "Epoch 322/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6496 - accuracy: 1.0000 - val_loss: 0.6433 - val_accuracy: 1.0000\n",
            "Epoch 323/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6696 - accuracy: 1.0000 - val_loss: 0.6546 - val_accuracy: 1.0000\n",
            "Epoch 324/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6360 - accuracy: 1.0000 - val_loss: 0.6399 - val_accuracy: 1.0000\n",
            "Epoch 325/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6573 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 1.0000\n",
            "Epoch 326/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 1.0000\n",
            "Epoch 327/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6776 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 1.0000\n",
            "Epoch 328/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6753 - accuracy: 1.0000 - val_loss: 1.6605 - val_accuracy: 1.0000\n",
            "Epoch 329/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 1.0000\n",
            "Epoch 330/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6204 - accuracy: 1.0000 - val_loss: 0.6182 - val_accuracy: 1.0000\n",
            "Epoch 331/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 332/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6503 - accuracy: 1.0000 - val_loss: 0.6255 - val_accuracy: 1.0000\n",
            "Epoch 333/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6620 - accuracy: 1.0000 - val_loss: 0.6215 - val_accuracy: 1.0000\n",
            "Epoch 334/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7062 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 1.0000\n",
            "Epoch 335/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6125 - accuracy: 1.0000 - val_loss: 0.6246 - val_accuracy: 1.0000\n",
            "Epoch 336/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6342 - accuracy: 1.0000 - val_loss: 0.6621 - val_accuracy: 1.0000\n",
            "Epoch 337/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6554 - accuracy: 1.0000 - val_loss: 0.6701 - val_accuracy: 1.0000\n",
            "Epoch 338/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6735 - accuracy: 1.0000 - val_loss: 0.6286 - val_accuracy: 1.0000\n",
            "Epoch 339/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6209 - accuracy: 1.0000 - val_loss: 0.7593 - val_accuracy: 1.0000\n",
            "Epoch 340/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6628 - accuracy: 1.0000 - val_loss: 0.6777 - val_accuracy: 1.0000\n",
            "Epoch 341/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6496 - accuracy: 1.0000 - val_loss: 0.6191 - val_accuracy: 1.0000\n",
            "Epoch 342/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6492 - accuracy: 1.0000 - val_loss: 0.6483 - val_accuracy: 1.0000\n",
            "Epoch 343/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6424 - accuracy: 1.0000 - val_loss: 0.7818 - val_accuracy: 1.0000\n",
            "Epoch 344/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6382 - val_accuracy: 1.0000\n",
            "Epoch 345/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6487 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 1.0000\n",
            "Epoch 346/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6564 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 1.0000\n",
            "Epoch 347/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6734 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 1.0000\n",
            "Epoch 348/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6445 - accuracy: 1.0000 - val_loss: 0.6179 - val_accuracy: 1.0000\n",
            "Epoch 349/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6624 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 1.0000\n",
            "Epoch 350/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 0.6453 - val_accuracy: 1.0000\n",
            "Epoch 351/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6790 - accuracy: 1.0000 - val_loss: 0.6323 - val_accuracy: 1.0000\n",
            "Epoch 352/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6150 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 353/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6516 - accuracy: 1.0000 - val_loss: 0.6397 - val_accuracy: 1.0000\n",
            "Epoch 354/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6742 - accuracy: 1.0000 - val_loss: 0.6096 - val_accuracy: 1.0000\n",
            "Epoch 355/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6454 - accuracy: 1.0000 - val_loss: 0.6253 - val_accuracy: 1.0000\n",
            "Epoch 356/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6345 - accuracy: 1.0000 - val_loss: 0.6225 - val_accuracy: 1.0000\n",
            "Epoch 357/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6590 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 1.0000\n",
            "Epoch 358/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6625 - accuracy: 1.0000 - val_loss: 0.6736 - val_accuracy: 1.0000\n",
            "Epoch 359/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.6537 - accuracy: 1.0000 - val_loss: 0.6085 - val_accuracy: 1.0000\n",
            "Epoch 360/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.6219 - val_accuracy: 1.0000\n",
            "Epoch 361/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6407 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 1.0000\n",
            "Epoch 362/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6674 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 1.0000\n",
            "Epoch 363/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6649 - accuracy: 1.0000 - val_loss: 0.6052 - val_accuracy: 1.0000\n",
            "Epoch 364/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6239 - accuracy: 1.0000 - val_loss: 0.6220 - val_accuracy: 1.0000\n",
            "Epoch 365/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6536 - accuracy: 1.0000 - val_loss: 0.6899 - val_accuracy: 1.0000\n",
            "Epoch 366/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6495 - accuracy: 1.0000 - val_loss: 0.8794 - val_accuracy: 1.0000\n",
            "Epoch 367/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6463 - accuracy: 1.0000 - val_loss: 0.6335 - val_accuracy: 1.0000\n",
            "Epoch 368/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6710 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 1.0000\n",
            "Epoch 369/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6360 - accuracy: 1.0000 - val_loss: 0.6314 - val_accuracy: 1.0000\n",
            "Epoch 370/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6686 - accuracy: 1.0000 - val_loss: 0.6961 - val_accuracy: 1.0000\n",
            "Epoch 371/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6359 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 1.0000\n",
            "Epoch 372/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6755 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 373/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6308 - accuracy: 1.0000 - val_loss: 0.6476 - val_accuracy: 1.0000\n",
            "Epoch 374/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6594 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 1.0000\n",
            "Epoch 375/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6703 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 376/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6290 - accuracy: 1.0000 - val_loss: 0.6378 - val_accuracy: 1.0000\n",
            "Epoch 377/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6397 - accuracy: 1.0000 - val_loss: 0.6814 - val_accuracy: 1.0000\n",
            "Epoch 378/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6576 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 1.0000\n",
            "Epoch 379/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6581 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 1.0000\n",
            "Epoch 380/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6649 - accuracy: 1.0000 - val_loss: 0.6293 - val_accuracy: 1.0000\n",
            "Epoch 381/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6388 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 1.0000\n",
            "Epoch 382/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7056 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 1.0000\n",
            "Epoch 383/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6114 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 1.0000\n",
            "Epoch 384/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.7219 - val_accuracy: 1.0000\n",
            "Epoch 385/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6076 - val_accuracy: 1.0000\n",
            "Epoch 386/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6493 - accuracy: 1.0000 - val_loss: 0.6179 - val_accuracy: 1.0000\n",
            "Epoch 387/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6985 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 1.0000\n",
            "Epoch 388/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6100 - accuracy: 1.0000 - val_loss: 0.6918 - val_accuracy: 1.0000\n",
            "Epoch 389/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6560 - accuracy: 1.0000 - val_loss: 0.6076 - val_accuracy: 1.0000\n",
            "Epoch 390/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6588 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 1.0000\n",
            "Epoch 391/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.6202 - val_accuracy: 1.0000\n",
            "Epoch 392/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6527 - accuracy: 1.0000 - val_loss: 0.6200 - val_accuracy: 1.0000\n",
            "Epoch 393/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6566 - accuracy: 1.0000 - val_loss: 0.6191 - val_accuracy: 1.0000\n",
            "Epoch 394/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6612 - accuracy: 1.0000 - val_loss: 0.6176 - val_accuracy: 1.0000\n",
            "Epoch 395/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6277 - accuracy: 1.0000 - val_loss: 0.6433 - val_accuracy: 1.0000\n",
            "Epoch 396/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6551 - accuracy: 1.0000 - val_loss: 0.6698 - val_accuracy: 1.0000\n",
            "Epoch 397/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6672 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 1.0000\n",
            "Epoch 398/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6362 - accuracy: 1.0000 - val_loss: 0.6187 - val_accuracy: 1.0000\n",
            "Epoch 399/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6482 - accuracy: 1.0000 - val_loss: 1.0935 - val_accuracy: 1.0000\n",
            "Epoch 400/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6485 - accuracy: 1.0000 - val_loss: 0.6437 - val_accuracy: 1.0000\n",
            "Epoch 401/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6759 - accuracy: 1.0000 - val_loss: 0.6109 - val_accuracy: 1.0000\n",
            "Epoch 402/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6217 - accuracy: 1.0000 - val_loss: 0.8092 - val_accuracy: 1.0000\n",
            "Epoch 403/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6546 - accuracy: 1.0000 - val_loss: 0.7216 - val_accuracy: 1.0000\n",
            "Epoch 404/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6473 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 405/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6450 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 1.0000\n",
            "Epoch 406/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6531 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 1.0000\n",
            "Epoch 407/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6547 - accuracy: 1.0000 - val_loss: 0.7472 - val_accuracy: 1.0000\n",
            "Epoch 408/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6601 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 1.0000\n",
            "Epoch 409/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6403 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 1.0000\n",
            "Epoch 410/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6388 - accuracy: 1.0000 - val_loss: 1.3378 - val_accuracy: 1.0000\n",
            "Epoch 411/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6597 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 1.0000\n",
            "Epoch 412/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6586 - accuracy: 1.0000 - val_loss: 0.6238 - val_accuracy: 1.0000\n",
            "Epoch 413/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6677 - accuracy: 1.0000 - val_loss: 1.0754 - val_accuracy: 1.0000\n",
            "Epoch 414/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6354 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 415/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6371 - accuracy: 1.0000 - val_loss: 0.6767 - val_accuracy: 1.0000\n",
            "Epoch 416/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6469 - accuracy: 1.0000 - val_loss: 0.6636 - val_accuracy: 1.0000\n",
            "Epoch 417/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6941 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 1.0000\n",
            "Epoch 418/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6222 - accuracy: 1.0000 - val_loss: 0.8029 - val_accuracy: 1.0000\n",
            "Epoch 419/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6445 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 420/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6916 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 421/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6360 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 1.0000\n",
            "Epoch 422/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6358 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 1.0000\n",
            "Epoch 423/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6623 - accuracy: 1.0000 - val_loss: 0.9223 - val_accuracy: 1.0000\n",
            "Epoch 424/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.7123 - val_accuracy: 1.0000\n",
            "Epoch 425/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6517 - accuracy: 1.0000 - val_loss: 0.6167 - val_accuracy: 1.0000\n",
            "Epoch 426/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6517 - accuracy: 1.0000 - val_loss: 0.9992 - val_accuracy: 1.0000\n",
            "Epoch 427/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6737 - accuracy: 1.0000 - val_loss: 0.6165 - val_accuracy: 1.0000\n",
            "Epoch 428/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6149 - accuracy: 1.0000 - val_loss: 0.6171 - val_accuracy: 1.0000\n",
            "Epoch 429/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7004 - accuracy: 1.0000 - val_loss: 0.6318 - val_accuracy: 1.0000\n",
            "Epoch 430/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6130 - accuracy: 1.0000 - val_loss: 0.6129 - val_accuracy: 1.0000\n",
            "Epoch 431/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6397 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 1.0000\n",
            "Epoch 432/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6490 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 1.0000\n",
            "Epoch 433/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6630 - accuracy: 1.0000 - val_loss: 0.6231 - val_accuracy: 1.0000\n",
            "Epoch 434/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6413 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 1.0000\n",
            "Epoch 435/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6612 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 1.0000\n",
            "Epoch 436/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6610 - accuracy: 1.0000 - val_loss: 0.6146 - val_accuracy: 1.0000\n",
            "Epoch 437/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6471 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 1.0000\n",
            "Epoch 438/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6410 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
            "Epoch 439/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6978 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 1.0000\n",
            "Epoch 440/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6116 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 1.0000\n",
            "Epoch 441/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6361 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 1.0000\n",
            "Epoch 442/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6869 - accuracy: 1.0000 - val_loss: 0.6171 - val_accuracy: 1.0000\n",
            "Epoch 443/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6234 - accuracy: 1.0000 - val_loss: 1.1482 - val_accuracy: 1.0000\n",
            "Epoch 444/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.6746 - val_accuracy: 1.0000\n",
            "Epoch 445/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6480 - accuracy: 1.0000 - val_loss: 0.6246 - val_accuracy: 1.0000\n",
            "Epoch 446/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6510 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 1.0000\n",
            "Epoch 447/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6542 - accuracy: 1.0000 - val_loss: 0.6384 - val_accuracy: 1.0000\n",
            "Epoch 448/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6499 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 1.0000\n",
            "Epoch 449/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6656 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 1.0000\n",
            "Epoch 450/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6567 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 1.0000\n",
            "Epoch 451/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6409 - accuracy: 1.0000 - val_loss: 0.6910 - val_accuracy: 1.0000\n",
            "Epoch 452/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6453 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 453/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6134 - val_accuracy: 1.0000\n",
            "Epoch 454/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6496 - accuracy: 1.0000 - val_loss: 0.9418 - val_accuracy: 1.0000\n",
            "Epoch 455/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6408 - accuracy: 1.0000 - val_loss: 0.6280 - val_accuracy: 1.0000\n",
            "Epoch 456/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6774 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 1.0000\n",
            "Epoch 457/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6280 - accuracy: 1.0000 - val_loss: 0.9192 - val_accuracy: 1.0000\n",
            "Epoch 458/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6548 - accuracy: 1.0000 - val_loss: 0.6632 - val_accuracy: 1.0000\n",
            "Epoch 459/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 1.0000\n",
            "Epoch 460/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.6177 - val_accuracy: 1.0000\n",
            "Epoch 461/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6409 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 1.0000\n",
            "Epoch 462/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6600 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 1.0000\n",
            "Epoch 463/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6425 - accuracy: 1.0000 - val_loss: 1.6115 - val_accuracy: 1.0000\n",
            "Epoch 464/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6499 - accuracy: 1.0000 - val_loss: 0.6221 - val_accuracy: 1.0000\n",
            "Epoch 465/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6544 - accuracy: 1.0000 - val_loss: 0.6305 - val_accuracy: 1.0000\n",
            "Epoch 466/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6488 - accuracy: 1.0000 - val_loss: 0.6236 - val_accuracy: 1.0000\n",
            "Epoch 467/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6324 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 468/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6724 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 1.0000\n",
            "Epoch 469/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6288 - accuracy: 1.0000 - val_loss: 0.6422 - val_accuracy: 1.0000\n",
            "Epoch 470/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6587 - accuracy: 1.0000 - val_loss: 0.6571 - val_accuracy: 1.0000\n",
            "Epoch 471/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6497 - accuracy: 1.0000 - val_loss: 0.7845 - val_accuracy: 1.0000\n",
            "Epoch 472/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.7069 - val_accuracy: 1.0000\n",
            "Epoch 473/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 1.1629 - val_accuracy: 1.0000\n",
            "Epoch 474/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6435 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 475/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6497 - accuracy: 1.0000 - val_loss: 0.7667 - val_accuracy: 1.0000\n",
            "Epoch 476/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6489 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 1.0000\n",
            "Epoch 477/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6473 - accuracy: 1.0000 - val_loss: 0.6267 - val_accuracy: 1.0000\n",
            "Epoch 478/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6485 - accuracy: 1.0000 - val_loss: 0.6477 - val_accuracy: 1.0000\n",
            "Epoch 479/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6714 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 480/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6616 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 481/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6517 - accuracy: 1.0000 - val_loss: 0.7542 - val_accuracy: 1.0000\n",
            "Epoch 482/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6639 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 1.0000\n",
            "Epoch 483/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6611 - accuracy: 1.0000 - val_loss: 0.6638 - val_accuracy: 1.0000\n",
            "Epoch 484/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6401 - accuracy: 1.0000 - val_loss: 0.7430 - val_accuracy: 1.0000\n",
            "Epoch 485/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6369 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 1.0000\n",
            "Epoch 486/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6431 - accuracy: 1.0000 - val_loss: 0.6727 - val_accuracy: 1.0000\n",
            "Epoch 487/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6191 - val_accuracy: 1.0000\n",
            "Epoch 488/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6641 - accuracy: 1.0000 - val_loss: 0.8742 - val_accuracy: 1.0000\n",
            "Epoch 489/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6376 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 1.0000\n",
            "Epoch 490/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6462 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 1.0000\n",
            "Epoch 491/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6690 - accuracy: 1.0000 - val_loss: 0.6479 - val_accuracy: 1.0000\n",
            "Epoch 492/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6289 - accuracy: 1.0000 - val_loss: 0.7244 - val_accuracy: 1.0000\n",
            "Epoch 493/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6446 - accuracy: 1.0000 - val_loss: 0.7177 - val_accuracy: 1.0000\n",
            "Epoch 494/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6660 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 1.0000\n",
            "Epoch 495/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6265 - accuracy: 1.0000 - val_loss: 0.6320 - val_accuracy: 1.0000\n",
            "Epoch 496/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6545 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 1.0000\n",
            "Epoch 497/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6469 - accuracy: 1.0000 - val_loss: 0.6400 - val_accuracy: 1.0000\n",
            "Epoch 498/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6530 - accuracy: 1.0000 - val_loss: 0.6373 - val_accuracy: 1.0000\n",
            "Epoch 499/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6625 - accuracy: 1.0000 - val_loss: 0.6390 - val_accuracy: 1.0000\n",
            "Epoch 500/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6399 - val_accuracy: 1.0000\n",
            "Epoch 501/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6841 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 1.0000\n",
            "Epoch 502/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6111 - accuracy: 1.0000 - val_loss: 0.6262 - val_accuracy: 1.0000\n",
            "Epoch 503/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6693 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 1.0000\n",
            "Epoch 504/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6866 - accuracy: 1.0000 - val_loss: 0.7076 - val_accuracy: 1.0000\n",
            "Epoch 505/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6100 - accuracy: 1.0000 - val_loss: 0.6731 - val_accuracy: 1.0000\n",
            "Epoch 506/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 1.0000\n",
            "Epoch 507/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6478 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 1.0000\n",
            "Epoch 508/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6480 - accuracy: 1.0000 - val_loss: 0.6877 - val_accuracy: 1.0000\n",
            "Epoch 509/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6613 - accuracy: 1.0000 - val_loss: 2.0978 - val_accuracy: 1.0000\n",
            "Epoch 510/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6466 - accuracy: 1.0000 - val_loss: 0.6351 - val_accuracy: 1.0000\n",
            "Epoch 511/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6415 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 1.0000\n",
            "Epoch 512/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6798 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 1.0000\n",
            "Epoch 513/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6210 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 1.0000\n",
            "Epoch 514/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6407 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 1.0000\n",
            "Epoch 515/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6714 - accuracy: 1.0000 - val_loss: 0.6307 - val_accuracy: 1.0000\n",
            "Epoch 516/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6420 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 517/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6481 - accuracy: 1.0000 - val_loss: 0.6166 - val_accuracy: 1.0000\n",
            "Epoch 518/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 1.0000\n",
            "Epoch 519/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6629 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
            "Epoch 520/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6279 - accuracy: 1.0000 - val_loss: 0.6253 - val_accuracy: 1.0000\n",
            "Epoch 521/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6472 - accuracy: 1.0000 - val_loss: 0.7510 - val_accuracy: 1.0000\n",
            "Epoch 522/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7121 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 1.0000\n",
            "Epoch 523/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6081 - accuracy: 1.0000 - val_loss: 0.7565 - val_accuracy: 1.0000\n",
            "Epoch 524/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6811 - accuracy: 1.0000 - val_loss: 0.6659 - val_accuracy: 1.0000\n",
            "Epoch 525/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6121 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 1.0000\n",
            "Epoch 526/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6628 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 1.0000\n",
            "Epoch 527/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6246 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 528/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6651 - accuracy: 1.0000 - val_loss: 0.6310 - val_accuracy: 1.0000\n",
            "Epoch 529/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6319 - accuracy: 1.0000 - val_loss: 0.6211 - val_accuracy: 1.0000\n",
            "Epoch 530/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6834 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 531/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 1.0000\n",
            "Epoch 532/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6402 - accuracy: 1.0000 - val_loss: 0.6213 - val_accuracy: 1.0000\n",
            "Epoch 533/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6529 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 1.0000\n",
            "Epoch 534/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6683 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 1.0000\n",
            "Epoch 535/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6182 - accuracy: 1.0000 - val_loss: 0.6239 - val_accuracy: 1.0000\n",
            "Epoch 536/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6546 - accuracy: 1.0000 - val_loss: 0.6535 - val_accuracy: 1.0000\n",
            "Epoch 537/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6484 - accuracy: 1.0000 - val_loss: 0.7761 - val_accuracy: 1.0000\n",
            "Epoch 538/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6447 - accuracy: 1.0000 - val_loss: 0.7992 - val_accuracy: 1.0000\n",
            "Epoch 539/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.6718 - val_accuracy: 1.0000\n",
            "Epoch 540/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 1.0000\n",
            "Epoch 541/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6596 - accuracy: 1.0000 - val_loss: 0.6671 - val_accuracy: 1.0000\n",
            "Epoch 542/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6429 - accuracy: 1.0000 - val_loss: 0.6465 - val_accuracy: 1.0000\n",
            "Epoch 543/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6527 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 1.0000\n",
            "Epoch 544/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6419 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 1.0000\n",
            "Epoch 545/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6774 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 546/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6218 - accuracy: 1.0000 - val_loss: 0.6306 - val_accuracy: 1.0000\n",
            "Epoch 547/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6780 - accuracy: 1.0000 - val_loss: 0.6398 - val_accuracy: 1.0000\n",
            "Epoch 548/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6333 - accuracy: 1.0000 - val_loss: 0.6230 - val_accuracy: 1.0000\n",
            "Epoch 549/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6549 - accuracy: 1.0000 - val_loss: 0.8275 - val_accuracy: 1.0000\n",
            "Epoch 550/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6085 - val_accuracy: 1.0000\n",
            "Epoch 551/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6082 - val_accuracy: 1.0000\n",
            "Epoch 552/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6491 - accuracy: 1.0000 - val_loss: 0.6045 - val_accuracy: 1.0000\n",
            "Epoch 553/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6726 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 1.0000\n",
            "Epoch 554/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6091 - accuracy: 1.0000 - val_loss: 0.6117 - val_accuracy: 1.0000\n",
            "Epoch 555/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6828 - accuracy: 1.0000 - val_loss: 0.6119 - val_accuracy: 1.0000\n",
            "Epoch 556/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6154 - accuracy: 1.0000 - val_loss: 0.9087 - val_accuracy: 1.0000\n",
            "Epoch 557/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6491 - accuracy: 1.0000 - val_loss: 0.6335 - val_accuracy: 1.0000\n",
            "Epoch 558/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6603 - accuracy: 1.0000 - val_loss: 0.6214 - val_accuracy: 1.0000\n",
            "Epoch 559/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6554 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 1.0000\n",
            "Epoch 560/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6721 - accuracy: 1.0000 - val_loss: 0.6102 - val_accuracy: 1.0000\n",
            "Epoch 561/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 562/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6678 - accuracy: 1.0000 - val_loss: 0.6295 - val_accuracy: 1.0000\n",
            "Epoch 563/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6164 - accuracy: 1.0000 - val_loss: 1.0956 - val_accuracy: 1.0000\n",
            "Epoch 564/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6493 - accuracy: 1.0000 - val_loss: 0.6360 - val_accuracy: 1.0000\n",
            "Epoch 565/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6546 - accuracy: 1.0000 - val_loss: 0.6436 - val_accuracy: 1.0000\n",
            "Epoch 566/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6684 - accuracy: 1.0000 - val_loss: 0.6285 - val_accuracy: 1.0000\n",
            "Epoch 567/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6235 - accuracy: 1.0000 - val_loss: 1.2059 - val_accuracy: 1.0000\n",
            "Epoch 568/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6532 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 1.0000\n",
            "Epoch 569/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.7147 - accuracy: 1.0000 - val_loss: 1.3437 - val_accuracy: 1.0000\n",
            "Epoch 570/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6232 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 1.0000\n",
            "Epoch 571/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6151 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 1.0000\n",
            "Epoch 572/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6640 - accuracy: 1.0000 - val_loss: 0.6129 - val_accuracy: 1.0000\n",
            "Epoch 573/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6313 - accuracy: 1.0000 - val_loss: 0.7181 - val_accuracy: 1.0000\n",
            "Epoch 574/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7258 - accuracy: 1.0000 - val_loss: 0.6195 - val_accuracy: 1.0000\n",
            "Epoch 575/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6099 - accuracy: 1.0000 - val_loss: 0.6231 - val_accuracy: 1.0000\n",
            "Epoch 576/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6313 - accuracy: 1.0000 - val_loss: 0.6462 - val_accuracy: 1.0000\n",
            "Epoch 577/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6497 - accuracy: 1.0000 - val_loss: 0.6119 - val_accuracy: 1.0000\n",
            "Epoch 578/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6478 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 1.0000\n",
            "Epoch 579/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6497 - accuracy: 1.0000 - val_loss: 0.6435 - val_accuracy: 1.0000\n",
            "Epoch 580/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6385 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 1.0000\n",
            "Epoch 581/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6440 - accuracy: 1.0000 - val_loss: 0.7012 - val_accuracy: 1.0000\n",
            "Epoch 582/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6524 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 1.0000\n",
            "Epoch 583/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6537 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 1.0000\n",
            "Epoch 584/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 1.0000\n",
            "Epoch 585/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 1.0000\n",
            "Epoch 586/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6670 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 1.0000\n",
            "Epoch 587/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 0.6214 - val_accuracy: 1.0000\n",
            "Epoch 588/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6511 - accuracy: 1.0000 - val_loss: 0.6256 - val_accuracy: 1.0000\n",
            "Epoch 589/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6497 - accuracy: 1.0000 - val_loss: 0.6230 - val_accuracy: 1.0000\n",
            "Epoch 590/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6312 - accuracy: 1.0000 - val_loss: 0.6528 - val_accuracy: 1.0000\n",
            "Epoch 591/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7145 - accuracy: 1.0000 - val_loss: 0.6066 - val_accuracy: 1.0000\n",
            "Epoch 592/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6127 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 1.0000\n",
            "Epoch 593/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6533 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 594/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 595/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 1.0000\n",
            "Epoch 596/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6522 - accuracy: 1.0000 - val_loss: 1.6117 - val_accuracy: 1.0000\n",
            "Epoch 597/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6691 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 1.0000\n",
            "Epoch 598/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.6458 - val_accuracy: 1.0000\n",
            "Epoch 599/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6320 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 1.0000\n",
            "Epoch 600/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6818 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 1.0000\n",
            "Epoch 601/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6272 - accuracy: 1.0000 - val_loss: 1.2362 - val_accuracy: 1.0000\n",
            "Epoch 602/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7053 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 1.0000\n",
            "Epoch 603/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6506 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 1.0000\n",
            "Epoch 604/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6463 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
            "Epoch 605/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6255 - accuracy: 1.0000 - val_loss: 0.6492 - val_accuracy: 1.0000\n",
            "Epoch 606/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6472 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 1.0000\n",
            "Epoch 607/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6460 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 608/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 1.0000\n",
            "Epoch 609/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6448 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 610/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6519 - accuracy: 1.0000 - val_loss: 0.6085 - val_accuracy: 1.0000\n",
            "Epoch 611/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6345 - accuracy: 1.0000 - val_loss: 0.6183 - val_accuracy: 1.0000\n",
            "Epoch 612/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6657 - accuracy: 1.0000 - val_loss: 0.6174 - val_accuracy: 1.0000\n",
            "Epoch 613/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 1.0000\n",
            "Epoch 614/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6583 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 1.0000\n",
            "Epoch 615/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6446 - accuracy: 1.0000 - val_loss: 0.6199 - val_accuracy: 1.0000\n",
            "Epoch 616/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6329 - accuracy: 1.0000 - val_loss: 0.7083 - val_accuracy: 1.0000\n",
            "Epoch 617/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6961 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 618/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6487 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 1.0000\n",
            "Epoch 619/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.7026 - val_accuracy: 1.0000\n",
            "Epoch 620/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6344 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 1.0000\n",
            "Epoch 621/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6398 - accuracy: 1.0000 - val_loss: 0.7419 - val_accuracy: 1.0000\n",
            "Epoch 622/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6439 - accuracy: 1.0000 - val_loss: 0.7238 - val_accuracy: 1.0000\n",
            "Epoch 623/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6522 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 1.0000\n",
            "Epoch 624/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6526 - accuracy: 1.0000 - val_loss: 0.6117 - val_accuracy: 1.0000\n",
            "Epoch 625/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6550 - accuracy: 1.0000 - val_loss: 0.6234 - val_accuracy: 1.0000\n",
            "Epoch 626/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6471 - accuracy: 1.0000 - val_loss: 0.6263 - val_accuracy: 1.0000\n",
            "Epoch 627/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6408 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 1.0000\n",
            "Epoch 628/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6434 - accuracy: 1.0000 - val_loss: 0.6062 - val_accuracy: 1.0000\n",
            "Epoch 629/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6490 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 1.0000\n",
            "Epoch 630/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6080 - val_accuracy: 1.0000\n",
            "Epoch 631/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 632/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 633/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6413 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 1.0000\n",
            "Epoch 634/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6942 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 1.0000\n",
            "Epoch 635/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6075 - accuracy: 1.0000 - val_loss: 0.6307 - val_accuracy: 1.0000\n",
            "Epoch 636/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6462 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
            "Epoch 637/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6390 - accuracy: 1.0000 - val_loss: 1.0915 - val_accuracy: 1.0000\n",
            "Epoch 638/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6566 - accuracy: 1.0000 - val_loss: 0.6109 - val_accuracy: 1.0000\n",
            "Epoch 639/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6509 - accuracy: 1.0000 - val_loss: 0.9891 - val_accuracy: 1.0000\n",
            "Epoch 640/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6409 - accuracy: 1.0000 - val_loss: 0.6283 - val_accuracy: 1.0000\n",
            "Epoch 641/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.6433 - val_accuracy: 1.0000\n",
            "Epoch 642/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 1.0000\n",
            "Epoch 643/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6445 - accuracy: 1.0000 - val_loss: 0.6147 - val_accuracy: 1.0000\n",
            "Epoch 644/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6687 - accuracy: 1.0000 - val_loss: 0.8946 - val_accuracy: 1.0000\n",
            "Epoch 645/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6261 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 1.0000\n",
            "Epoch 646/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6426 - accuracy: 1.0000 - val_loss: 0.6647 - val_accuracy: 1.0000\n",
            "Epoch 647/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6975 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 1.0000\n",
            "Epoch 648/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6192 - accuracy: 1.0000 - val_loss: 0.6541 - val_accuracy: 1.0000\n",
            "Epoch 649/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 1.0000\n",
            "Epoch 650/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.6183 - val_accuracy: 1.0000\n",
            "Epoch 651/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6464 - accuracy: 1.0000 - val_loss: 0.6359 - val_accuracy: 1.0000\n",
            "Epoch 652/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6649 - accuracy: 1.0000 - val_loss: 0.6085 - val_accuracy: 1.0000\n",
            "Epoch 653/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6130 - val_accuracy: 1.0000\n",
            "Epoch 654/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6948 - val_accuracy: 1.0000\n",
            "Epoch 655/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 1.5415 - val_accuracy: 1.0000\n",
            "Epoch 656/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6527 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 657/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6963 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 658/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6084 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 1.0000\n",
            "Epoch 659/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6447 - accuracy: 1.0000 - val_loss: 0.6383 - val_accuracy: 1.0000\n",
            "Epoch 660/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6710 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 661/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6476 - accuracy: 1.0000 - val_loss: 0.6255 - val_accuracy: 1.0000\n",
            "Epoch 662/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6144 - accuracy: 1.0000 - val_loss: 0.6259 - val_accuracy: 1.0000\n",
            "Epoch 663/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6824 - accuracy: 1.0000 - val_loss: 0.6317 - val_accuracy: 1.0000\n",
            "Epoch 664/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 1.0000 - val_loss: 0.6696 - val_accuracy: 1.0000\n",
            "Epoch 665/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6534 - val_accuracy: 1.0000\n",
            "Epoch 666/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 1.0000 - val_loss: 0.6178 - val_accuracy: 1.0000\n",
            "Epoch 667/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6621 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 1.0000\n",
            "Epoch 668/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6424 - accuracy: 1.0000 - val_loss: 0.6141 - val_accuracy: 1.0000\n",
            "Epoch 669/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6348 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 1.0000\n",
            "Epoch 670/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6780 - accuracy: 1.0000 - val_loss: 0.6360 - val_accuracy: 1.0000\n",
            "Epoch 671/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6287 - accuracy: 1.0000 - val_loss: 0.6080 - val_accuracy: 1.0000\n",
            "Epoch 672/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6377 - accuracy: 1.0000 - val_loss: 0.6283 - val_accuracy: 1.0000\n",
            "Epoch 673/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6590 - accuracy: 1.0000 - val_loss: 0.6301 - val_accuracy: 1.0000\n",
            "Epoch 674/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6515 - accuracy: 1.0000 - val_loss: 0.7310 - val_accuracy: 1.0000\n",
            "Epoch 675/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6304 - accuracy: 1.0000 - val_loss: 0.6215 - val_accuracy: 1.0000\n",
            "Epoch 676/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6613 - accuracy: 1.0000 - val_loss: 0.7331 - val_accuracy: 1.0000\n",
            "Epoch 677/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6988 - val_accuracy: 1.0000\n",
            "Epoch 678/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 679/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6542 - accuracy: 1.0000 - val_loss: 0.6624 - val_accuracy: 1.0000\n",
            "Epoch 680/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 0.6267 - val_accuracy: 1.0000\n",
            "Epoch 681/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6539 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 1.0000\n",
            "Epoch 682/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6516 - accuracy: 1.0000 - val_loss: 0.6103 - val_accuracy: 1.0000\n",
            "Epoch 683/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6449 - accuracy: 1.0000 - val_loss: 0.6198 - val_accuracy: 1.0000\n",
            "Epoch 684/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6435 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 1.0000\n",
            "Epoch 685/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6593 - accuracy: 1.0000 - val_loss: 0.6233 - val_accuracy: 1.0000\n",
            "Epoch 686/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6621 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 687/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6244 - accuracy: 1.0000 - val_loss: 0.6249 - val_accuracy: 1.0000\n",
            "Epoch 688/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6419 - accuracy: 1.0000 - val_loss: 0.6600 - val_accuracy: 1.0000\n",
            "Epoch 689/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6433 - accuracy: 1.0000 - val_loss: 0.8319 - val_accuracy: 1.0000\n",
            "Epoch 690/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6494 - accuracy: 1.0000 - val_loss: 0.7583 - val_accuracy: 1.0000\n",
            "Epoch 691/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6438 - accuracy: 1.0000 - val_loss: 0.6395 - val_accuracy: 1.0000\n",
            "Epoch 692/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7089 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 693/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6190 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 1.0000\n",
            "Epoch 694/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6448 - accuracy: 1.0000 - val_loss: 1.0409 - val_accuracy: 1.0000\n",
            "Epoch 695/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6517 - accuracy: 1.0000 - val_loss: 0.7834 - val_accuracy: 1.0000\n",
            "Epoch 696/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6286 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 1.0000\n",
            "Epoch 697/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6586 - accuracy: 1.0000 - val_loss: 0.6637 - val_accuracy: 1.0000\n",
            "Epoch 698/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6683 - accuracy: 1.0000 - val_loss: 0.6231 - val_accuracy: 1.0000\n",
            "Epoch 699/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6167 - accuracy: 1.0000 - val_loss: 0.6714 - val_accuracy: 1.0000\n",
            "Epoch 700/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 1.0000\n",
            "Epoch 701/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6503 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 1.0000\n",
            "Epoch 702/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6587 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 1.0000\n",
            "Epoch 703/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6478 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 1.0000\n",
            "Epoch 704/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6291 - accuracy: 1.0000 - val_loss: 0.6172 - val_accuracy: 1.0000\n",
            "Epoch 705/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6630 - accuracy: 1.0000 - val_loss: 0.6796 - val_accuracy: 1.0000\n",
            "Epoch 706/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6487 - accuracy: 1.0000 - val_loss: 0.7287 - val_accuracy: 1.0000\n",
            "Epoch 707/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6422 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 1.0000\n",
            "Epoch 708/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6774 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 709/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6145 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 1.0000\n",
            "Epoch 710/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6487 - accuracy: 1.0000 - val_loss: 0.6234 - val_accuracy: 1.0000\n",
            "Epoch 711/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 1.0000\n",
            "Epoch 712/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6578 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 1.0000\n",
            "Epoch 713/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6340 - accuracy: 1.0000 - val_loss: 0.6482 - val_accuracy: 1.0000\n",
            "Epoch 714/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6795 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 1.0000\n",
            "Epoch 715/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6484 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 1.0000\n",
            "Epoch 716/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6252 - accuracy: 1.0000 - val_loss: 0.7951 - val_accuracy: 1.0000\n",
            "Epoch 717/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6373 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 1.0000\n",
            "Epoch 718/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6564 - accuracy: 1.0000 - val_loss: 0.6214 - val_accuracy: 1.0000\n",
            "Epoch 719/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6831 - accuracy: 1.0000 - val_loss: 0.9450 - val_accuracy: 1.0000\n",
            "Epoch 720/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6206 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 1.0000\n",
            "Epoch 721/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6313 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 1.0000\n",
            "Epoch 722/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6704 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 723/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7207 - accuracy: 1.0000 - val_loss: 0.6452 - val_accuracy: 1.0000\n",
            "Epoch 724/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6115 - accuracy: 1.0000 - val_loss: 0.6169 - val_accuracy: 1.0000\n",
            "Epoch 725/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6398 - accuracy: 1.0000 - val_loss: 0.6672 - val_accuracy: 1.0000\n",
            "Epoch 726/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 1.0000\n",
            "Epoch 727/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6573 - accuracy: 1.0000 - val_loss: 0.6890 - val_accuracy: 1.0000\n",
            "Epoch 728/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6351 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 1.0000\n",
            "Epoch 729/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6477 - accuracy: 1.0000 - val_loss: 1.0315 - val_accuracy: 1.0000\n",
            "Epoch 730/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6396 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 731/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6464 - accuracy: 1.0000 - val_loss: 0.6191 - val_accuracy: 1.0000\n",
            "Epoch 732/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 0.6299 - val_accuracy: 1.0000\n",
            "Epoch 733/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6497 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 1.0000\n",
            "Epoch 734/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6433 - accuracy: 1.0000 - val_loss: 0.6351 - val_accuracy: 1.0000\n",
            "Epoch 735/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6493 - accuracy: 1.0000 - val_loss: 0.7508 - val_accuracy: 1.0000\n",
            "Epoch 736/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6494 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 1.0000\n",
            "Epoch 737/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 1.0000\n",
            "Epoch 738/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 739/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6480 - accuracy: 1.0000 - val_loss: 0.6229 - val_accuracy: 1.0000\n",
            "Epoch 740/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6462 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 1.0000\n",
            "Epoch 741/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6613 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 1.0000\n",
            "Epoch 742/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6744 - val_accuracy: 1.0000\n",
            "Epoch 743/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 1.0000\n",
            "Epoch 744/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6468 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 1.0000\n",
            "Epoch 745/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6217 - val_accuracy: 1.0000\n",
            "Epoch 746/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6403 - accuracy: 1.0000 - val_loss: 0.6437 - val_accuracy: 1.0000\n",
            "Epoch 747/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6641 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 1.0000\n",
            "Epoch 748/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6296 - accuracy: 1.0000 - val_loss: 0.7437 - val_accuracy: 1.0000\n",
            "Epoch 749/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6478 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 1.0000\n",
            "Epoch 750/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6453 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 1.0000\n",
            "Epoch 751/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6534 - accuracy: 1.0000 - val_loss: 0.8352 - val_accuracy: 1.0000\n",
            "Epoch 752/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6179 - val_accuracy: 1.0000\n",
            "Epoch 753/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6226 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 1.0000\n",
            "Epoch 754/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6526 - accuracy: 1.0000 - val_loss: 0.6103 - val_accuracy: 1.0000\n",
            "Epoch 755/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6500 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 1.0000\n",
            "Epoch 756/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6577 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 1.0000\n",
            "Epoch 757/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6311 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 1.0000\n",
            "Epoch 758/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6427 - accuracy: 1.0000 - val_loss: 0.6511 - val_accuracy: 1.0000\n",
            "Epoch 759/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6433 - accuracy: 1.0000 - val_loss: 1.0723 - val_accuracy: 1.0000\n",
            "Epoch 760/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6661 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 1.0000\n",
            "Epoch 761/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6318 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 762/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6612 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 1.0000\n",
            "Epoch 763/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6314 - accuracy: 1.0000 - val_loss: 0.7081 - val_accuracy: 1.0000\n",
            "Epoch 764/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6441 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 1.0000\n",
            "Epoch 765/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6623 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 1.0000\n",
            "Epoch 766/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6291 - accuracy: 1.0000 - val_loss: 0.7954 - val_accuracy: 1.0000\n",
            "Epoch 767/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6586 - accuracy: 1.0000 - val_loss: 0.6107 - val_accuracy: 1.0000\n",
            "Epoch 768/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6441 - accuracy: 1.0000 - val_loss: 0.6016 - val_accuracy: 1.0000\n",
            "Epoch 769/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6554 - accuracy: 1.0000 - val_loss: 0.6177 - val_accuracy: 1.0000\n",
            "Epoch 770/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6174 - accuracy: 1.0000 - val_loss: 0.9387 - val_accuracy: 1.0000\n",
            "Epoch 771/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6630 - accuracy: 1.0000 - val_loss: 0.6870 - val_accuracy: 1.0000\n",
            "Epoch 772/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6482 - accuracy: 1.0000 - val_loss: 0.6253 - val_accuracy: 1.0000\n",
            "Epoch 773/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6407 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 1.0000\n",
            "Epoch 774/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6880 - accuracy: 1.0000 - val_loss: 0.6117 - val_accuracy: 1.0000\n",
            "Epoch 775/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6054 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 1.0000\n",
            "Epoch 776/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 1.0000\n",
            "Epoch 777/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6533 - accuracy: 1.0000 - val_loss: 1.0511 - val_accuracy: 1.0000\n",
            "Epoch 778/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.6210 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 1.0000\n",
            "Epoch 779/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6494 - val_accuracy: 1.0000\n",
            "Epoch 780/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6461 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 1.0000\n",
            "Epoch 781/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6550 - accuracy: 1.0000 - val_loss: 0.6174 - val_accuracy: 1.0000\n",
            "Epoch 782/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6366 - accuracy: 1.0000 - val_loss: 0.6506 - val_accuracy: 1.0000\n",
            "Epoch 783/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6481 - accuracy: 1.0000 - val_loss: 0.6195 - val_accuracy: 1.0000\n",
            "Epoch 784/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6530 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 1.0000\n",
            "Epoch 785/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.7660 - val_accuracy: 1.0000\n",
            "Epoch 786/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6347 - accuracy: 1.0000 - val_loss: 0.7030 - val_accuracy: 1.0000\n",
            "Epoch 787/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6403 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 1.0000\n",
            "Epoch 788/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6439 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 1.0000\n",
            "Epoch 789/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6413 - accuracy: 1.0000 - val_loss: 0.6335 - val_accuracy: 1.0000\n",
            "Epoch 790/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6478 - accuracy: 1.0000 - val_loss: 0.6288 - val_accuracy: 1.0000\n",
            "Epoch 791/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6398 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 1.0000\n",
            "Epoch 792/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 793/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6398 - accuracy: 1.0000 - val_loss: 0.6234 - val_accuracy: 1.0000\n",
            "Epoch 794/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 795/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 1.0000 - val_loss: 0.6167 - val_accuracy: 1.0000\n",
            "Epoch 796/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6474 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 1.0000\n",
            "Epoch 797/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6428 - accuracy: 1.0000 - val_loss: 0.6445 - val_accuracy: 1.0000\n",
            "Epoch 798/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6679 - accuracy: 1.0000 - val_loss: 0.6139 - val_accuracy: 1.0000\n",
            "Epoch 799/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6294 - accuracy: 1.0000 - val_loss: 0.7039 - val_accuracy: 1.0000\n",
            "Epoch 800/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6710 - val_accuracy: 1.0000\n",
            "Epoch 801/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6612 - accuracy: 1.0000 - val_loss: 0.6179 - val_accuracy: 1.0000\n",
            "Epoch 802/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6361 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 1.0000\n",
            "Epoch 803/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6506 - accuracy: 1.0000 - val_loss: 0.7046 - val_accuracy: 1.0000\n",
            "Epoch 804/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6470 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 1.0000\n",
            "Epoch 805/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6414 - accuracy: 1.0000 - val_loss: 0.6490 - val_accuracy: 1.0000\n",
            "Epoch 806/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6318 - accuracy: 1.0000 - val_loss: 0.6306 - val_accuracy: 1.0000\n",
            "Epoch 807/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6785 - accuracy: 1.0000 - val_loss: 0.7005 - val_accuracy: 1.0000\n",
            "Epoch 808/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6384 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 1.0000\n",
            "Epoch 809/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6345 - accuracy: 1.0000 - val_loss: 0.6291 - val_accuracy: 1.0000\n",
            "Epoch 810/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6590 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 811/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6255 - accuracy: 1.0000 - val_loss: 0.6386 - val_accuracy: 1.0000\n",
            "Epoch 812/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6466 - accuracy: 1.0000 - val_loss: 0.6328 - val_accuracy: 1.0000\n",
            "Epoch 813/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6496 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 1.0000\n",
            "Epoch 814/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6629 - accuracy: 1.0000 - val_loss: 1.2944 - val_accuracy: 1.0000\n",
            "Epoch 815/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6400 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 1.0000\n",
            "Epoch 816/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6369 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 1.0000\n",
            "Epoch 817/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6378 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 1.0000\n",
            "Epoch 818/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6388 - accuracy: 1.0000 - val_loss: 0.6655 - val_accuracy: 1.0000\n",
            "Epoch 819/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6609 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 1.0000\n",
            "Epoch 820/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6714 - accuracy: 1.0000 - val_loss: 0.6139 - val_accuracy: 1.0000\n",
            "Epoch 821/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6231 - accuracy: 1.0000 - val_loss: 0.8683 - val_accuracy: 1.0000\n",
            "Epoch 822/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6297 - accuracy: 1.0000 - val_loss: 0.6172 - val_accuracy: 1.0000\n",
            "Epoch 823/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6911 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 1.0000\n",
            "Epoch 824/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6150 - accuracy: 1.0000 - val_loss: 0.6223 - val_accuracy: 1.0000\n",
            "Epoch 825/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6501 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 1.0000\n",
            "Epoch 826/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7094 - accuracy: 1.0000 - val_loss: 0.6057 - val_accuracy: 1.0000\n",
            "Epoch 827/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6023 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 828/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6447 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 1.0000\n",
            "Epoch 829/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 1.0000\n",
            "Epoch 830/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6481 - accuracy: 1.0000 - val_loss: 0.8458 - val_accuracy: 1.0000\n",
            "Epoch 831/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.7964 - val_accuracy: 1.0000\n",
            "Epoch 832/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6519 - accuracy: 1.0000 - val_loss: 0.7050 - val_accuracy: 1.0000\n",
            "Epoch 833/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.6501 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 1.0000\n",
            "Epoch 834/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.6802 - val_accuracy: 1.0000\n",
            "Epoch 835/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6532 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 1.0000\n",
            "Epoch 836/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6385 - accuracy: 1.0000 - val_loss: 0.7196 - val_accuracy: 1.0000\n",
            "Epoch 837/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6492 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 1.0000\n",
            "Epoch 838/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6953 - accuracy: 1.0000 - val_loss: 0.6624 - val_accuracy: 1.0000\n",
            "Epoch 839/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6083 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 1.0000\n",
            "Epoch 840/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6369 - accuracy: 1.0000 - val_loss: 0.6169 - val_accuracy: 1.0000\n",
            "Epoch 841/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6510 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 1.0000\n",
            "Epoch 842/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6766 - accuracy: 1.0000 - val_loss: 0.6129 - val_accuracy: 1.0000\n",
            "Epoch 843/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6126 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 1.0000\n",
            "Epoch 844/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6356 - accuracy: 1.0000 - val_loss: 0.7708 - val_accuracy: 1.0000\n",
            "Epoch 845/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6803 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 1.0000\n",
            "Epoch 846/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6296 - accuracy: 1.0000 - val_loss: 0.8436 - val_accuracy: 1.0000\n",
            "Epoch 847/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6452 - accuracy: 1.0000 - val_loss: 0.7292 - val_accuracy: 1.0000\n",
            "Epoch 848/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 1.0000\n",
            "Epoch 849/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6388 - accuracy: 1.0000 - val_loss: 0.7180 - val_accuracy: 1.0000\n",
            "Epoch 850/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6553 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 1.0000\n",
            "Epoch 851/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6551 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 1.0000\n",
            "Epoch 852/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6326 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 1.0000\n",
            "Epoch 853/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6396 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 854/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6425 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 855/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6704 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 1.0000\n",
            "Epoch 856/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6230 - accuracy: 1.0000 - val_loss: 0.8078 - val_accuracy: 1.0000\n",
            "Epoch 857/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6529 - accuracy: 1.0000 - val_loss: 0.6102 - val_accuracy: 1.0000\n",
            "Epoch 858/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6272 - accuracy: 1.0000 - val_loss: 0.7183 - val_accuracy: 1.0000\n",
            "Epoch 859/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6550 - accuracy: 1.0000 - val_loss: 0.6257 - val_accuracy: 1.0000\n",
            "Epoch 860/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6755 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 1.0000\n",
            "Epoch 861/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6176 - accuracy: 1.0000 - val_loss: 0.6836 - val_accuracy: 1.0000\n",
            "Epoch 862/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6424 - accuracy: 1.0000 - val_loss: 0.6491 - val_accuracy: 1.0000\n",
            "Epoch 863/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6828 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 1.0000\n",
            "Epoch 864/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6072 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
            "Epoch 865/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6602 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 1.0000\n",
            "Epoch 866/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 1.0000\n",
            "Epoch 867/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6502 - val_accuracy: 1.0000\n",
            "Epoch 868/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6387 - accuracy: 1.0000 - val_loss: 0.9690 - val_accuracy: 1.0000\n",
            "Epoch 869/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 1.0000\n",
            "Epoch 870/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6356 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 871/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6588 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 1.0000\n",
            "Epoch 872/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6586 - accuracy: 1.0000 - val_loss: 1.1267 - val_accuracy: 1.0000\n",
            "Epoch 873/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6297 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 1.0000\n",
            "Epoch 874/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
            "Epoch 875/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6444 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 1.0000\n",
            "Epoch 876/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6437 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 1.0000\n",
            "Epoch 877/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6536 - accuracy: 1.0000 - val_loss: 0.8626 - val_accuracy: 1.0000\n",
            "Epoch 878/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6864 - accuracy: 1.0000 - val_loss: 0.7847 - val_accuracy: 1.0000\n",
            "Epoch 879/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6156 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 1.0000\n",
            "Epoch 880/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6196 - accuracy: 1.0000 - val_loss: 0.6384 - val_accuracy: 1.0000\n",
            "Epoch 881/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7160 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 882/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6043 - accuracy: 1.0000 - val_loss: 0.7699 - val_accuracy: 1.0000\n",
            "Epoch 883/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6482 - accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 1.0000\n",
            "Epoch 884/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 1.0000\n",
            "Epoch 885/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6439 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 1.0000\n",
            "Epoch 886/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6556 - accuracy: 1.0000 - val_loss: 0.6100 - val_accuracy: 1.0000\n",
            "Epoch 887/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 1.0000\n",
            "Epoch 888/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6384 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 1.0000\n",
            "Epoch 889/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6707 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 1.0000\n",
            "Epoch 890/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6130 - accuracy: 1.0000 - val_loss: 0.6820 - val_accuracy: 1.0000\n",
            "Epoch 891/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6392 - accuracy: 1.0000 - val_loss: 0.8921 - val_accuracy: 1.0000\n",
            "Epoch 892/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6579 - accuracy: 1.0000 - val_loss: 0.6154 - val_accuracy: 1.0000\n",
            "Epoch 893/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6292 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 1.0000\n",
            "Epoch 894/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6508 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 1.0000\n",
            "Epoch 895/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6554 - accuracy: 1.0000 - val_loss: 0.7335 - val_accuracy: 1.0000\n",
            "Epoch 896/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6625 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 1.0000\n",
            "Epoch 897/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6171 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 1.0000\n",
            "Epoch 898/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 1.0000\n",
            "Epoch 899/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6619 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
            "Epoch 900/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.6296 - val_accuracy: 1.0000\n",
            "Epoch 901/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 0.7455 - val_accuracy: 1.0000\n",
            "Epoch 902/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6406 - accuracy: 1.0000 - val_loss: 0.8098 - val_accuracy: 1.0000\n",
            "Epoch 903/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6485 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 904/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 1.0000\n",
            "Epoch 905/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6504 - accuracy: 1.0000 - val_loss: 0.6239 - val_accuracy: 1.0000\n",
            "Epoch 906/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6469 - accuracy: 1.0000 - val_loss: 0.6550 - val_accuracy: 1.0000\n",
            "Epoch 907/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6413 - accuracy: 1.0000 - val_loss: 0.6117 - val_accuracy: 1.0000\n",
            "Epoch 908/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6585 - accuracy: 1.0000 - val_loss: 0.6118 - val_accuracy: 1.0000\n",
            "Epoch 909/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6411 - accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 1.0000\n",
            "Epoch 910/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6531 - accuracy: 1.0000 - val_loss: 0.6919 - val_accuracy: 1.0000\n",
            "Epoch 911/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.7327 - val_accuracy: 1.0000\n",
            "Epoch 912/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6234 - accuracy: 1.0000 - val_loss: 0.6370 - val_accuracy: 1.0000\n",
            "Epoch 913/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6740 - accuracy: 1.0000 - val_loss: 0.6114 - val_accuracy: 1.0000\n",
            "Epoch 914/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6158 - accuracy: 1.0000 - val_loss: 0.6815 - val_accuracy: 1.0000\n",
            "Epoch 915/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6377 - accuracy: 1.0000 - val_loss: 0.6326 - val_accuracy: 1.0000\n",
            "Epoch 916/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6447 - accuracy: 1.0000 - val_loss: 0.6839 - val_accuracy: 1.0000\n",
            "Epoch 917/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6505 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 1.0000\n",
            "Epoch 918/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6433 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 1.0000\n",
            "Epoch 919/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 1.0000\n",
            "Epoch 920/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6399 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 1.0000\n",
            "Epoch 921/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.7576 - val_accuracy: 1.0000\n",
            "Epoch 922/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 0.6286 - val_accuracy: 1.0000\n",
            "Epoch 923/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6304 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 924/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6672 - accuracy: 1.0000 - val_loss: 0.6141 - val_accuracy: 1.0000\n",
            "Epoch 925/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.6112 - val_accuracy: 1.0000\n",
            "Epoch 926/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6476 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 1.0000\n",
            "Epoch 927/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6230 - accuracy: 1.0000 - val_loss: 0.6238 - val_accuracy: 1.0000\n",
            "Epoch 928/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6457 - accuracy: 1.0000 - val_loss: 0.6045 - val_accuracy: 1.0000\n",
            "Epoch 929/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6622 - accuracy: 1.0000 - val_loss: 1.0697 - val_accuracy: 1.0000\n",
            "Epoch 930/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6281 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 1.0000\n",
            "Epoch 931/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6355 - accuracy: 1.0000 - val_loss: 0.6842 - val_accuracy: 1.0000\n",
            "Epoch 932/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6520 - val_accuracy: 1.0000\n",
            "Epoch 933/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 1.0018 - val_accuracy: 1.0000\n",
            "Epoch 934/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6399 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
            "Epoch 935/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 0.7005 - val_accuracy: 1.0000\n",
            "Epoch 936/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6422 - accuracy: 1.0000 - val_loss: 1.9640 - val_accuracy: 1.0000\n",
            "Epoch 937/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6794 - accuracy: 1.0000 - val_loss: 0.6027 - val_accuracy: 1.0000\n",
            "Epoch 938/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6153 - accuracy: 1.0000 - val_loss: 0.6850 - val_accuracy: 1.0000\n",
            "Epoch 939/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6718 - accuracy: 1.0000 - val_loss: 0.6838 - val_accuracy: 1.0000\n",
            "Epoch 940/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6399 - accuracy: 1.0000 - val_loss: 0.6903 - val_accuracy: 1.0000\n",
            "Epoch 941/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6305 - accuracy: 1.0000 - val_loss: 0.6377 - val_accuracy: 1.0000\n",
            "Epoch 942/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6492 - accuracy: 1.0000 - val_loss: 0.6039 - val_accuracy: 1.0000\n",
            "Epoch 943/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6356 - accuracy: 1.0000 - val_loss: 0.6279 - val_accuracy: 1.0000\n",
            "Epoch 944/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6697 - accuracy: 1.0000 - val_loss: 0.6276 - val_accuracy: 1.0000\n",
            "Epoch 945/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6480 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 946/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6384 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 1.0000\n",
            "Epoch 947/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6594 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 1.0000\n",
            "Epoch 948/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6132 - accuracy: 1.0000 - val_loss: 0.6167 - val_accuracy: 1.0000\n",
            "Epoch 949/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6538 - accuracy: 1.0000 - val_loss: 0.6553 - val_accuracy: 1.0000\n",
            "Epoch 950/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.6397 - val_accuracy: 1.0000\n",
            "Epoch 951/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6794 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 1.0000\n",
            "Epoch 952/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6150 - accuracy: 1.0000 - val_loss: 0.7466 - val_accuracy: 1.0000\n",
            "Epoch 953/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6786 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 954/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6131 - accuracy: 1.0000 - val_loss: 0.6637 - val_accuracy: 1.0000\n",
            "Epoch 955/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6591 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 1.0000\n",
            "Epoch 956/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6238 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 1.0000\n",
            "Epoch 957/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6518 - accuracy: 1.0000 - val_loss: 0.6057 - val_accuracy: 1.0000\n",
            "Epoch 958/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6519 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 1.0000\n",
            "Epoch 959/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6333 - accuracy: 1.0000 - val_loss: 0.7540 - val_accuracy: 1.0000\n",
            "Epoch 960/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6520 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 1.0000\n",
            "Epoch 961/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.6637 - val_accuracy: 1.0000\n",
            "Epoch 962/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6560 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
            "Epoch 963/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6344 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 964/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 965/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 1.0000\n",
            "Epoch 966/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6541 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
            "Epoch 967/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6397 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 1.0000\n",
            "Epoch 968/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6477 - accuracy: 1.0000 - val_loss: 0.6225 - val_accuracy: 1.0000\n",
            "Epoch 969/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6361 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 1.0000\n",
            "Epoch 970/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6591 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 1.0000\n",
            "Epoch 971/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6461 - accuracy: 1.0000 - val_loss: 0.6183 - val_accuracy: 1.0000\n",
            "Epoch 972/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6426 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 1.0000\n",
            "Epoch 973/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6429 - val_accuracy: 1.0000\n",
            "Epoch 974/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6301 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 1.0000\n",
            "Epoch 975/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6333 - accuracy: 1.0000 - val_loss: 0.6465 - val_accuracy: 1.0000\n",
            "Epoch 976/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6522 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 1.0000\n",
            "Epoch 977/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6625 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 1.0000\n",
            "Epoch 978/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6470 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 1.0000\n",
            "Epoch 979/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6117 - accuracy: 1.0000 - val_loss: 0.9249 - val_accuracy: 1.0000\n",
            "Epoch 980/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6571 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 1.0000\n",
            "Epoch 981/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 1.0000\n",
            "Epoch 982/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6554 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 1.0000\n",
            "Epoch 983/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6228 - val_accuracy: 1.0000\n",
            "Epoch 984/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6541 - accuracy: 1.0000 - val_loss: 0.6146 - val_accuracy: 1.0000\n",
            "Epoch 985/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6373 - accuracy: 1.0000 - val_loss: 0.6221 - val_accuracy: 1.0000\n",
            "Epoch 986/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6377 - accuracy: 1.0000 - val_loss: 0.6312 - val_accuracy: 1.0000\n",
            "Epoch 987/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6596 - accuracy: 1.0000 - val_loss: 0.7606 - val_accuracy: 1.0000\n",
            "Epoch 988/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6744 - accuracy: 1.0000 - val_loss: 0.7155 - val_accuracy: 1.0000\n",
            "Epoch 989/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.6290 - val_accuracy: 1.0000\n",
            "Epoch 990/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6192 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 1.0000\n",
            "Epoch 991/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6492 - accuracy: 1.0000 - val_loss: 0.6356 - val_accuracy: 1.0000\n",
            "Epoch 992/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6468 - accuracy: 1.0000 - val_loss: 0.6062 - val_accuracy: 1.0000\n",
            "Epoch 993/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 1.0000\n",
            "Epoch 994/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6322 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 1.0000\n",
            "Epoch 995/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6518 - accuracy: 1.0000 - val_loss: 0.6362 - val_accuracy: 1.0000\n",
            "Epoch 996/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6247 - accuracy: 1.0000 - val_loss: 0.6149 - val_accuracy: 1.0000\n",
            "Epoch 997/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6542 - accuracy: 1.0000 - val_loss: 0.6000 - val_accuracy: 1.0000\n",
            "Epoch 998/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6707 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 999/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6183 - accuracy: 1.0000 - val_loss: 0.6237 - val_accuracy: 1.0000\n",
            "Epoch 1000/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6765 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 1.0000\n",
            "Epoch 1001/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6393 - accuracy: 1.0000 - val_loss: 0.6371 - val_accuracy: 1.0000\n",
            "Epoch 1002/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6665 - accuracy: 1.0000 - val_loss: 0.5994 - val_accuracy: 1.0000\n",
            "Epoch 1003/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6285 - accuracy: 1.0000 - val_loss: 0.6358 - val_accuracy: 1.0000\n",
            "Epoch 1004/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.7280 - val_accuracy: 1.0000\n",
            "Epoch 1005/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6486 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 1.0000\n",
            "Epoch 1006/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6026 - val_accuracy: 1.0000\n",
            "Epoch 1007/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 1.0000\n",
            "Epoch 1008/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.6565 - val_accuracy: 1.0000\n",
            "Epoch 1009/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6477 - accuracy: 1.0000 - val_loss: 0.6210 - val_accuracy: 1.0000\n",
            "Epoch 1010/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.6321 - val_accuracy: 1.0000\n",
            "Epoch 1011/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6449 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 1.0000\n",
            "Epoch 1012/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6601 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 1.0000\n",
            "Epoch 1013/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6118 - val_accuracy: 1.0000\n",
            "Epoch 1014/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6386 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 1.0000\n",
            "Epoch 1015/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6307 - accuracy: 1.0000 - val_loss: 0.6004 - val_accuracy: 1.0000\n",
            "Epoch 1016/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 1.0000\n",
            "Epoch 1017/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6374 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
            "Epoch 1018/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6473 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 1019/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6415 - accuracy: 1.0000 - val_loss: 0.7452 - val_accuracy: 1.0000\n",
            "Epoch 1020/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 0.6012 - val_accuracy: 1.0000\n",
            "Epoch 1021/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6344 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 1.0000\n",
            "Epoch 1022/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 1.0000\n",
            "Epoch 1023/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6510 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 1024/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6426 - accuracy: 1.0000 - val_loss: 0.6195 - val_accuracy: 1.0000\n",
            "Epoch 1025/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6377 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 1.0000\n",
            "Epoch 1026/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6419 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 1027/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6666 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 1028/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6224 - accuracy: 1.0000 - val_loss: 0.6306 - val_accuracy: 1.0000\n",
            "Epoch 1029/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6413 - accuracy: 1.0000 - val_loss: 0.7124 - val_accuracy: 1.0000\n",
            "Epoch 1030/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6445 - accuracy: 1.0000 - val_loss: 1.1523 - val_accuracy: 1.0000\n",
            "Epoch 1031/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6632 - accuracy: 1.0000 - val_loss: 0.6811 - val_accuracy: 1.0000\n",
            "Epoch 1032/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6325 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 1033/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 1.0000\n",
            "Epoch 1034/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6441 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 1035/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6508 - accuracy: 1.0000 - val_loss: 0.7448 - val_accuracy: 1.0000\n",
            "Epoch 1036/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6427 - accuracy: 1.0000 - val_loss: 0.6116 - val_accuracy: 1.0000\n",
            "Epoch 1037/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6404 - accuracy: 1.0000 - val_loss: 0.6294 - val_accuracy: 1.0000\n",
            "Epoch 1038/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6307 - accuracy: 1.0000 - val_loss: 0.6968 - val_accuracy: 1.0000\n",
            "Epoch 1039/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6489 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 1.0000\n",
            "Epoch 1040/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6498 - accuracy: 1.0000 - val_loss: 0.6096 - val_accuracy: 1.0000\n",
            "Epoch 1041/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 1.0000\n",
            "Epoch 1042/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 1.0000\n",
            "Epoch 1043/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6560 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 1044/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6364 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 1.0000\n",
            "Epoch 1045/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6557 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 1.0000\n",
            "Epoch 1046/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6546 - accuracy: 1.0000 - val_loss: 0.7112 - val_accuracy: 1.0000\n",
            "Epoch 1047/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 1.0000\n",
            "Epoch 1048/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 0.6168 - val_accuracy: 1.0000\n",
            "Epoch 1049/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6613 - accuracy: 1.0000 - val_loss: 0.8699 - val_accuracy: 1.0000\n",
            "Epoch 1050/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6127 - accuracy: 1.0000 - val_loss: 0.6408 - val_accuracy: 1.0000\n",
            "Epoch 1051/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6477 - accuracy: 1.0000 - val_loss: 0.6247 - val_accuracy: 1.0000\n",
            "Epoch 1052/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6431 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 1053/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6285 - accuracy: 1.0000 - val_loss: 0.7362 - val_accuracy: 1.0000\n",
            "Epoch 1054/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 1.0000\n",
            "Epoch 1055/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6386 - accuracy: 1.0000 - val_loss: 0.8965 - val_accuracy: 1.0000\n",
            "Epoch 1056/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6573 - accuracy: 1.0000 - val_loss: 0.6139 - val_accuracy: 1.0000\n",
            "Epoch 1057/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6248 - accuracy: 1.0000 - val_loss: 0.6147 - val_accuracy: 1.0000\n",
            "Epoch 1058/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6489 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 1.0000\n",
            "Epoch 1059/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6327 - accuracy: 1.0000 - val_loss: 0.6035 - val_accuracy: 1.0000\n",
            "Epoch 1060/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.9795 - val_accuracy: 1.0000\n",
            "Epoch 1061/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6500 - accuracy: 1.0000 - val_loss: 0.7835 - val_accuracy: 1.0000\n",
            "Epoch 1062/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6448 - accuracy: 1.0000 - val_loss: 0.6451 - val_accuracy: 1.0000\n",
            "Epoch 1063/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 1064/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6424 - accuracy: 1.0000 - val_loss: 0.6119 - val_accuracy: 1.0000\n",
            "Epoch 1065/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 1.0000 - val_loss: 0.6057 - val_accuracy: 1.0000\n",
            "Epoch 1066/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6568 - accuracy: 1.0000 - val_loss: 0.6785 - val_accuracy: 1.0000\n",
            "Epoch 1067/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6774 - accuracy: 1.0000 - val_loss: 0.6977 - val_accuracy: 1.0000\n",
            "Epoch 1068/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6261 - accuracy: 1.0000 - val_loss: 0.6435 - val_accuracy: 1.0000\n",
            "Epoch 1069/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6484 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
            "Epoch 1070/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6292 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 1.0000\n",
            "Epoch 1071/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6429 - accuracy: 1.0000 - val_loss: 0.7727 - val_accuracy: 1.0000\n",
            "Epoch 1072/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6733 - accuracy: 1.0000 - val_loss: 0.6191 - val_accuracy: 1.0000\n",
            "Epoch 1073/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6059 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 1.0000\n",
            "Epoch 1074/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6654 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 1.0000\n",
            "Epoch 1075/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6251 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 1.0000\n",
            "Epoch 1076/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.5996 - val_accuracy: 1.0000\n",
            "Epoch 1077/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6540 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 1078/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6690 - accuracy: 1.0000 - val_loss: 0.6207 - val_accuracy: 1.0000\n",
            "Epoch 1079/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6105 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 1080/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6532 - accuracy: 1.0000 - val_loss: 0.6251 - val_accuracy: 1.0000\n",
            "Epoch 1081/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6440 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 1082/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6398 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 1.0000\n",
            "Epoch 1083/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6388 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 1.0000\n",
            "Epoch 1084/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.6432 - val_accuracy: 1.0000\n",
            "Epoch 1085/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6471 - accuracy: 1.0000 - val_loss: 0.6076 - val_accuracy: 1.0000\n",
            "Epoch 1086/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6440 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 1.0000\n",
            "Epoch 1087/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6434 - accuracy: 1.0000 - val_loss: 0.9073 - val_accuracy: 1.0000\n",
            "Epoch 1088/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6634 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 1.0000\n",
            "Epoch 1089/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6343 - accuracy: 1.0000 - val_loss: 0.6433 - val_accuracy: 1.0000\n",
            "Epoch 1090/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6738 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 1.0000\n",
            "Epoch 1091/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6227 - accuracy: 1.0000 - val_loss: 0.7383 - val_accuracy: 1.0000\n",
            "Epoch 1092/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6477 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 1.0000\n",
            "Epoch 1093/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6228 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 1.0000\n",
            "Epoch 1094/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6598 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 1.0000\n",
            "Epoch 1095/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6311 - accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 1.0000\n",
            "Epoch 1096/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6453 - accuracy: 1.0000 - val_loss: 0.6272 - val_accuracy: 1.0000\n",
            "Epoch 1097/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 0.6310 - val_accuracy: 1.0000\n",
            "Epoch 1098/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6263 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 1.0000\n",
            "Epoch 1099/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6356 - accuracy: 1.0000 - val_loss: 0.7275 - val_accuracy: 1.0000\n",
            "Epoch 1100/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6875 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 1.0000\n",
            "Epoch 1101/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6076 - accuracy: 1.0000 - val_loss: 0.6159 - val_accuracy: 1.0000\n",
            "Epoch 1102/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6349 - accuracy: 1.0000 - val_loss: 0.6965 - val_accuracy: 1.0000\n",
            "Epoch 1103/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6420 - accuracy: 1.0000 - val_loss: 0.6186 - val_accuracy: 1.0000\n",
            "Epoch 1104/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.7550 - val_accuracy: 1.0000\n",
            "Epoch 1105/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6506 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 1.0000\n",
            "Epoch 1106/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6453 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
            "Epoch 1107/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 1.0000\n",
            "Epoch 1108/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6365 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 1.0000\n",
            "Epoch 1109/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 1.0264 - val_accuracy: 1.0000\n",
            "Epoch 1110/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6666 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 1.0000\n",
            "Epoch 1111/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6248 - accuracy: 1.0000 - val_loss: 0.7245 - val_accuracy: 1.0000\n",
            "Epoch 1112/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6686 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 1.0000\n",
            "Epoch 1113/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6225 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 1114/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6402 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 1115/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6414 - accuracy: 1.0000 - val_loss: 0.6095 - val_accuracy: 1.0000\n",
            "Epoch 1116/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6470 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 1.0000\n",
            "Epoch 1117/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6433 - accuracy: 1.0000 - val_loss: 0.6154 - val_accuracy: 1.0000\n",
            "Epoch 1118/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6291 - accuracy: 1.0000 - val_loss: 0.6196 - val_accuracy: 1.0000\n",
            "Epoch 1119/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6691 - accuracy: 1.0000 - val_loss: 0.6016 - val_accuracy: 1.0000\n",
            "Epoch 1120/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6227 - accuracy: 1.0000 - val_loss: 0.6512 - val_accuracy: 1.0000\n",
            "Epoch 1121/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6774 - accuracy: 1.0000 - val_loss: 0.6017 - val_accuracy: 1.0000\n",
            "Epoch 1122/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6094 - accuracy: 1.0000 - val_loss: 0.9193 - val_accuracy: 1.0000\n",
            "Epoch 1123/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6673 - accuracy: 1.0000 - val_loss: 0.6371 - val_accuracy: 1.0000\n",
            "Epoch 1124/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6135 - accuracy: 1.0000 - val_loss: 0.6302 - val_accuracy: 1.0000\n",
            "Epoch 1125/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6470 - accuracy: 1.0000 - val_loss: 0.6229 - val_accuracy: 1.0000\n",
            "Epoch 1126/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 1.0000\n",
            "Epoch 1127/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 1.0000 - val_loss: 0.8074 - val_accuracy: 1.0000\n",
            "Epoch 1128/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6626 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 1.0000\n",
            "Epoch 1129/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6200 - accuracy: 1.0000 - val_loss: 0.6178 - val_accuracy: 1.0000\n",
            "Epoch 1130/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6998 - accuracy: 1.0000 - val_loss: 0.6174 - val_accuracy: 1.0000\n",
            "Epoch 1131/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6289 - accuracy: 1.0000 - val_loss: 0.6082 - val_accuracy: 1.0000\n",
            "Epoch 1132/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6428 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 1.0000\n",
            "Epoch 1133/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6294 - accuracy: 1.0000 - val_loss: 0.6015 - val_accuracy: 1.0000\n",
            "Epoch 1134/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6664 - accuracy: 1.0000 - val_loss: 0.7121 - val_accuracy: 1.0000\n",
            "Epoch 1135/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6651 - accuracy: 1.0000 - val_loss: 0.6632 - val_accuracy: 1.0000\n",
            "Epoch 1136/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6048 - accuracy: 1.0000 - val_loss: 0.6263 - val_accuracy: 1.0000\n",
            "Epoch 1137/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6677 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
            "Epoch 1138/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6089 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 1.0000\n",
            "Epoch 1139/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6384 - accuracy: 1.0000 - val_loss: 0.6026 - val_accuracy: 1.0000\n",
            "Epoch 1140/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6445 - accuracy: 1.0000 - val_loss: 0.6562 - val_accuracy: 1.0000\n",
            "Epoch 1141/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6488 - accuracy: 1.0000 - val_loss: 0.6355 - val_accuracy: 1.0000\n",
            "Epoch 1142/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6398 - accuracy: 1.0000 - val_loss: 0.6459 - val_accuracy: 1.0000\n",
            "Epoch 1143/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6329 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 1144/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6393 - val_accuracy: 1.0000\n",
            "Epoch 1145/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6466 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 1.0000\n",
            "Epoch 1146/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6456 - accuracy: 1.0000 - val_loss: 0.6057 - val_accuracy: 1.0000\n",
            "Epoch 1147/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6446 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 1.0000\n",
            "Epoch 1148/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6644 - accuracy: 1.0000 - val_loss: 0.6138 - val_accuracy: 1.0000\n",
            "Epoch 1149/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6190 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 1.0000\n",
            "Epoch 1150/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.5985 - val_accuracy: 1.0000\n",
            "Epoch 1151/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6567 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
            "Epoch 1152/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6461 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 1.0000\n",
            "Epoch 1153/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6295 - accuracy: 1.0000 - val_loss: 0.6312 - val_accuracy: 1.0000\n",
            "Epoch 1154/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6524 - accuracy: 1.0000 - val_loss: 0.6027 - val_accuracy: 1.0000\n",
            "Epoch 1155/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6134 - val_accuracy: 1.0000\n",
            "Epoch 1156/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6341 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 1.0000\n",
            "Epoch 1157/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6419 - accuracy: 1.0000 - val_loss: 0.6116 - val_accuracy: 1.0000\n",
            "Epoch 1158/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6351 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 1.0000\n",
            "Epoch 1159/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6686 - accuracy: 1.0000 - val_loss: 0.6160 - val_accuracy: 1.0000\n",
            "Epoch 1160/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6195 - accuracy: 1.0000 - val_loss: 0.6385 - val_accuracy: 1.0000\n",
            "Epoch 1161/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 1.0000\n",
            "Epoch 1162/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6362 - accuracy: 1.0000 - val_loss: 0.6107 - val_accuracy: 1.0000\n",
            "Epoch 1163/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6713 - accuracy: 1.0000 - val_loss: 0.5998 - val_accuracy: 1.0000\n",
            "Epoch 1164/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6575 - accuracy: 1.0000 - val_loss: 0.6105 - val_accuracy: 1.0000\n",
            "Epoch 1165/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6238 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 1.0000\n",
            "Epoch 1166/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6477 - accuracy: 1.0000 - val_loss: 0.6182 - val_accuracy: 1.0000\n",
            "Epoch 1167/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6598 - accuracy: 1.0000 - val_loss: 0.6414 - val_accuracy: 1.0000\n",
            "Epoch 1168/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6074 - accuracy: 1.0000 - val_loss: 0.7300 - val_accuracy: 1.0000\n",
            "Epoch 1169/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6637 - accuracy: 1.0000 - val_loss: 0.6415 - val_accuracy: 1.0000\n",
            "Epoch 1170/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6252 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 1171/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6504 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
            "Epoch 1172/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6301 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 1.0000\n",
            "Epoch 1173/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6253 - val_accuracy: 1.0000\n",
            "Epoch 1174/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6558 - accuracy: 1.0000 - val_loss: 0.6018 - val_accuracy: 1.0000\n",
            "Epoch 1175/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6562 - accuracy: 1.0000 - val_loss: 0.6294 - val_accuracy: 1.0000\n",
            "Epoch 1176/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6105 - accuracy: 1.0000 - val_loss: 0.6202 - val_accuracy: 1.0000\n",
            "Epoch 1177/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6873 - accuracy: 1.0000 - val_loss: 0.6019 - val_accuracy: 1.0000\n",
            "Epoch 1178/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6172 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 1.0000\n",
            "Epoch 1179/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6252 - accuracy: 1.0000 - val_loss: 0.6373 - val_accuracy: 1.0000\n",
            "Epoch 1180/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6533 - accuracy: 1.0000 - val_loss: 0.7305 - val_accuracy: 1.0000\n",
            "Epoch 1181/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6508 - accuracy: 1.0000 - val_loss: 0.6329 - val_accuracy: 1.0000\n",
            "Epoch 1182/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6271 - accuracy: 1.0000 - val_loss: 0.7500 - val_accuracy: 1.0000\n",
            "Epoch 1183/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6485 - accuracy: 1.0000 - val_loss: 0.6981 - val_accuracy: 1.0000\n",
            "Epoch 1184/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6521 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 1.0000\n",
            "Epoch 1185/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6186 - accuracy: 1.0000 - val_loss: 0.6095 - val_accuracy: 1.0000\n",
            "Epoch 1186/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6419 - accuracy: 1.0000 - val_loss: 0.6428 - val_accuracy: 1.0000\n",
            "Epoch 1187/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6476 - accuracy: 1.0000 - val_loss: 0.6249 - val_accuracy: 1.0000\n",
            "Epoch 1188/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6603 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 1.0000\n",
            "Epoch 1189/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6089 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 1.0000\n",
            "Epoch 1190/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7374 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
            "Epoch 1191/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6007 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 1192/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6296 - accuracy: 1.0000 - val_loss: 0.6023 - val_accuracy: 1.0000\n",
            "Epoch 1193/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 1.0000\n",
            "Epoch 1194/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6678 - accuracy: 1.0000 - val_loss: 0.6186 - val_accuracy: 1.0000\n",
            "Epoch 1195/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6051 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 1.0000\n",
            "Epoch 1196/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6018 - val_accuracy: 1.0000\n",
            "Epoch 1197/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6009 - val_accuracy: 1.0000\n",
            "Epoch 1198/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6472 - accuracy: 1.0000 - val_loss: 0.6892 - val_accuracy: 1.0000\n",
            "Epoch 1199/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6298 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
            "Epoch 1200/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6466 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 1201/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6425 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 1202/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 0.6008 - val_accuracy: 1.0000\n",
            "Epoch 1203/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6593 - accuracy: 1.0000 - val_loss: 0.6027 - val_accuracy: 1.0000\n",
            "Epoch 1204/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6324 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
            "Epoch 1205/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6314 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 1.0000\n",
            "Epoch 1206/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6603 - accuracy: 1.0000 - val_loss: 0.6023 - val_accuracy: 1.0000\n",
            "Epoch 1207/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6493 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 1208/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6414 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
            "Epoch 1209/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6224 - accuracy: 1.0000 - val_loss: 0.7229 - val_accuracy: 1.0000\n",
            "Epoch 1210/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6548 - val_accuracy: 1.0000\n",
            "Epoch 1211/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6498 - accuracy: 1.0000 - val_loss: 0.6107 - val_accuracy: 1.0000\n",
            "Epoch 1212/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 1.5582 - val_accuracy: 1.0000\n",
            "Epoch 1213/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6785 - accuracy: 1.0000 - val_loss: 0.6082 - val_accuracy: 1.0000\n",
            "Epoch 1214/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6226 - accuracy: 1.0000 - val_loss: 0.6118 - val_accuracy: 1.0000\n",
            "Epoch 1215/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6620 - accuracy: 1.0000 - val_loss: 0.6012 - val_accuracy: 1.0000\n",
            "Epoch 1216/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6304 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 1.0000\n",
            "Epoch 1217/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6198 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 1.0000\n",
            "Epoch 1218/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6875 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 1.0000\n",
            "Epoch 1219/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6154 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 1.0000\n",
            "Epoch 1220/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6371 - accuracy: 1.0000 - val_loss: 0.6534 - val_accuracy: 1.0000\n",
            "Epoch 1221/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6393 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 1.0000\n",
            "Epoch 1222/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6765 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 1.0000\n",
            "Epoch 1223/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6136 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 1224/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6861 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 1.0000\n",
            "Epoch 1225/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 1.0000\n",
            "Epoch 1226/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6363 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 1.0000\n",
            "Epoch 1227/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6390 - accuracy: 1.0000 - val_loss: 0.6117 - val_accuracy: 1.0000\n",
            "Epoch 1228/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6758 - accuracy: 1.0000 - val_loss: 0.6291 - val_accuracy: 1.0000\n",
            "Epoch 1229/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6106 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 1230/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6308 - accuracy: 1.0000 - val_loss: 0.6688 - val_accuracy: 1.0000\n",
            "Epoch 1231/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6521 - accuracy: 1.0000 - val_loss: 0.6100 - val_accuracy: 1.0000\n",
            "Epoch 1232/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6464 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 1.0000\n",
            "Epoch 1233/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6618 - accuracy: 1.0000 - val_loss: 0.6533 - val_accuracy: 1.0000\n",
            "Epoch 1234/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6081 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 1235/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 1236/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6324 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 1.0000\n",
            "Epoch 1237/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6661 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 1.0000\n",
            "Epoch 1238/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6340 - accuracy: 1.0000 - val_loss: 0.6607 - val_accuracy: 1.0000\n",
            "Epoch 1239/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.5996 - val_accuracy: 1.0000\n",
            "Epoch 1240/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6158 - accuracy: 1.0000 - val_loss: 0.7894 - val_accuracy: 1.0000\n",
            "Epoch 1241/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6521 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 1.0000\n",
            "Epoch 1242/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6362 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 1.0000\n",
            "Epoch 1243/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 1244/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.6666 - val_accuracy: 1.0000\n",
            "Epoch 1245/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6426 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 1.0000\n",
            "Epoch 1246/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6413 - accuracy: 1.0000 - val_loss: 0.6026 - val_accuracy: 1.0000\n",
            "Epoch 1247/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6773 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
            "Epoch 1248/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6152 - accuracy: 1.0000 - val_loss: 0.7635 - val_accuracy: 1.0000\n",
            "Epoch 1249/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6650 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 1.0000\n",
            "Epoch 1250/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6054 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 1.0000\n",
            "Epoch 1251/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.6972 - val_accuracy: 1.0000\n",
            "Epoch 1252/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6378 - accuracy: 1.0000 - val_loss: 1.1367 - val_accuracy: 1.0000\n",
            "Epoch 1253/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6417 - accuracy: 1.0000 - val_loss: 0.6223 - val_accuracy: 1.0000\n",
            "Epoch 1254/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 1.0000\n",
            "Epoch 1255/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6480 - accuracy: 1.0000 - val_loss: 0.6119 - val_accuracy: 1.0000\n",
            "Epoch 1256/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6440 - accuracy: 1.0000 - val_loss: 0.7723 - val_accuracy: 1.0000\n",
            "Epoch 1257/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6382 - accuracy: 1.0000 - val_loss: 0.6375 - val_accuracy: 1.0000\n",
            "Epoch 1258/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6446 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 1.0000\n",
            "Epoch 1259/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.6279 - val_accuracy: 1.0000\n",
            "Epoch 1260/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6509 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
            "Epoch 1261/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6421 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 1.0000\n",
            "Epoch 1262/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6270 - accuracy: 1.0000 - val_loss: 0.6139 - val_accuracy: 1.0000\n",
            "Epoch 1263/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6399 - accuracy: 1.0000 - val_loss: 0.6314 - val_accuracy: 1.0000\n",
            "Epoch 1264/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6651 - accuracy: 1.0000 - val_loss: 0.6327 - val_accuracy: 1.0000\n",
            "Epoch 1265/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6114 - accuracy: 1.0000 - val_loss: 0.6378 - val_accuracy: 1.0000\n",
            "Epoch 1266/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6468 - accuracy: 1.0000 - val_loss: 0.6683 - val_accuracy: 1.0000\n",
            "Epoch 1267/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 1.0000\n",
            "Epoch 1268/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6269 - accuracy: 1.0000 - val_loss: 0.8383 - val_accuracy: 1.0000\n",
            "Epoch 1269/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6460 - accuracy: 1.0000 - val_loss: 0.6082 - val_accuracy: 1.0000\n",
            "Epoch 1270/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6501 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 1.0000\n",
            "Epoch 1271/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6244 - accuracy: 1.0000 - val_loss: 0.6591 - val_accuracy: 1.0000\n",
            "Epoch 1272/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.5969 - val_accuracy: 1.0000\n",
            "Epoch 1273/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6529 - accuracy: 1.0000 - val_loss: 0.6891 - val_accuracy: 1.0000\n",
            "Epoch 1274/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6203 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 1275/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6663 - accuracy: 1.0000 - val_loss: 0.7245 - val_accuracy: 1.0000\n",
            "Epoch 1276/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6880 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 1.0000\n",
            "Epoch 1277/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6016 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 1.0000\n",
            "Epoch 1278/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6182 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 1.0000\n",
            "Epoch 1279/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6586 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 1.0000\n",
            "Epoch 1280/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6682 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 1.0000\n",
            "Epoch 1281/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6066 - accuracy: 1.0000 - val_loss: 0.6034 - val_accuracy: 1.0000\n",
            "Epoch 1282/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 1.0000\n",
            "Epoch 1283/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6404 - accuracy: 1.0000 - val_loss: 0.6057 - val_accuracy: 1.0000\n",
            "Epoch 1284/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6392 - accuracy: 1.0000 - val_loss: 0.6279 - val_accuracy: 1.0000\n",
            "Epoch 1285/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6678 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 1286/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 0.6223 - val_accuracy: 1.0000\n",
            "Epoch 1287/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6236 - accuracy: 1.0000 - val_loss: 0.6106 - val_accuracy: 1.0000\n",
            "Epoch 1288/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.8392 - val_accuracy: 1.0000\n",
            "Epoch 1289/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6481 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 1.0000\n",
            "Epoch 1290/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6522 - val_accuracy: 1.0000\n",
            "Epoch 1291/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6566 - accuracy: 1.0000 - val_loss: 0.6026 - val_accuracy: 1.0000\n",
            "Epoch 1292/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6190 - accuracy: 1.0000 - val_loss: 0.6107 - val_accuracy: 1.0000\n",
            "Epoch 1293/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6637 - accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 1.0000\n",
            "Epoch 1294/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.8397 - val_accuracy: 1.0000\n",
            "Epoch 1295/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 1.0000 - val_loss: 0.7281 - val_accuracy: 1.0000\n",
            "Epoch 1296/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 1.0000\n",
            "Epoch 1297/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6446 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 1.0000\n",
            "Epoch 1298/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6340 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 1.0000\n",
            "Epoch 1299/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6344 - accuracy: 1.0000 - val_loss: 0.6485 - val_accuracy: 1.0000\n",
            "Epoch 1300/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6411 - accuracy: 1.0000 - val_loss: 0.6272 - val_accuracy: 1.0000\n",
            "Epoch 1301/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6480 - accuracy: 1.0000 - val_loss: 0.6187 - val_accuracy: 1.0000\n",
            "Epoch 1302/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6360 - accuracy: 1.0000 - val_loss: 0.6590 - val_accuracy: 1.0000\n",
            "Epoch 1303/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 1.0000\n",
            "Epoch 1304/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6516 - accuracy: 1.0000 - val_loss: 0.6019 - val_accuracy: 1.0000\n",
            "Epoch 1305/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6267 - accuracy: 1.0000 - val_loss: 0.6134 - val_accuracy: 1.0000\n",
            "Epoch 1306/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6428 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 1.0000\n",
            "Epoch 1307/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.6199 - val_accuracy: 1.0000\n",
            "Epoch 1308/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6463 - accuracy: 1.0000 - val_loss: 0.6068 - val_accuracy: 1.0000\n",
            "Epoch 1309/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 1.0053 - val_accuracy: 1.0000\n",
            "Epoch 1310/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6458 - accuracy: 1.0000 - val_loss: 0.6017 - val_accuracy: 1.0000\n",
            "Epoch 1311/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6325 - accuracy: 1.0000 - val_loss: 0.9161 - val_accuracy: 1.0000\n",
            "Epoch 1312/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6413 - accuracy: 1.0000 - val_loss: 0.7118 - val_accuracy: 1.0000\n",
            "Epoch 1313/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6434 - accuracy: 1.0000 - val_loss: 0.7364 - val_accuracy: 1.0000\n",
            "Epoch 1314/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.6213 - val_accuracy: 1.0000\n",
            "Epoch 1315/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6492 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 1.0000\n",
            "Epoch 1316/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 1.0000\n",
            "Epoch 1317/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.7560 - val_accuracy: 1.0000\n",
            "Epoch 1318/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6648 - accuracy: 1.0000 - val_loss: 0.7746 - val_accuracy: 1.0000\n",
            "Epoch 1319/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6104 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 1.0000\n",
            "Epoch 1320/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6314 - accuracy: 1.0000 - val_loss: 0.6619 - val_accuracy: 1.0000\n",
            "Epoch 1321/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6479 - accuracy: 1.0000 - val_loss: 0.6635 - val_accuracy: 1.0000\n",
            "Epoch 1322/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6331 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 1.0000\n",
            "Epoch 1323/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6332 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 1.0000\n",
            "Epoch 1324/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.7742 - val_accuracy: 1.0000\n",
            "Epoch 1325/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6451 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 1.0000\n",
            "Epoch 1326/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 1.0000\n",
            "Epoch 1327/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6403 - val_accuracy: 1.0000\n",
            "Epoch 1328/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6655 - accuracy: 1.0000 - val_loss: 0.6164 - val_accuracy: 1.0000\n",
            "Epoch 1329/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6293 - accuracy: 1.0000 - val_loss: 0.6035 - val_accuracy: 1.0000\n",
            "Epoch 1330/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6434 - accuracy: 1.0000 - val_loss: 0.6783 - val_accuracy: 1.0000\n",
            "Epoch 1331/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6200 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 1.0000\n",
            "Epoch 1332/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 1.0000\n",
            "Epoch 1333/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 1.0000\n",
            "Epoch 1334/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6394 - accuracy: 1.0000 - val_loss: 0.6813 - val_accuracy: 1.0000\n",
            "Epoch 1335/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6544 - accuracy: 1.0000 - val_loss: 0.6873 - val_accuracy: 1.0000\n",
            "Epoch 1336/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6409 - accuracy: 1.0000 - val_loss: 0.6039 - val_accuracy: 1.0000\n",
            "Epoch 1337/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 1.0000\n",
            "Epoch 1338/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6403 - accuracy: 1.0000 - val_loss: 0.6012 - val_accuracy: 1.0000\n",
            "Epoch 1339/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6304 - accuracy: 1.0000 - val_loss: 0.7787 - val_accuracy: 1.0000\n",
            "Epoch 1340/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6692 - accuracy: 1.0000 - val_loss: 0.6423 - val_accuracy: 1.0000\n",
            "Epoch 1341/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6216 - accuracy: 1.0000 - val_loss: 0.6570 - val_accuracy: 1.0000\n",
            "Epoch 1342/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6360 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 1.0000\n",
            "Epoch 1343/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6457 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 1344/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6345 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 1345/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6565 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 1346/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6295 - accuracy: 1.0000 - val_loss: 0.6940 - val_accuracy: 1.0000\n",
            "Epoch 1347/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6312 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 1348/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6409 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 1349/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6540 - val_accuracy: 1.0000\n",
            "Epoch 1350/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6791 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 1.0000\n",
            "Epoch 1351/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6058 - accuracy: 1.0000 - val_loss: 0.6443 - val_accuracy: 1.0000\n",
            "Epoch 1352/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6334 - accuracy: 1.0000 - val_loss: 0.7144 - val_accuracy: 1.0000\n",
            "Epoch 1353/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6497 - accuracy: 1.0000 - val_loss: 0.7192 - val_accuracy: 1.0000\n",
            "Epoch 1354/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
            "Epoch 1355/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6918 - accuracy: 1.0000 - val_loss: 0.6028 - val_accuracy: 1.0000\n",
            "Epoch 1356/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6027 - accuracy: 1.0000 - val_loss: 0.6240 - val_accuracy: 1.0000\n",
            "Epoch 1357/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6378 - accuracy: 1.0000 - val_loss: 0.6445 - val_accuracy: 1.0000\n",
            "Epoch 1358/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6021 - val_accuracy: 1.0000\n",
            "Epoch 1359/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 1.0000\n",
            "Epoch 1360/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6301 - accuracy: 1.0000 - val_loss: 0.6318 - val_accuracy: 1.0000\n",
            "Epoch 1361/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6676 - accuracy: 1.0000 - val_loss: 0.5971 - val_accuracy: 1.0000\n",
            "Epoch 1362/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6450 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 1363/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6532 - accuracy: 1.0000 - val_loss: 0.6163 - val_accuracy: 1.0000\n",
            "Epoch 1364/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6134 - accuracy: 1.0000 - val_loss: 2.0203 - val_accuracy: 1.0000\n",
            "Epoch 1365/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6643 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 1366/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6300 - accuracy: 1.0000 - val_loss: 0.6619 - val_accuracy: 1.0000\n",
            "Epoch 1367/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6461 - accuracy: 1.0000 - val_loss: 0.6209 - val_accuracy: 1.0000\n",
            "Epoch 1368/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6307 - accuracy: 1.0000 - val_loss: 0.6993 - val_accuracy: 1.0000\n",
            "Epoch 1369/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 1.0000\n",
            "Epoch 1370/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6199 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 1.0000\n",
            "Epoch 1371/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.6018 - val_accuracy: 1.0000\n",
            "Epoch 1372/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 1.0000\n",
            "Epoch 1373/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 1.0000\n",
            "Epoch 1374/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6568 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 1375/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6401 - accuracy: 1.0000 - val_loss: 0.6631 - val_accuracy: 1.0000\n",
            "Epoch 1376/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6176 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 1.0000\n",
            "Epoch 1377/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 0.6433 - val_accuracy: 1.0000\n",
            "Epoch 1378/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 1.0000\n",
            "Epoch 1379/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6480 - accuracy: 1.0000 - val_loss: 0.9087 - val_accuracy: 1.0000\n",
            "Epoch 1380/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6381 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 1.0000\n",
            "Epoch 1381/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6591 - accuracy: 1.0000 - val_loss: 0.5975 - val_accuracy: 1.0000\n",
            "Epoch 1382/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6184 - accuracy: 1.0000 - val_loss: 0.6224 - val_accuracy: 1.0000\n",
            "Epoch 1383/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6363 - accuracy: 1.0000 - val_loss: 1.0684 - val_accuracy: 1.0000\n",
            "Epoch 1384/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6447 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 1.0000\n",
            "Epoch 1385/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6454 - accuracy: 1.0000 - val_loss: 0.6456 - val_accuracy: 1.0000\n",
            "Epoch 1386/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 1.0000\n",
            "Epoch 1387/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6960 - accuracy: 1.0000 - val_loss: 0.6010 - val_accuracy: 1.0000\n",
            "Epoch 1388/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6068 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 1389/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.6528 - val_accuracy: 1.0000\n",
            "Epoch 1390/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.6009 - val_accuracy: 1.0000\n",
            "Epoch 1391/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6310 - accuracy: 1.0000 - val_loss: 0.6933 - val_accuracy: 1.0000\n",
            "Epoch 1392/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6186 - val_accuracy: 1.0000\n",
            "Epoch 1393/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6350 - accuracy: 1.0000 - val_loss: 0.6314 - val_accuracy: 1.0000\n",
            "Epoch 1394/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6476 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 1.0000\n",
            "Epoch 1395/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6454 - accuracy: 1.0000 - val_loss: 0.6379 - val_accuracy: 1.0000\n",
            "Epoch 1396/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 1397/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6533 - accuracy: 1.0000 - val_loss: 0.6167 - val_accuracy: 1.0000\n",
            "Epoch 1398/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6185 - accuracy: 1.0000 - val_loss: 0.6146 - val_accuracy: 1.0000\n",
            "Epoch 1399/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6679 - accuracy: 1.0000 - val_loss: 0.7021 - val_accuracy: 1.0000\n",
            "Epoch 1400/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 0.6004 - val_accuracy: 1.0000\n",
            "Epoch 1401/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6247 - accuracy: 1.0000 - val_loss: 0.6026 - val_accuracy: 1.0000\n",
            "Epoch 1402/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.6408 - val_accuracy: 1.0000\n",
            "Epoch 1403/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6415 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 1.0000\n",
            "Epoch 1404/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6328 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 1.0000\n",
            "Epoch 1405/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6579 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 1406/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6263 - accuracy: 1.0000 - val_loss: 0.6351 - val_accuracy: 1.0000\n",
            "Epoch 1407/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6510 - accuracy: 1.0000 - val_loss: 0.6118 - val_accuracy: 1.0000\n",
            "Epoch 1408/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6405 - accuracy: 1.0000 - val_loss: 0.6251 - val_accuracy: 1.0000\n",
            "Epoch 1409/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6278 - accuracy: 1.0000 - val_loss: 0.6019 - val_accuracy: 1.0000\n",
            "Epoch 1410/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6304 - accuracy: 1.0000 - val_loss: 0.6156 - val_accuracy: 1.0000\n",
            "Epoch 1411/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6486 - accuracy: 1.0000 - val_loss: 1.2445 - val_accuracy: 1.0000\n",
            "Epoch 1412/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6334 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 1.0000\n",
            "Epoch 1413/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6381 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 1.0000\n",
            "Epoch 1414/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6545 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 1.0000\n",
            "Epoch 1415/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6501 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
            "Epoch 1416/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6186 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 1.0000\n",
            "Epoch 1417/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 1418/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6422 - accuracy: 1.0000 - val_loss: 0.6741 - val_accuracy: 1.0000\n",
            "Epoch 1419/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6381 - accuracy: 1.0000 - val_loss: 0.6119 - val_accuracy: 1.0000\n",
            "Epoch 1420/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6397 - accuracy: 1.0000 - val_loss: 0.6350 - val_accuracy: 1.0000\n",
            "Epoch 1421/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6431 - accuracy: 1.0000 - val_loss: 0.6194 - val_accuracy: 1.0000\n",
            "Epoch 1422/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6547 - accuracy: 1.0000 - val_loss: 0.6236 - val_accuracy: 1.0000\n",
            "Epoch 1423/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6282 - accuracy: 1.0000 - val_loss: 0.6692 - val_accuracy: 1.0000\n",
            "Epoch 1424/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6455 - accuracy: 1.0000 - val_loss: 0.6178 - val_accuracy: 1.0000\n",
            "Epoch 1425/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6220 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 1.0000\n",
            "Epoch 1426/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6307 - accuracy: 1.0000 - val_loss: 0.7292 - val_accuracy: 1.0000\n",
            "Epoch 1427/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6496 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
            "Epoch 1428/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6429 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 1.0000\n",
            "Epoch 1429/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6446 - accuracy: 1.0000 - val_loss: 0.6328 - val_accuracy: 1.0000\n",
            "Epoch 1430/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6712 - accuracy: 1.0000 - val_loss: 0.6197 - val_accuracy: 1.0000\n",
            "Epoch 1431/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6063 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 1.0000\n",
            "Epoch 1432/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.6155 - val_accuracy: 1.0000\n",
            "Epoch 1433/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6431 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
            "Epoch 1434/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
            "Epoch 1435/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6456 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 1.0000\n",
            "Epoch 1436/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6294 - accuracy: 1.0000 - val_loss: 0.6027 - val_accuracy: 1.0000\n",
            "Epoch 1437/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6580 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
            "Epoch 1438/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6383 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 1.0000\n",
            "Epoch 1439/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6364 - accuracy: 1.0000 - val_loss: 0.6258 - val_accuracy: 1.0000\n",
            "Epoch 1440/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6185 - accuracy: 1.0000 - val_loss: 0.6902 - val_accuracy: 1.0000\n",
            "Epoch 1441/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6610 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 1.0000\n",
            "Epoch 1442/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6233 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 1.0000\n",
            "Epoch 1443/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.5971 - val_accuracy: 1.0000\n",
            "Epoch 1444/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6485 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 1.0000\n",
            "Epoch 1445/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6296 - accuracy: 1.0000 - val_loss: 0.6799 - val_accuracy: 1.0000\n",
            "Epoch 1446/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6392 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 1.0000\n",
            "Epoch 1447/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6406 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 1448/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6401 - accuracy: 1.0000 - val_loss: 0.6840 - val_accuracy: 1.0000\n",
            "Epoch 1449/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6245 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 1.0000\n",
            "Epoch 1450/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6540 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 1.0000\n",
            "Epoch 1451/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6296 - accuracy: 1.0000 - val_loss: 0.7207 - val_accuracy: 1.0000\n",
            "Epoch 1452/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6411 - accuracy: 1.0000 - val_loss: 0.6034 - val_accuracy: 1.0000\n",
            "Epoch 1453/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6384 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 1.0000\n",
            "Epoch 1454/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6429 - accuracy: 1.0000 - val_loss: 0.6646 - val_accuracy: 1.0000\n",
            "Epoch 1455/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6317 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 1.0000\n",
            "Epoch 1456/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6608 - accuracy: 1.0000 - val_loss: 0.7055 - val_accuracy: 1.0000\n",
            "Epoch 1457/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6567 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 1.0000\n",
            "Epoch 1458/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6004 - val_accuracy: 1.0000\n",
            "Epoch 1459/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6268 - val_accuracy: 1.0000\n",
            "Epoch 1460/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6462 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 1.0000\n",
            "Epoch 1461/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6058 - accuracy: 1.0000 - val_loss: 0.6154 - val_accuracy: 1.0000\n",
            "Epoch 1462/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6540 - accuracy: 1.0000 - val_loss: 0.6080 - val_accuracy: 1.0000\n",
            "Epoch 1463/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 1.0000\n",
            "Epoch 1464/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6697 - accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 1.0000\n",
            "Epoch 1465/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6140 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 1466/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6606 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 1.0000\n",
            "Epoch 1467/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6248 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 1.0000\n",
            "Epoch 1468/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6378 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 1.0000\n",
            "Epoch 1469/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6394 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 1.0000\n",
            "Epoch 1470/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6455 - accuracy: 1.0000 - val_loss: 0.6326 - val_accuracy: 1.0000\n",
            "Epoch 1471/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6390 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 1.0000\n",
            "Epoch 1472/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6191 - accuracy: 1.0000 - val_loss: 0.6611 - val_accuracy: 1.0000\n",
            "Epoch 1473/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6889 - accuracy: 1.0000 - val_loss: 0.6035 - val_accuracy: 1.0000\n",
            "Epoch 1474/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6373 - accuracy: 1.0000 - val_loss: 0.5984 - val_accuracy: 1.0000\n",
            "Epoch 1475/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6294 - accuracy: 1.0000 - val_loss: 0.6331 - val_accuracy: 1.0000\n",
            "Epoch 1476/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6387 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 1.0000\n",
            "Epoch 1477/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6306 - accuracy: 1.0000 - val_loss: 0.6183 - val_accuracy: 1.0000\n",
            "Epoch 1478/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 1.0000\n",
            "Epoch 1479/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6214 - accuracy: 1.0000 - val_loss: 0.7357 - val_accuracy: 1.0000\n",
            "Epoch 1480/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7198 - accuracy: 1.0000 - val_loss: 0.5983 - val_accuracy: 1.0000\n",
            "Epoch 1481/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6324 - accuracy: 1.0000 - val_loss: 0.6169 - val_accuracy: 1.0000\n",
            "Epoch 1482/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6279 - accuracy: 1.0000 - val_loss: 0.6012 - val_accuracy: 1.0000\n",
            "Epoch 1483/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6363 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 1484/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6629 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 1.0000\n",
            "Epoch 1485/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6181 - accuracy: 1.0000 - val_loss: 0.8314 - val_accuracy: 1.0000\n",
            "Epoch 1486/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6486 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 1.0000\n",
            "Epoch 1487/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6225 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 1.0000\n",
            "Epoch 1488/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6452 - accuracy: 1.0000 - val_loss: 0.6029 - val_accuracy: 1.0000\n",
            "Epoch 1489/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6203 - accuracy: 1.0000 - val_loss: 0.6915 - val_accuracy: 1.0000\n",
            "Epoch 1490/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6551 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 1491/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.5999 - val_accuracy: 1.0000\n",
            "Epoch 1492/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6258 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 1493/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6381 - accuracy: 1.0000 - val_loss: 0.6004 - val_accuracy: 1.0000\n",
            "Epoch 1494/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6322 - accuracy: 1.0000 - val_loss: 0.6819 - val_accuracy: 1.0000\n",
            "Epoch 1495/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6512 - accuracy: 1.0000 - val_loss: 0.6033 - val_accuracy: 1.0000\n",
            "Epoch 1496/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6358 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 1.0000\n",
            "Epoch 1497/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6536 - accuracy: 1.0000 - val_loss: 0.6034 - val_accuracy: 1.0000\n",
            "Epoch 1498/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6252 - accuracy: 1.0000 - val_loss: 0.6348 - val_accuracy: 1.0000\n",
            "Epoch 1499/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6451 - accuracy: 1.0000 - val_loss: 0.6356 - val_accuracy: 1.0000\n",
            "Epoch 1500/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6540 - accuracy: 1.0000 - val_loss: 0.6919 - val_accuracy: 1.0000\n",
            "Epoch 1501/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6166 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 1.0000\n",
            "Epoch 1502/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6462 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
            "Epoch 1503/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6347 - accuracy: 1.0000 - val_loss: 0.7138 - val_accuracy: 1.0000\n",
            "Epoch 1504/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6386 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 1.0000\n",
            "Epoch 1505/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 0.6235 - val_accuracy: 1.0000\n",
            "Epoch 1506/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6393 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 1.0000\n",
            "Epoch 1507/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6815 - accuracy: 1.0000 - val_loss: 0.7051 - val_accuracy: 1.0000\n",
            "Epoch 1508/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6062 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
            "Epoch 1509/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6341 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 1510/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6406 - accuracy: 1.0000 - val_loss: 0.6393 - val_accuracy: 1.0000\n",
            "Epoch 1511/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6275 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 1.0000\n",
            "Epoch 1512/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6455 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 1.0000\n",
            "Epoch 1513/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6458 - val_accuracy: 1.0000\n",
            "Epoch 1514/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6605 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 1.0000\n",
            "Epoch 1515/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6188 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 1516/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6438 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
            "Epoch 1517/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6472 - accuracy: 1.0000 - val_loss: 0.5994 - val_accuracy: 1.0000\n",
            "Epoch 1518/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6603 - accuracy: 1.0000 - val_loss: 0.7243 - val_accuracy: 1.0000\n",
            "Epoch 1519/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6066 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 1.0000\n",
            "Epoch 1520/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 1521/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6187 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 1522/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6741 - accuracy: 1.0000 - val_loss: 0.6375 - val_accuracy: 1.0000\n",
            "Epoch 1523/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6060 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 1524/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6464 - accuracy: 1.0000 - val_loss: 0.7449 - val_accuracy: 1.0000\n",
            "Epoch 1525/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6343 - accuracy: 1.0000 - val_loss: 0.5996 - val_accuracy: 1.0000\n",
            "Epoch 1526/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6416 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 1.0000\n",
            "Epoch 1527/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.6004 - val_accuracy: 1.0000\n",
            "Epoch 1528/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6140 - accuracy: 1.0000 - val_loss: 0.6737 - val_accuracy: 1.0000\n",
            "Epoch 1529/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6665 - accuracy: 1.0000 - val_loss: 0.6280 - val_accuracy: 1.0000\n",
            "Epoch 1530/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6096 - accuracy: 1.0000 - val_loss: 0.6709 - val_accuracy: 1.0000\n",
            "Epoch 1531/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6419 - accuracy: 1.0000 - val_loss: 0.6112 - val_accuracy: 1.0000\n",
            "Epoch 1532/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6641 - accuracy: 1.0000 - val_loss: 0.6950 - val_accuracy: 1.0000\n",
            "Epoch 1533/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6100 - accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 1.0000\n",
            "Epoch 1534/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6320 - accuracy: 1.0000 - val_loss: 0.6166 - val_accuracy: 1.0000\n",
            "Epoch 1535/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.6010 - val_accuracy: 1.0000\n",
            "Epoch 1536/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6331 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 1537/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6487 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 1.0000\n",
            "Epoch 1538/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6396 - accuracy: 1.0000 - val_loss: 0.6616 - val_accuracy: 1.0000\n",
            "Epoch 1539/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6386 - accuracy: 1.0000 - val_loss: 0.6015 - val_accuracy: 1.0000\n",
            "Epoch 1540/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6275 - accuracy: 1.0000 - val_loss: 0.6886 - val_accuracy: 1.0000\n",
            "Epoch 1541/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6374 - accuracy: 1.0000 - val_loss: 0.6324 - val_accuracy: 1.0000\n",
            "Epoch 1542/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 1.0000\n",
            "Epoch 1543/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6342 - accuracy: 1.0000 - val_loss: 0.6114 - val_accuracy: 1.0000\n",
            "Epoch 1544/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 1545/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6354 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 1.0000\n",
            "Epoch 1546/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6509 - accuracy: 1.0000 - val_loss: 0.6203 - val_accuracy: 1.0000\n",
            "Epoch 1547/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6144 - accuracy: 1.0000 - val_loss: 0.6311 - val_accuracy: 1.0000\n",
            "Epoch 1548/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6551 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 1.0000\n",
            "Epoch 1549/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6238 - accuracy: 1.0000 - val_loss: 0.6151 - val_accuracy: 1.0000\n",
            "Epoch 1550/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.6361 - val_accuracy: 1.0000\n",
            "Epoch 1551/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6360 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 1.0000\n",
            "Epoch 1552/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6626 - accuracy: 1.0000 - val_loss: 0.6480 - val_accuracy: 1.0000\n",
            "Epoch 1553/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6320 - accuracy: 1.0000 - val_loss: 0.6359 - val_accuracy: 1.0000\n",
            "Epoch 1554/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6396 - accuracy: 1.0000 - val_loss: 0.6580 - val_accuracy: 1.0000\n",
            "Epoch 1555/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6335 - accuracy: 1.0000 - val_loss: 0.6019 - val_accuracy: 1.0000\n",
            "Epoch 1556/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 1.4054 - val_accuracy: 1.0000\n",
            "Epoch 1557/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6833 - accuracy: 1.0000 - val_loss: 0.5999 - val_accuracy: 1.0000\n",
            "Epoch 1558/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6014 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 1.0000\n",
            "Epoch 1559/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6656 - accuracy: 1.0000 - val_loss: 0.6339 - val_accuracy: 1.0000\n",
            "Epoch 1560/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6224 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 1561/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6345 - accuracy: 1.0000 - val_loss: 0.6062 - val_accuracy: 1.0000\n",
            "Epoch 1562/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7067 - accuracy: 1.0000 - val_loss: 0.8292 - val_accuracy: 1.0000\n",
            "Epoch 1563/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6371 - accuracy: 1.0000 - val_loss: 0.5982 - val_accuracy: 1.0000\n",
            "Epoch 1564/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6337 - accuracy: 1.0000 - val_loss: 0.6177 - val_accuracy: 1.0000\n",
            "Epoch 1565/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6874 - accuracy: 1.0000 - val_loss: 0.7187 - val_accuracy: 1.0000\n",
            "Epoch 1566/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6036 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 1.0000\n",
            "Epoch 1567/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6458 - accuracy: 1.0000 - val_loss: 0.6977 - val_accuracy: 1.0000\n",
            "Epoch 1568/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6353 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 1.0000\n",
            "Epoch 1569/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 0.5962 - val_accuracy: 1.0000\n",
            "Epoch 1570/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6369 - accuracy: 1.0000 - val_loss: 0.6615 - val_accuracy: 1.0000\n",
            "Epoch 1571/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6156 - val_accuracy: 1.0000\n",
            "Epoch 1572/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6366 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 1573/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6431 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 1574/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6411 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 1.0000\n",
            "Epoch 1575/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6225 - accuracy: 1.0000 - val_loss: 0.6235 - val_accuracy: 1.0000\n",
            "Epoch 1576/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6490 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 1577/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6680 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 1.0000\n",
            "Epoch 1578/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6046 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 1.0000\n",
            "Epoch 1579/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6509 - accuracy: 1.0000 - val_loss: 0.6129 - val_accuracy: 1.0000\n",
            "Epoch 1580/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6997 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 1.0000\n",
            "Epoch 1581/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6422 - accuracy: 1.0000 - val_loss: 0.5991 - val_accuracy: 1.0000\n",
            "Epoch 1582/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6173 - accuracy: 1.0000 - val_loss: 0.6483 - val_accuracy: 1.0000\n",
            "Epoch 1583/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
            "Epoch 1584/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6269 - accuracy: 1.0000 - val_loss: 0.6453 - val_accuracy: 1.0000\n",
            "Epoch 1585/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6454 - accuracy: 1.0000 - val_loss: 0.6196 - val_accuracy: 1.0000\n",
            "Epoch 1586/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 1.0000\n",
            "Epoch 1587/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6295 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 1.0000\n",
            "Epoch 1588/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6501 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
            "Epoch 1589/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6420 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 1590/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6305 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 1.0000\n",
            "Epoch 1591/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6817 - accuracy: 1.0000 - val_loss: 0.5987 - val_accuracy: 1.0000\n",
            "Epoch 1592/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6308 - accuracy: 1.0000 - val_loss: 0.6269 - val_accuracy: 1.0000\n",
            "Epoch 1593/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6435 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 1.0000\n",
            "Epoch 1594/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6307 - accuracy: 1.0000 - val_loss: 0.6139 - val_accuracy: 1.0000\n",
            "Epoch 1595/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6737 - accuracy: 1.0000 - val_loss: 0.6754 - val_accuracy: 1.0000\n",
            "Epoch 1596/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6029 - accuracy: 1.0000 - val_loss: 0.6431 - val_accuracy: 1.0000\n",
            "Epoch 1597/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6310 - accuracy: 1.0000 - val_loss: 0.6084 - val_accuracy: 1.0000\n",
            "Epoch 1598/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6553 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
            "Epoch 1599/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6182 - accuracy: 1.0000 - val_loss: 0.6039 - val_accuracy: 1.0000\n",
            "Epoch 1600/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6327 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 1.0000\n",
            "Epoch 1601/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6481 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 1602/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6256 - accuracy: 1.0000 - val_loss: 0.7240 - val_accuracy: 1.0000\n",
            "Epoch 1603/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6501 - accuracy: 1.0000 - val_loss: 0.6413 - val_accuracy: 1.0000\n",
            "Epoch 1604/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6423 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 1.0000\n",
            "Epoch 1605/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6627 - accuracy: 1.0000 - val_loss: 0.6194 - val_accuracy: 1.0000\n",
            "Epoch 1606/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 1607/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6474 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 1.0000\n",
            "Epoch 1608/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6129 - accuracy: 1.0000 - val_loss: 0.6951 - val_accuracy: 1.0000\n",
            "Epoch 1609/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6346 - accuracy: 1.0000 - val_loss: 0.6361 - val_accuracy: 1.0000\n",
            "Epoch 1610/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6528 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 1.0000\n",
            "Epoch 1611/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6328 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 1612/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6245 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 1.0000\n",
            "Epoch 1613/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 1614/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6406 - accuracy: 1.0000 - val_loss: 0.6329 - val_accuracy: 1.0000\n",
            "Epoch 1615/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6634 - val_accuracy: 1.0000\n",
            "Epoch 1616/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6996 - accuracy: 1.0000 - val_loss: 0.5952 - val_accuracy: 1.0000\n",
            "Epoch 1617/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6408 - accuracy: 1.0000 - val_loss: 0.6308 - val_accuracy: 1.0000\n",
            "Epoch 1618/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6230 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 1.0000\n",
            "Epoch 1619/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6407 - val_accuracy: 1.0000\n",
            "Epoch 1620/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6268 - accuracy: 1.0000 - val_loss: 0.5979 - val_accuracy: 1.0000\n",
            "Epoch 1621/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6411 - accuracy: 1.0000 - val_loss: 0.6635 - val_accuracy: 1.0000\n",
            "Epoch 1622/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6481 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 1.0000\n",
            "Epoch 1623/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6312 - accuracy: 1.0000 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
            "Epoch 1624/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6287 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 1.0000\n",
            "Epoch 1625/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6576 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 1.0000\n",
            "Epoch 1626/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6556 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 1.0000\n",
            "Epoch 1627/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6135 - accuracy: 1.0000 - val_loss: 0.6849 - val_accuracy: 1.0000\n",
            "Epoch 1628/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6614 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 1.0000\n",
            "Epoch 1629/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6162 - accuracy: 1.0000 - val_loss: 0.6115 - val_accuracy: 1.0000\n",
            "Epoch 1630/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6364 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 1.0000\n",
            "Epoch 1631/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6724 - accuracy: 1.0000 - val_loss: 0.6244 - val_accuracy: 1.0000\n",
            "Epoch 1632/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6166 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 1.0000\n",
            "Epoch 1633/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6583 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 1.0000\n",
            "Epoch 1634/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6217 - accuracy: 1.0000 - val_loss: 0.6544 - val_accuracy: 1.0000\n",
            "Epoch 1635/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6505 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 1.0000\n",
            "Epoch 1636/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6175 - accuracy: 1.0000 - val_loss: 0.6107 - val_accuracy: 1.0000\n",
            "Epoch 1637/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6448 - accuracy: 1.0000 - val_loss: 0.5976 - val_accuracy: 1.0000\n",
            "Epoch 1638/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 1.0000\n",
            "Epoch 1639/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6684 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 1640/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6010 - accuracy: 1.0000 - val_loss: 0.6220 - val_accuracy: 1.0000\n",
            "Epoch 1641/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6456 - accuracy: 1.0000 - val_loss: 0.6052 - val_accuracy: 1.0000\n",
            "Epoch 1642/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6673 - accuracy: 1.0000 - val_loss: 0.6215 - val_accuracy: 1.0000\n",
            "Epoch 1643/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6055 - accuracy: 1.0000 - val_loss: 0.6167 - val_accuracy: 1.0000\n",
            "Epoch 1644/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 1.0000\n",
            "Epoch 1645/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6663 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 1646/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6165 - accuracy: 1.0000 - val_loss: 0.7092 - val_accuracy: 1.0000\n",
            "Epoch 1647/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6335 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 1.0000\n",
            "Epoch 1648/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6391 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 1.0000\n",
            "Epoch 1649/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6424 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 1.0000\n",
            "Epoch 1650/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6244 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 1.0000\n",
            "Epoch 1651/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 1.0000\n",
            "Epoch 1652/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6405 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 1.0000\n",
            "Epoch 1653/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6336 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 1.0000\n",
            "Epoch 1654/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6441 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 1.0000\n",
            "Epoch 1655/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6637 - accuracy: 1.0000 - val_loss: 0.7101 - val_accuracy: 1.0000\n",
            "Epoch 1656/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6266 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 1.0000\n",
            "Epoch 1657/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6319 - accuracy: 1.0000 - val_loss: 0.6490 - val_accuracy: 1.0000\n",
            "Epoch 1658/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6489 - accuracy: 1.0000 - val_loss: 0.6117 - val_accuracy: 1.0000\n",
            "Epoch 1659/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 1.0000\n",
            "Epoch 1660/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
            "Epoch 1661/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6292 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 1.0000\n",
            "Epoch 1662/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6511 - accuracy: 1.0000 - val_loss: 0.9408 - val_accuracy: 1.0000\n",
            "Epoch 1663/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6177 - accuracy: 1.0000 - val_loss: 0.5975 - val_accuracy: 1.0000\n",
            "Epoch 1664/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.7931 - val_accuracy: 1.0000\n",
            "Epoch 1665/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6610 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 1666/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6106 - accuracy: 1.0000 - val_loss: 0.5999 - val_accuracy: 1.0000\n",
            "Epoch 1667/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6716 - accuracy: 1.0000 - val_loss: 0.6208 - val_accuracy: 1.0000\n",
            "Epoch 1668/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6016 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 1669/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6377 - accuracy: 1.0000 - val_loss: 0.6011 - val_accuracy: 1.0000\n",
            "Epoch 1670/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6643 - accuracy: 1.0000 - val_loss: 0.6762 - val_accuracy: 1.0000\n",
            "Epoch 1671/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6279 - accuracy: 1.0000 - val_loss: 0.7267 - val_accuracy: 1.0000\n",
            "Epoch 1672/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6342 - accuracy: 1.0000 - val_loss: 0.5955 - val_accuracy: 1.0000\n",
            "Epoch 1673/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6275 - accuracy: 1.0000 - val_loss: 0.6558 - val_accuracy: 1.0000\n",
            "Epoch 1674/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6102 - val_accuracy: 1.0000\n",
            "Epoch 1675/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6371 - accuracy: 1.0000 - val_loss: 0.8009 - val_accuracy: 1.0000\n",
            "Epoch 1676/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 1.0000\n",
            "Epoch 1677/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6302 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 1.0000\n",
            "Epoch 1678/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6369 - accuracy: 1.0000 - val_loss: 0.5991 - val_accuracy: 1.0000\n",
            "Epoch 1679/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6703 - accuracy: 1.0000 - val_loss: 0.9130 - val_accuracy: 1.0000\n",
            "Epoch 1680/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6408 - accuracy: 1.0000 - val_loss: 0.7493 - val_accuracy: 1.0000\n",
            "Epoch 1681/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6275 - accuracy: 1.0000 - val_loss: 0.6836 - val_accuracy: 1.0000\n",
            "Epoch 1682/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6390 - accuracy: 1.0000 - val_loss: 0.5996 - val_accuracy: 1.0000\n",
            "Epoch 1683/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 2.3459 - val_accuracy: 1.0000\n",
            "Epoch 1684/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6483 - accuracy: 1.0000 - val_loss: 0.6062 - val_accuracy: 1.0000\n",
            "Epoch 1685/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6123 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 1686/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6409 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 1.0000\n",
            "Epoch 1687/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6384 - accuracy: 1.0000 - val_loss: 0.6508 - val_accuracy: 1.0000\n",
            "Epoch 1688/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6342 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 1.0000\n",
            "Epoch 1689/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 1.0000\n",
            "Epoch 1690/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6540 - accuracy: 1.0000 - val_loss: 0.9040 - val_accuracy: 1.0000\n",
            "Epoch 1691/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6333 - accuracy: 1.0000 - val_loss: 0.6966 - val_accuracy: 1.0000\n",
            "Epoch 1692/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6202 - val_accuracy: 1.0000\n",
            "Epoch 1693/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6386 - accuracy: 1.0000 - val_loss: 0.6539 - val_accuracy: 1.0000\n",
            "Epoch 1694/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6254 - accuracy: 1.0000 - val_loss: 0.6062 - val_accuracy: 1.0000\n",
            "Epoch 1695/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6344 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 1.0000\n",
            "Epoch 1696/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 1.0000\n",
            "Epoch 1697/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6434 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 1698/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6244 - accuracy: 1.0000 - val_loss: 0.6252 - val_accuracy: 1.0000\n",
            "Epoch 1699/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6582 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 1700/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6226 - accuracy: 1.0000 - val_loss: 0.6338 - val_accuracy: 1.0000\n",
            "Epoch 1701/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6609 - accuracy: 1.0000 - val_loss: 0.6182 - val_accuracy: 1.0000\n",
            "Epoch 1702/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6159 - accuracy: 1.0000 - val_loss: 0.6130 - val_accuracy: 1.0000\n",
            "Epoch 1703/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6324 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 1.0000\n",
            "Epoch 1704/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6360 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 1.0000\n",
            "Epoch 1705/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6378 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 1.0000\n",
            "Epoch 1706/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6271 - accuracy: 1.0000 - val_loss: 0.6165 - val_accuracy: 1.0000\n",
            "Epoch 1707/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6548 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 1.0000\n",
            "Epoch 1708/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6251 - accuracy: 1.0000 - val_loss: 0.6106 - val_accuracy: 1.0000\n",
            "Epoch 1709/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6337 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 1.0000\n",
            "Epoch 1710/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6563 - accuracy: 1.0000 - val_loss: 0.6116 - val_accuracy: 1.0000\n",
            "Epoch 1711/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6302 - accuracy: 1.0000 - val_loss: 0.5970 - val_accuracy: 1.0000\n",
            "Epoch 1712/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6670 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 1.0000\n",
            "Epoch 1713/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6010 - accuracy: 1.0000 - val_loss: 0.6129 - val_accuracy: 1.0000\n",
            "Epoch 1714/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6554 - accuracy: 1.0000 - val_loss: 0.5988 - val_accuracy: 1.0000\n",
            "Epoch 1715/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6221 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 1.0000\n",
            "Epoch 1716/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6309 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 1.0000\n",
            "Epoch 1717/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6331 - accuracy: 1.0000 - val_loss: 0.6535 - val_accuracy: 1.0000\n",
            "Epoch 1718/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 1.0000\n",
            "Epoch 1719/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6590 - accuracy: 1.0000 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
            "Epoch 1720/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6258 - accuracy: 1.0000 - val_loss: 0.6138 - val_accuracy: 1.0000\n",
            "Epoch 1721/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 1.0000\n",
            "Epoch 1722/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6301 - accuracy: 1.0000 - val_loss: 0.6000 - val_accuracy: 1.0000\n",
            "Epoch 1723/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
            "Epoch 1724/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6744 - accuracy: 1.0000 - val_loss: 0.6251 - val_accuracy: 1.0000\n",
            "Epoch 1725/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6014 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 1.0000\n",
            "Epoch 1726/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 1.0000\n",
            "Epoch 1727/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6399 - accuracy: 1.0000 - val_loss: 0.7399 - val_accuracy: 1.0000\n",
            "Epoch 1728/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6387 - accuracy: 1.0000 - val_loss: 0.6370 - val_accuracy: 1.0000\n",
            "Epoch 1729/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6279 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 1.0000\n",
            "Epoch 1730/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6422 - accuracy: 1.0000 - val_loss: 0.6678 - val_accuracy: 1.0000\n",
            "Epoch 1731/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6502 - accuracy: 1.0000 - val_loss: 0.6848 - val_accuracy: 1.0000\n",
            "Epoch 1732/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6264 - accuracy: 1.0000 - val_loss: 0.6463 - val_accuracy: 1.0000\n",
            "Epoch 1733/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6425 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 1.0000\n",
            "Epoch 1734/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6246 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 1.0000\n",
            "Epoch 1735/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6416 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 1.0000\n",
            "Epoch 1736/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6410 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 1.0000\n",
            "Epoch 1737/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6364 - accuracy: 1.0000 - val_loss: 0.6155 - val_accuracy: 1.0000\n",
            "Epoch 1738/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6276 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 1.0000\n",
            "Epoch 1739/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6426 - accuracy: 1.0000 - val_loss: 0.6328 - val_accuracy: 1.0000\n",
            "Epoch 1740/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6499 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 1.0000\n",
            "Epoch 1741/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6167 - accuracy: 1.0000 - val_loss: 0.6348 - val_accuracy: 1.0000\n",
            "Epoch 1742/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6285 - accuracy: 1.0000 - val_loss: 0.6042 - val_accuracy: 1.0000\n",
            "Epoch 1743/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6395 - accuracy: 1.0000 - val_loss: 0.6694 - val_accuracy: 1.0000\n",
            "Epoch 1744/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6733 - accuracy: 1.0000 - val_loss: 0.9062 - val_accuracy: 1.0000\n",
            "Epoch 1745/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6185 - accuracy: 1.0000 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
            "Epoch 1746/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6271 - accuracy: 1.0000 - val_loss: 0.6148 - val_accuracy: 1.0000\n",
            "Epoch 1747/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6366 - accuracy: 1.0000 - val_loss: 0.6151 - val_accuracy: 1.0000\n",
            "Epoch 1748/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6344 - accuracy: 1.0000 - val_loss: 0.6478 - val_accuracy: 1.0000\n",
            "Epoch 1749/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 1.0000\n",
            "Epoch 1750/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 0.7260 - val_accuracy: 1.0000\n",
            "Epoch 1751/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6264 - accuracy: 1.0000 - val_loss: 0.6285 - val_accuracy: 1.0000\n",
            "Epoch 1752/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6337 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 1.0000\n",
            "Epoch 1753/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6474 - accuracy: 1.0000 - val_loss: 0.5953 - val_accuracy: 1.0000\n",
            "Epoch 1754/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6212 - accuracy: 1.0000 - val_loss: 0.6415 - val_accuracy: 1.0000\n",
            "Epoch 1755/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6460 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 1.0000\n",
            "Epoch 1756/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6417 - accuracy: 1.0000 - val_loss: 0.6342 - val_accuracy: 1.0000\n",
            "Epoch 1757/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6378 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 1.0000\n",
            "Epoch 1758/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6348 - accuracy: 1.0000 - val_loss: 0.6543 - val_accuracy: 1.0000\n",
            "Epoch 1759/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6376 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 1.0000\n",
            "Epoch 1760/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6388 - accuracy: 1.0000 - val_loss: 0.6241 - val_accuracy: 1.0000\n",
            "Epoch 1761/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6164 - accuracy: 1.0000 - val_loss: 0.6185 - val_accuracy: 1.0000\n",
            "Epoch 1762/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6491 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 1.0000\n",
            "Epoch 1763/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6382 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 1.0000\n",
            "Epoch 1764/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6266 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 1.0000\n",
            "Epoch 1765/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6770 - accuracy: 1.0000 - val_loss: 0.6147 - val_accuracy: 1.0000\n",
            "Epoch 1766/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6006 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 1.0000\n",
            "Epoch 1767/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6335 - accuracy: 1.0000 - val_loss: 0.6084 - val_accuracy: 1.0000\n",
            "Epoch 1768/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6364 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 1.0000\n",
            "Epoch 1769/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.6386 - val_accuracy: 1.0000\n",
            "Epoch 1770/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6424 - accuracy: 1.0000 - val_loss: 0.6154 - val_accuracy: 1.0000\n",
            "Epoch 1771/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.7119 - val_accuracy: 1.0000\n",
            "Epoch 1772/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6716 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
            "Epoch 1773/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6076 - accuracy: 1.0000 - val_loss: 0.6746 - val_accuracy: 1.0000\n",
            "Epoch 1774/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6329 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 1.0000\n",
            "Epoch 1775/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6354 - accuracy: 1.0000 - val_loss: 0.6510 - val_accuracy: 1.0000\n",
            "Epoch 1776/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6381 - accuracy: 1.0000 - val_loss: 0.6474 - val_accuracy: 1.0000\n",
            "Epoch 1777/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6350 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 1778/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 1.0000\n",
            "Epoch 1779/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6524 - accuracy: 1.0000 - val_loss: 0.6937 - val_accuracy: 1.0000\n",
            "Epoch 1780/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6508 - accuracy: 1.0000 - val_loss: 0.6177 - val_accuracy: 1.0000\n",
            "Epoch 1781/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6063 - accuracy: 1.0000 - val_loss: 0.6239 - val_accuracy: 1.0000\n",
            "Epoch 1782/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6332 - accuracy: 1.0000 - val_loss: 0.6234 - val_accuracy: 1.0000\n",
            "Epoch 1783/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6655 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 1.0000\n",
            "Epoch 1784/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6265 - accuracy: 1.0000 - val_loss: 0.6822 - val_accuracy: 1.0000\n",
            "Epoch 1785/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6403 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 1786/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6197 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 1.0000\n",
            "Epoch 1787/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6527 - accuracy: 1.0000 - val_loss: 0.6540 - val_accuracy: 1.0000\n",
            "Epoch 1788/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6449 - accuracy: 1.0000 - val_loss: 0.6669 - val_accuracy: 1.0000\n",
            "Epoch 1789/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.5991 - val_accuracy: 1.0000\n",
            "Epoch 1790/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.6994 - val_accuracy: 1.0000\n",
            "Epoch 1791/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6262 - accuracy: 1.0000 - val_loss: 0.6085 - val_accuracy: 1.0000\n",
            "Epoch 1792/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6595 - accuracy: 1.0000 - val_loss: 0.5949 - val_accuracy: 1.0000\n",
            "Epoch 1793/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6533 - accuracy: 1.0000 - val_loss: 0.6219 - val_accuracy: 1.0000\n",
            "Epoch 1794/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5999 - accuracy: 1.0000 - val_loss: 0.6092 - val_accuracy: 1.0000\n",
            "Epoch 1795/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6393 - accuracy: 1.0000 - val_loss: 0.7377 - val_accuracy: 1.0000\n",
            "Epoch 1796/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6637 - accuracy: 1.0000 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
            "Epoch 1797/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6006 - accuracy: 1.0000 - val_loss: 0.6236 - val_accuracy: 1.0000\n",
            "Epoch 1798/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6371 - accuracy: 1.0000 - val_loss: 0.6102 - val_accuracy: 1.0000\n",
            "Epoch 1799/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6539 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 1.0000\n",
            "Epoch 1800/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6140 - accuracy: 1.0000 - val_loss: 0.6010 - val_accuracy: 1.0000\n",
            "Epoch 1801/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6674 - accuracy: 1.0000 - val_loss: 0.6033 - val_accuracy: 1.0000\n",
            "Epoch 1802/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6127 - accuracy: 1.0000 - val_loss: 1.6066 - val_accuracy: 1.0000\n",
            "Epoch 1803/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.5982 - val_accuracy: 1.0000\n",
            "Epoch 1804/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6245 - accuracy: 1.0000 - val_loss: 0.8890 - val_accuracy: 1.0000\n",
            "Epoch 1805/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6461 - accuracy: 1.0000 - val_loss: 0.5966 - val_accuracy: 1.0000\n",
            "Epoch 1806/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6348 - accuracy: 1.0000 - val_loss: 0.6658 - val_accuracy: 1.0000\n",
            "Epoch 1807/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6210 - accuracy: 1.0000 - val_loss: 0.5988 - val_accuracy: 1.0000\n",
            "Epoch 1808/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6650 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 1.0000\n",
            "Epoch 1809/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6185 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 1.0000\n",
            "Epoch 1810/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6691 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 1.0000\n",
            "Epoch 1811/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6016 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 1.0000\n",
            "Epoch 1812/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6392 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 1.0000\n",
            "Epoch 1813/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6343 - accuracy: 1.0000 - val_loss: 0.7135 - val_accuracy: 1.0000\n",
            "Epoch 1814/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6264 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 1.0000\n",
            "Epoch 1815/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6393 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 1816/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6420 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 1817/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6270 - accuracy: 1.0000 - val_loss: 0.6021 - val_accuracy: 1.0000\n",
            "Epoch 1818/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6359 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 1.0000\n",
            "Epoch 1819/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 1820/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6297 - accuracy: 1.0000 - val_loss: 0.6066 - val_accuracy: 1.0000\n",
            "Epoch 1821/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6348 - accuracy: 1.0000 - val_loss: 0.6247 - val_accuracy: 1.0000\n",
            "Epoch 1822/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6570 - accuracy: 1.0000 - val_loss: 0.5940 - val_accuracy: 1.0000\n",
            "Epoch 1823/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6346 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 1.0000\n",
            "Epoch 1824/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 1.0000\n",
            "Epoch 1825/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6065 - accuracy: 1.0000 - val_loss: 0.6348 - val_accuracy: 1.0000\n",
            "Epoch 1826/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6420 - accuracy: 1.0000 - val_loss: 0.6953 - val_accuracy: 1.0000\n",
            "Epoch 1827/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6414 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 1.0000\n",
            "Epoch 1828/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6437 - accuracy: 1.0000 - val_loss: 0.5965 - val_accuracy: 1.0000\n",
            "Epoch 1829/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.5938 - val_accuracy: 1.0000\n",
            "Epoch 1830/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6018 - val_accuracy: 1.0000\n",
            "Epoch 1831/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6215 - accuracy: 1.0000 - val_loss: 0.7098 - val_accuracy: 1.0000\n",
            "Epoch 1832/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6251 - accuracy: 1.0000 - val_loss: 0.6015 - val_accuracy: 1.0000\n",
            "Epoch 1833/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6643 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 1.0000\n",
            "Epoch 1834/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6341 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 1835/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6334 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 1.0000\n",
            "Epoch 1836/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6225 - accuracy: 1.0000 - val_loss: 0.6084 - val_accuracy: 1.0000\n",
            "Epoch 1837/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6639 - accuracy: 1.0000 - val_loss: 0.7601 - val_accuracy: 1.0000\n",
            "Epoch 1838/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6100 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 1.0000\n",
            "Epoch 1839/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6621 - accuracy: 1.0000 - val_loss: 0.6010 - val_accuracy: 1.0000\n",
            "Epoch 1840/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6053 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 1841/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6415 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 1.0000\n",
            "Epoch 1842/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6427 - accuracy: 1.0000 - val_loss: 0.6765 - val_accuracy: 1.0000\n",
            "Epoch 1843/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6254 - accuracy: 1.0000 - val_loss: 0.6184 - val_accuracy: 1.0000\n",
            "Epoch 1844/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6602 - accuracy: 1.0000 - val_loss: 0.6485 - val_accuracy: 1.0000\n",
            "Epoch 1845/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6205 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 1846/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6286 - accuracy: 1.0000 - val_loss: 0.6220 - val_accuracy: 1.0000\n",
            "Epoch 1847/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6366 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 1.0000\n",
            "Epoch 1848/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6356 - accuracy: 1.0000 - val_loss: 0.5962 - val_accuracy: 1.0000\n",
            "Epoch 1849/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 1.0000\n",
            "Epoch 1850/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6208 - accuracy: 1.0000 - val_loss: 0.6363 - val_accuracy: 1.0000\n",
            "Epoch 1851/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6533 - accuracy: 1.0000 - val_loss: 0.6183 - val_accuracy: 1.0000\n",
            "Epoch 1852/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.6217 - val_accuracy: 1.0000\n",
            "Epoch 1853/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6243 - accuracy: 1.0000 - val_loss: 0.6171 - val_accuracy: 1.0000\n",
            "Epoch 1854/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6318 - accuracy: 1.0000 - val_loss: 0.6488 - val_accuracy: 1.0000\n",
            "Epoch 1855/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6381 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 1.0000\n",
            "Epoch 1856/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6310 - accuracy: 1.0000 - val_loss: 0.5955 - val_accuracy: 1.0000\n",
            "Epoch 1857/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 1.6757 - val_accuracy: 1.0000\n",
            "Epoch 1858/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6818 - accuracy: 1.0000 - val_loss: 0.5987 - val_accuracy: 1.0000\n",
            "Epoch 1859/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6349 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
            "Epoch 1860/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6278 - accuracy: 1.0000 - val_loss: 0.6415 - val_accuracy: 1.0000\n",
            "Epoch 1861/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6302 - accuracy: 1.0000 - val_loss: 0.6147 - val_accuracy: 1.0000\n",
            "Epoch 1862/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6340 - accuracy: 1.0000 - val_loss: 0.5984 - val_accuracy: 1.0000\n",
            "Epoch 1863/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6256 - accuracy: 1.0000 - val_loss: 0.6320 - val_accuracy: 1.0000\n",
            "Epoch 1864/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.6008 - val_accuracy: 1.0000\n",
            "Epoch 1865/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6636 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 1.0000\n",
            "Epoch 1866/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6120 - accuracy: 1.0000 - val_loss: 0.6062 - val_accuracy: 1.0000\n",
            "Epoch 1867/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6406 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 1868/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6082 - val_accuracy: 1.0000\n",
            "Epoch 1869/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6171 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 1.0000\n",
            "Epoch 1870/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6356 - accuracy: 1.0000 - val_loss: 0.5971 - val_accuracy: 1.0000\n",
            "Epoch 1871/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 1.0000\n",
            "Epoch 1872/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6594 - accuracy: 1.0000 - val_loss: 0.6378 - val_accuracy: 1.0000\n",
            "Epoch 1873/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6105 - accuracy: 1.0000 - val_loss: 0.7573 - val_accuracy: 1.0000\n",
            "Epoch 1874/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6350 - accuracy: 1.0000 - val_loss: 0.6892 - val_accuracy: 1.0000\n",
            "Epoch 1875/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.6267 - val_accuracy: 1.0000\n",
            "Epoch 1876/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6301 - accuracy: 1.0000 - val_loss: 0.5981 - val_accuracy: 1.0000\n",
            "Epoch 1877/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6453 - accuracy: 1.0000 - val_loss: 0.5939 - val_accuracy: 1.0000\n",
            "Epoch 1878/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6425 - accuracy: 1.0000 - val_loss: 0.8050 - val_accuracy: 1.0000\n",
            "Epoch 1879/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6149 - accuracy: 1.0000 - val_loss: 0.6348 - val_accuracy: 1.0000\n",
            "Epoch 1880/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6456 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 1.0000\n",
            "Epoch 1881/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6366 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 1.0000\n",
            "Epoch 1882/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 1.0000\n",
            "Epoch 1883/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6616 - accuracy: 1.0000 - val_loss: 0.6009 - val_accuracy: 1.0000\n",
            "Epoch 1884/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6029 - accuracy: 1.0000 - val_loss: 0.6174 - val_accuracy: 1.0000\n",
            "Epoch 1885/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6693 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 1.0000\n",
            "Epoch 1886/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6073 - accuracy: 1.0000 - val_loss: 0.6415 - val_accuracy: 1.0000\n",
            "Epoch 1887/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6506 - accuracy: 1.0000 - val_loss: 0.6080 - val_accuracy: 1.0000\n",
            "Epoch 1888/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6221 - accuracy: 1.0000 - val_loss: 0.6019 - val_accuracy: 1.0000\n",
            "Epoch 1889/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6362 - accuracy: 1.0000 - val_loss: 0.6021 - val_accuracy: 1.0000\n",
            "Epoch 1890/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6517 - accuracy: 1.0000 - val_loss: 0.6042 - val_accuracy: 1.0000\n",
            "Epoch 1891/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6376 - accuracy: 1.0000 - val_loss: 0.5956 - val_accuracy: 1.0000\n",
            "Epoch 1892/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6224 - accuracy: 1.0000 - val_loss: 0.5978 - val_accuracy: 1.0000\n",
            "Epoch 1893/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6280 - accuracy: 1.0000 - val_loss: 0.6183 - val_accuracy: 1.0000\n",
            "Epoch 1894/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6519 - accuracy: 1.0000 - val_loss: 0.5978 - val_accuracy: 1.0000\n",
            "Epoch 1895/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6303 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 1.0000\n",
            "Epoch 1896/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6385 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 1.0000\n",
            "Epoch 1897/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6886 - accuracy: 1.0000 - val_loss: 0.7015 - val_accuracy: 1.0000\n",
            "Epoch 1898/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6031 - accuracy: 1.0000 - val_loss: 0.6197 - val_accuracy: 1.0000\n",
            "Epoch 1899/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6104 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 1.0000\n",
            "Epoch 1900/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6455 - accuracy: 1.0000 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
            "Epoch 1901/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6248 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 1.0000\n",
            "Epoch 1902/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6666 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 1.0000\n",
            "Epoch 1903/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6332 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 1904/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6224 - accuracy: 1.0000 - val_loss: 0.6207 - val_accuracy: 1.0000\n",
            "Epoch 1905/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6293 - accuracy: 1.0000 - val_loss: 0.5979 - val_accuracy: 1.0000\n",
            "Epoch 1906/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6325 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 1.0000\n",
            "Epoch 1907/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6389 - accuracy: 1.0000 - val_loss: 0.6028 - val_accuracy: 1.0000\n",
            "Epoch 1908/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6401 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 1.0000\n",
            "Epoch 1909/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6298 - accuracy: 1.0000 - val_loss: 0.5979 - val_accuracy: 1.0000\n",
            "Epoch 1910/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6335 - accuracy: 1.0000 - val_loss: 0.7388 - val_accuracy: 1.0000\n",
            "Epoch 1911/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 1.0000\n",
            "Epoch 1912/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6520 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 1913/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6183 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 1914/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6377 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
            "Epoch 1915/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6300 - accuracy: 1.0000 - val_loss: 0.6026 - val_accuracy: 1.0000\n",
            "Epoch 1916/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6611 - accuracy: 1.0000 - val_loss: 0.8737 - val_accuracy: 1.0000\n",
            "Epoch 1917/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6189 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 1.0000\n",
            "Epoch 1918/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6270 - accuracy: 1.0000 - val_loss: 0.6653 - val_accuracy: 1.0000\n",
            "Epoch 1919/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6420 - accuracy: 1.0000 - val_loss: 0.6786 - val_accuracy: 1.0000\n",
            "Epoch 1920/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.6527 - val_accuracy: 1.0000\n",
            "Epoch 1921/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6486 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 1.0000\n",
            "Epoch 1922/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6274 - accuracy: 1.0000 - val_loss: 0.5983 - val_accuracy: 1.0000\n",
            "Epoch 1923/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.5960 - val_accuracy: 1.0000\n",
            "Epoch 1924/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 1.0000\n",
            "Epoch 1925/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6422 - accuracy: 1.0000 - val_loss: 0.7639 - val_accuracy: 1.0000\n",
            "Epoch 1926/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6203 - accuracy: 1.0000 - val_loss: 0.6778 - val_accuracy: 1.0000\n",
            "Epoch 1927/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6431 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 1.0000\n",
            "Epoch 1928/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6319 - accuracy: 1.0000 - val_loss: 1.1336 - val_accuracy: 1.0000\n",
            "Epoch 1929/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6526 - accuracy: 1.0000 - val_loss: 0.5920 - val_accuracy: 1.0000\n",
            "Epoch 1930/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 1.0000\n",
            "Epoch 1931/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6278 - accuracy: 1.0000 - val_loss: 0.5960 - val_accuracy: 1.0000\n",
            "Epoch 1932/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6474 - accuracy: 1.0000 - val_loss: 0.6375 - val_accuracy: 1.0000\n",
            "Epoch 1933/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6309 - accuracy: 1.0000 - val_loss: 0.6017 - val_accuracy: 1.0000\n",
            "Epoch 1934/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6358 - accuracy: 1.0000 - val_loss: 0.6084 - val_accuracy: 1.0000\n",
            "Epoch 1935/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6268 - accuracy: 1.0000 - val_loss: 0.6029 - val_accuracy: 1.0000\n",
            "Epoch 1936/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6368 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 1.0000\n",
            "Epoch 1937/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6187 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 1.0000\n",
            "Epoch 1938/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6451 - accuracy: 1.0000 - val_loss: 0.6295 - val_accuracy: 1.0000\n",
            "Epoch 1939/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.6016 - val_accuracy: 1.0000\n",
            "Epoch 1940/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6545 - accuracy: 1.0000 - val_loss: 0.5960 - val_accuracy: 1.0000\n",
            "Epoch 1941/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6055 - accuracy: 1.0000 - val_loss: 0.6264 - val_accuracy: 1.0000\n",
            "Epoch 1942/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.6035 - val_accuracy: 1.0000\n",
            "Epoch 1943/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6772 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 1.0000\n",
            "Epoch 1944/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6012 - accuracy: 1.0000 - val_loss: 0.6176 - val_accuracy: 1.0000\n",
            "Epoch 1945/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6401 - accuracy: 1.0000 - val_loss: 1.3538 - val_accuracy: 1.0000\n",
            "Epoch 1946/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6269 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 1.0000\n",
            "Epoch 1947/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6302 - accuracy: 1.0000 - val_loss: 0.6592 - val_accuracy: 1.0000\n",
            "Epoch 1948/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 1.0000\n",
            "Epoch 1949/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 1.0000\n",
            "Epoch 1950/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6418 - accuracy: 1.0000 - val_loss: 0.6086 - val_accuracy: 1.0000\n",
            "Epoch 1951/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6309 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 1.0000\n",
            "Epoch 1952/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6314 - accuracy: 1.0000 - val_loss: 0.6494 - val_accuracy: 1.0000\n",
            "Epoch 1953/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6376 - accuracy: 1.0000 - val_loss: 0.6205 - val_accuracy: 1.0000\n",
            "Epoch 1954/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6294 - accuracy: 1.0000 - val_loss: 0.6800 - val_accuracy: 1.0000\n",
            "Epoch 1955/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6384 - accuracy: 1.0000 - val_loss: 0.6207 - val_accuracy: 1.0000\n",
            "Epoch 1956/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6483 - accuracy: 1.0000 - val_loss: 0.6052 - val_accuracy: 1.0000\n",
            "Epoch 1957/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6300 - accuracy: 1.0000 - val_loss: 0.7113 - val_accuracy: 1.0000\n",
            "Epoch 1958/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6245 - accuracy: 1.0000 - val_loss: 0.5991 - val_accuracy: 1.0000\n",
            "Epoch 1959/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 1960/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6375 - accuracy: 1.0000 - val_loss: 0.6326 - val_accuracy: 1.0000\n",
            "Epoch 1961/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6629 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 1.0000\n",
            "Epoch 1962/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6106 - accuracy: 1.0000 - val_loss: 0.7147 - val_accuracy: 1.0000\n",
            "Epoch 1963/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 1.0000\n",
            "Epoch 1964/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6235 - accuracy: 1.0000 - val_loss: 0.5962 - val_accuracy: 1.0000\n",
            "Epoch 1965/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6627 - accuracy: 1.0000 - val_loss: 0.6035 - val_accuracy: 1.0000\n",
            "Epoch 1966/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6072 - accuracy: 1.0000 - val_loss: 0.6559 - val_accuracy: 1.0000\n",
            "Epoch 1967/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6503 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 1.0000\n",
            "Epoch 1968/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.9054 - val_accuracy: 1.0000\n",
            "Epoch 1969/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6291 - accuracy: 1.0000 - val_loss: 0.6611 - val_accuracy: 1.0000\n",
            "Epoch 1970/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6454 - accuracy: 1.0000 - val_loss: 0.9419 - val_accuracy: 1.0000\n",
            "Epoch 1971/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.5962 - val_accuracy: 1.0000\n",
            "Epoch 1972/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6226 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 1.0000\n",
            "Epoch 1973/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6533 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 1.0000\n",
            "Epoch 1974/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6112 - accuracy: 1.0000 - val_loss: 0.6394 - val_accuracy: 1.0000\n",
            "Epoch 1975/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6470 - accuracy: 1.0000 - val_loss: 0.5958 - val_accuracy: 1.0000\n",
            "Epoch 1976/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6310 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 1.0000\n",
            "Epoch 1977/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.5974 - val_accuracy: 1.0000\n",
            "Epoch 1978/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6234 - accuracy: 1.0000 - val_loss: 1.0872 - val_accuracy: 1.0000\n",
            "Epoch 1979/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6557 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 1980/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6309 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 1.0000\n",
            "Epoch 1981/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6219 - val_accuracy: 1.0000\n",
            "Epoch 1982/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6245 - accuracy: 1.0000 - val_loss: 0.5940 - val_accuracy: 1.0000\n",
            "Epoch 1983/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6313 - accuracy: 1.0000 - val_loss: 0.7175 - val_accuracy: 1.0000\n",
            "Epoch 1984/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6415 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 1.0000\n",
            "Epoch 1985/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6257 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 1.0000\n",
            "Epoch 1986/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6311 - accuracy: 1.0000 - val_loss: 0.6458 - val_accuracy: 1.0000\n",
            "Epoch 1987/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.6743 - val_accuracy: 1.0000\n",
            "Epoch 1988/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6411 - accuracy: 1.0000 - val_loss: 0.6156 - val_accuracy: 1.0000\n",
            "Epoch 1989/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6310 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 1990/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.5979 - val_accuracy: 1.0000\n",
            "Epoch 1991/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6640 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 1.0000\n",
            "Epoch 1992/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6017 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 1.0000\n",
            "Epoch 1993/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6300 - accuracy: 1.0000 - val_loss: 0.6247 - val_accuracy: 1.0000\n",
            "Epoch 1994/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6349 - accuracy: 1.0000 - val_loss: 0.6249 - val_accuracy: 1.0000\n",
            "Epoch 1995/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6552 - accuracy: 1.0000 - val_loss: 0.6844 - val_accuracy: 1.0000\n",
            "Epoch 1996/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6126 - accuracy: 1.0000 - val_loss: 0.7397 - val_accuracy: 1.0000\n",
            "Epoch 1997/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6391 - accuracy: 1.0000 - val_loss: 0.7139 - val_accuracy: 1.0000\n",
            "Epoch 1998/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6415 - accuracy: 1.0000 - val_loss: 0.6429 - val_accuracy: 1.0000\n",
            "Epoch 1999/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6231 - accuracy: 1.0000 - val_loss: 0.6391 - val_accuracy: 1.0000\n",
            "Epoch 2000/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6330 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
            "Epoch 2001/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6440 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 1.0000\n",
            "Epoch 2002/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6220 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 1.0000\n",
            "Epoch 2003/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6412 - accuracy: 1.0000 - val_loss: 0.6214 - val_accuracy: 1.0000\n",
            "Epoch 2004/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6463 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
            "Epoch 2005/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6197 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 2006/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6394 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 1.0000\n",
            "Epoch 2007/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6448 - accuracy: 1.0000 - val_loss: 0.6034 - val_accuracy: 1.0000\n",
            "Epoch 2008/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6131 - accuracy: 1.0000 - val_loss: 0.8130 - val_accuracy: 1.0000\n",
            "Epoch 2009/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6278 - val_accuracy: 1.0000\n",
            "Epoch 2010/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6787 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 1.0000\n",
            "Epoch 2011/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5982 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 1.0000\n",
            "Epoch 2012/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6405 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 2013/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6288 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
            "Epoch 2014/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6275 - accuracy: 1.0000 - val_loss: 0.9760 - val_accuracy: 1.0000\n",
            "Epoch 2015/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6243 - accuracy: 1.0000 - val_loss: 0.6315 - val_accuracy: 1.0000\n",
            "Epoch 2016/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.5956 - val_accuracy: 1.0000\n",
            "Epoch 2017/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6274 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 1.0000\n",
            "Epoch 2018/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6561 - accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 1.0000\n",
            "Epoch 2019/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6179 - accuracy: 1.0000 - val_loss: 0.6084 - val_accuracy: 1.0000\n",
            "Epoch 2020/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6663 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 1.0000\n",
            "Epoch 2021/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6304 - accuracy: 1.0000 - val_loss: 0.5942 - val_accuracy: 1.0000\n",
            "Epoch 2022/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6310 - accuracy: 1.0000 - val_loss: 0.6194 - val_accuracy: 1.0000\n",
            "Epoch 2023/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 2024/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6200 - accuracy: 1.0000 - val_loss: 0.6375 - val_accuracy: 1.0000\n",
            "Epoch 2025/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6548 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
            "Epoch 2026/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6274 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 1.0000\n",
            "Epoch 2027/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6433 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 1.0000\n",
            "Epoch 2028/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6207 - accuracy: 1.0000 - val_loss: 0.6242 - val_accuracy: 1.0000\n",
            "Epoch 2029/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6391 - accuracy: 1.0000 - val_loss: 0.6034 - val_accuracy: 1.0000\n",
            "Epoch 2030/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6226 - accuracy: 1.0000 - val_loss: 0.6672 - val_accuracy: 1.0000\n",
            "Epoch 2031/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6362 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 1.0000\n",
            "Epoch 2032/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6300 - accuracy: 1.0000 - val_loss: 0.6106 - val_accuracy: 1.0000\n",
            "Epoch 2033/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6565 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 1.0000\n",
            "Epoch 2034/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6196 - accuracy: 1.0000 - val_loss: 0.6994 - val_accuracy: 1.0000\n",
            "Epoch 2035/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 1.0000\n",
            "Epoch 2036/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6365 - accuracy: 1.0000 - val_loss: 0.8142 - val_accuracy: 1.0000\n",
            "Epoch 2037/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6162 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 1.0000\n",
            "Epoch 2038/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.5960 - val_accuracy: 1.0000\n",
            "Epoch 2039/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6414 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
            "Epoch 2040/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6277 - accuracy: 1.0000 - val_loss: 0.5960 - val_accuracy: 1.0000\n",
            "Epoch 2041/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6325 - accuracy: 1.0000 - val_loss: 0.6174 - val_accuracy: 1.0000\n",
            "Epoch 2042/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6266 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 1.0000\n",
            "Epoch 2043/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6383 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 1.0000\n",
            "Epoch 2044/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6319 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 1.0000\n",
            "Epoch 2045/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6465 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 1.0000\n",
            "Epoch 2046/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6176 - accuracy: 1.0000 - val_loss: 0.5954 - val_accuracy: 1.0000\n",
            "Epoch 2047/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6365 - accuracy: 1.0000 - val_loss: 0.6468 - val_accuracy: 1.0000\n",
            "Epoch 2048/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.6049 - val_accuracy: 1.0000\n",
            "Epoch 2049/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 0.6615 - val_accuracy: 1.0000\n",
            "Epoch 2050/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6428 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 1.0000\n",
            "Epoch 2051/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6309 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 1.0000\n",
            "Epoch 2052/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6481 - accuracy: 1.0000 - val_loss: 0.8470 - val_accuracy: 1.0000\n",
            "Epoch 2053/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6092 - accuracy: 1.0000 - val_loss: 0.6091 - val_accuracy: 1.0000\n",
            "Epoch 2054/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.6096 - val_accuracy: 1.0000\n",
            "Epoch 2055/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6387 - accuracy: 1.0000 - val_loss: 0.6039 - val_accuracy: 1.0000\n",
            "Epoch 2056/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6211 - accuracy: 1.0000 - val_loss: 0.6236 - val_accuracy: 1.0000\n",
            "Epoch 2057/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6388 - accuracy: 1.0000 - val_loss: 0.6238 - val_accuracy: 1.0000\n",
            "Epoch 2058/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6625 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 1.0000\n",
            "Epoch 2059/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6087 - accuracy: 1.0000 - val_loss: 0.7186 - val_accuracy: 1.0000\n",
            "Epoch 2060/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 1.0000\n",
            "Epoch 2061/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6364 - accuracy: 1.0000 - val_loss: 0.6225 - val_accuracy: 1.0000\n",
            "Epoch 2062/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6682 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 1.0000\n",
            "Epoch 2063/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6083 - accuracy: 1.0000 - val_loss: 0.6447 - val_accuracy: 1.0000\n",
            "Epoch 2064/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6428 - accuracy: 1.0000 - val_loss: 0.5946 - val_accuracy: 1.0000\n",
            "Epoch 2065/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6218 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 2066/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6375 - accuracy: 1.0000 - val_loss: 0.6028 - val_accuracy: 1.0000\n",
            "Epoch 2067/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6256 - accuracy: 1.0000 - val_loss: 0.6679 - val_accuracy: 1.0000\n",
            "Epoch 2068/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6500 - accuracy: 1.0000 - val_loss: 0.6278 - val_accuracy: 1.0000\n",
            "Epoch 2069/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6482 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 1.0000\n",
            "Epoch 2070/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6077 - accuracy: 1.0000 - val_loss: 0.6928 - val_accuracy: 1.0000\n",
            "Epoch 2071/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6477 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 2072/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6200 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 1.0000\n",
            "Epoch 2073/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6650 - accuracy: 1.0000 - val_loss: 0.6120 - val_accuracy: 1.0000\n",
            "Epoch 2074/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6017 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 1.0000\n",
            "Epoch 2075/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6444 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 1.0000\n",
            "Epoch 2076/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6769 - accuracy: 1.0000 - val_loss: 0.7207 - val_accuracy: 1.0000\n",
            "Epoch 2077/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6263 - accuracy: 1.0000 - val_loss: 0.5928 - val_accuracy: 1.0000\n",
            "Epoch 2078/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6345 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 1.0000\n",
            "Epoch 2079/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6178 - accuracy: 1.0000 - val_loss: 0.5979 - val_accuracy: 1.0000\n",
            "Epoch 2080/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 1.0000\n",
            "Epoch 2081/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6228 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
            "Epoch 2082/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6529 - accuracy: 1.0000 - val_loss: 0.6148 - val_accuracy: 1.0000\n",
            "Epoch 2083/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6185 - accuracy: 1.0000 - val_loss: 0.6976 - val_accuracy: 1.0000\n",
            "Epoch 2084/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6218 - accuracy: 1.0000 - val_loss: 0.6103 - val_accuracy: 1.0000\n",
            "Epoch 2085/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6665 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 1.0000\n",
            "Epoch 2086/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6063 - accuracy: 1.0000 - val_loss: 0.9546 - val_accuracy: 1.0000\n",
            "Epoch 2087/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6544 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
            "Epoch 2088/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6203 - accuracy: 1.0000 - val_loss: 0.6987 - val_accuracy: 1.0000\n",
            "Epoch 2089/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6695 - accuracy: 1.0000 - val_loss: 0.7581 - val_accuracy: 1.0000\n",
            "Epoch 2090/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6350 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 1.0000\n",
            "Epoch 2091/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 1.0000\n",
            "Epoch 2092/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6253 - accuracy: 1.0000 - val_loss: 0.6008 - val_accuracy: 1.0000\n",
            "Epoch 2093/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6327 - accuracy: 1.0000 - val_loss: 0.6109 - val_accuracy: 1.0000\n",
            "Epoch 2094/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6272 - accuracy: 1.0000 - val_loss: 0.6368 - val_accuracy: 1.0000\n",
            "Epoch 2095/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6305 - accuracy: 1.0000 - val_loss: 0.5963 - val_accuracy: 1.0000\n",
            "Epoch 2096/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6463 - accuracy: 1.0000 - val_loss: 0.6283 - val_accuracy: 1.0000\n",
            "Epoch 2097/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6481 - accuracy: 1.0000 - val_loss: 0.8011 - val_accuracy: 1.0000\n",
            "Epoch 2098/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6065 - accuracy: 1.0000 - val_loss: 0.5963 - val_accuracy: 1.0000\n",
            "Epoch 2099/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6262 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 1.0000\n",
            "Epoch 2100/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6427 - accuracy: 1.0000 - val_loss: 0.6082 - val_accuracy: 1.0000\n",
            "Epoch 2101/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6321 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 1.0000\n",
            "Epoch 2102/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6240 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 1.0000\n",
            "Epoch 2103/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6534 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
            "Epoch 2104/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6514 - accuracy: 1.0000 - val_loss: 0.6308 - val_accuracy: 1.0000\n",
            "Epoch 2105/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6018 - accuracy: 1.0000 - val_loss: 0.6120 - val_accuracy: 1.0000\n",
            "Epoch 2106/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6287 - accuracy: 1.0000 - val_loss: 0.7394 - val_accuracy: 1.0000\n",
            "Epoch 2107/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6434 - accuracy: 1.0000 - val_loss: 0.5961 - val_accuracy: 1.0000\n",
            "Epoch 2108/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6527 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
            "Epoch 2109/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6333 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 1.0000\n",
            "Epoch 2110/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.9938 - val_accuracy: 1.0000\n",
            "Epoch 2111/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6183 - accuracy: 1.0000 - val_loss: 0.6271 - val_accuracy: 1.0000\n",
            "Epoch 2112/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6276 - accuracy: 1.0000 - val_loss: 1.2828 - val_accuracy: 1.0000\n",
            "Epoch 2113/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6479 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 1.0000\n",
            "Epoch 2114/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6244 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 1.0000\n",
            "Epoch 2115/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6328 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 1.0000\n",
            "Epoch 2116/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6319 - accuracy: 1.0000 - val_loss: 1.1255 - val_accuracy: 1.0000\n",
            "Epoch 2117/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6365 - accuracy: 1.0000 - val_loss: 0.6037 - val_accuracy: 1.0000\n",
            "Epoch 2118/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6573 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 1.0000\n",
            "Epoch 2119/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6233 - accuracy: 1.0000 - val_loss: 0.7439 - val_accuracy: 1.0000\n",
            "Epoch 2120/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6385 - accuracy: 1.0000 - val_loss: 0.5985 - val_accuracy: 1.0000\n",
            "Epoch 2121/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6091 - accuracy: 1.0000 - val_loss: 0.6012 - val_accuracy: 1.0000\n",
            "Epoch 2122/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6327 - accuracy: 1.0000 - val_loss: 0.6213 - val_accuracy: 1.0000\n",
            "Epoch 2123/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6416 - accuracy: 1.0000 - val_loss: 0.6826 - val_accuracy: 1.0000\n",
            "Epoch 2124/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6460 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 2125/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6207 - accuracy: 1.0000 - val_loss: 0.6242 - val_accuracy: 1.0000\n",
            "Epoch 2126/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6320 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 1.0000\n",
            "Epoch 2127/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6484 - accuracy: 1.0000 - val_loss: 0.6042 - val_accuracy: 1.0000\n",
            "Epoch 2128/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6534 - accuracy: 1.0000 - val_loss: 0.7835 - val_accuracy: 1.0000\n",
            "Epoch 2129/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6058 - accuracy: 1.0000 - val_loss: 0.6138 - val_accuracy: 1.0000\n",
            "Epoch 2130/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 1.0000\n",
            "Epoch 2131/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6795 - accuracy: 1.0000 - val_loss: 0.5988 - val_accuracy: 1.0000\n",
            "Epoch 2132/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6016 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 2133/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 1.0000\n",
            "Epoch 2134/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6330 - accuracy: 1.0000 - val_loss: 0.5951 - val_accuracy: 1.0000\n",
            "Epoch 2135/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6344 - accuracy: 1.0000 - val_loss: 0.8482 - val_accuracy: 1.0000\n",
            "Epoch 2136/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6247 - accuracy: 1.0000 - val_loss: 0.6129 - val_accuracy: 1.0000\n",
            "Epoch 2137/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6649 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
            "Epoch 2138/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6045 - accuracy: 1.0000 - val_loss: 0.6306 - val_accuracy: 1.0000\n",
            "Epoch 2139/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6663 - accuracy: 1.0000 - val_loss: 0.6601 - val_accuracy: 1.0000\n",
            "Epoch 2140/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6024 - accuracy: 1.0000 - val_loss: 0.6151 - val_accuracy: 1.0000\n",
            "Epoch 2141/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6678 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 1.0000\n",
            "Epoch 2142/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6028 - accuracy: 1.0000 - val_loss: 0.6914 - val_accuracy: 1.0000\n",
            "Epoch 2143/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6411 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 1.0000\n",
            "Epoch 2144/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6290 - accuracy: 1.0000 - val_loss: 0.6015 - val_accuracy: 1.0000\n",
            "Epoch 2145/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6385 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 1.0000\n",
            "Epoch 2146/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6404 - accuracy: 1.0000 - val_loss: 0.6130 - val_accuracy: 1.0000\n",
            "Epoch 2147/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6720 - accuracy: 1.0000 - val_loss: 0.6758 - val_accuracy: 1.0000\n",
            "Epoch 2148/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5981 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 1.0000\n",
            "Epoch 2149/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6308 - accuracy: 1.0000 - val_loss: 0.5960 - val_accuracy: 1.0000\n",
            "Epoch 2150/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6334 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 1.0000\n",
            "Epoch 2151/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6336 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 1.0000\n",
            "Epoch 2152/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6326 - accuracy: 1.0000 - val_loss: 0.5950 - val_accuracy: 1.0000\n",
            "Epoch 2153/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 1.0000\n",
            "Epoch 2154/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6282 - accuracy: 1.0000 - val_loss: 0.6175 - val_accuracy: 1.0000\n",
            "Epoch 2155/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6294 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 1.0000\n",
            "Epoch 2156/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6536 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 1.0000\n",
            "Epoch 2157/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6139 - accuracy: 1.0000 - val_loss: 0.6103 - val_accuracy: 1.0000\n",
            "Epoch 2158/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6369 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 2159/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6316 - accuracy: 1.0000 - val_loss: 0.6608 - val_accuracy: 1.0000\n",
            "Epoch 2160/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6287 - accuracy: 1.0000 - val_loss: 0.6205 - val_accuracy: 1.0000\n",
            "Epoch 2161/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.7129 - val_accuracy: 1.0000\n",
            "Epoch 2162/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6543 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 1.0000\n",
            "Epoch 2163/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6084 - accuracy: 1.0000 - val_loss: 0.7096 - val_accuracy: 1.0000\n",
            "Epoch 2164/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.6239 - val_accuracy: 1.0000\n",
            "Epoch 2165/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 1.0000\n",
            "Epoch 2166/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6474 - accuracy: 1.0000 - val_loss: 0.5944 - val_accuracy: 1.0000\n",
            "Epoch 2167/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6605 - accuracy: 1.0000 - val_loss: 0.5970 - val_accuracy: 1.0000\n",
            "Epoch 2168/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5980 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 1.0000\n",
            "Epoch 2169/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6374 - accuracy: 1.0000 - val_loss: 0.7474 - val_accuracy: 1.0000\n",
            "Epoch 2170/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7575 - accuracy: 1.0000 - val_loss: 0.6230 - val_accuracy: 1.0000\n",
            "Epoch 2171/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6142 - accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 1.0000\n",
            "Epoch 2172/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6141 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 1.0000\n",
            "Epoch 2173/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6282 - accuracy: 1.0000 - val_loss: 0.5954 - val_accuracy: 1.0000\n",
            "Epoch 2174/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6296 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 1.0000\n",
            "Epoch 2175/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6256 - accuracy: 1.0000 - val_loss: 0.7330 - val_accuracy: 1.0000\n",
            "Epoch 2176/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6637 - accuracy: 1.0000 - val_loss: 0.6094 - val_accuracy: 1.0000\n",
            "Epoch 2177/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6282 - accuracy: 1.0000 - val_loss: 0.6165 - val_accuracy: 1.0000\n",
            "Epoch 2178/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6362 - accuracy: 1.0000 - val_loss: 0.6614 - val_accuracy: 1.0000\n",
            "Epoch 2179/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6488 - accuracy: 1.0000 - val_loss: 0.5930 - val_accuracy: 1.0000\n",
            "Epoch 2180/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6170 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 1.0000\n",
            "Epoch 2181/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6303 - accuracy: 1.0000 - val_loss: 0.6207 - val_accuracy: 1.0000\n",
            "Epoch 2182/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6278 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 1.0000\n",
            "Epoch 2183/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6317 - accuracy: 1.0000 - val_loss: 0.6358 - val_accuracy: 1.0000\n",
            "Epoch 2184/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6370 - accuracy: 1.0000 - val_loss: 0.6007 - val_accuracy: 1.0000\n",
            "Epoch 2185/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6222 - accuracy: 1.0000 - val_loss: 0.6359 - val_accuracy: 1.0000\n",
            "Epoch 2186/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6477 - accuracy: 1.0000 - val_loss: 1.0185 - val_accuracy: 1.0000\n",
            "Epoch 2187/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6176 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 1.0000\n",
            "Epoch 2188/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6314 - accuracy: 1.0000 - val_loss: 0.6889 - val_accuracy: 1.0000\n",
            "Epoch 2189/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6383 - accuracy: 1.0000 - val_loss: 0.6074 - val_accuracy: 1.0000\n",
            "Epoch 2190/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 1.0000\n",
            "Epoch 2191/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6385 - accuracy: 1.0000 - val_loss: 0.6490 - val_accuracy: 1.0000\n",
            "Epoch 2192/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 1.0000\n",
            "Epoch 2193/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6347 - accuracy: 1.0000 - val_loss: 0.5932 - val_accuracy: 1.0000\n",
            "Epoch 2194/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6257 - accuracy: 1.0000 - val_loss: 1.7571 - val_accuracy: 1.0000\n",
            "Epoch 2195/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6628 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 1.0000\n",
            "Epoch 2196/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6063 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 1.0000\n",
            "Epoch 2197/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6397 - accuracy: 1.0000 - val_loss: 0.5929 - val_accuracy: 1.0000\n",
            "Epoch 2198/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6351 - accuracy: 1.0000 - val_loss: 0.6153 - val_accuracy: 1.0000\n",
            "Epoch 2199/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6228 - accuracy: 1.0000 - val_loss: 0.6525 - val_accuracy: 1.0000\n",
            "Epoch 2200/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6380 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 1.0000\n",
            "Epoch 2201/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6427 - accuracy: 1.0000 - val_loss: 0.6095 - val_accuracy: 1.0000\n",
            "Epoch 2202/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6223 - accuracy: 1.0000 - val_loss: 0.6943 - val_accuracy: 1.0000\n",
            "Epoch 2203/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6467 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 1.0000\n",
            "Epoch 2204/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6218 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 1.0000\n",
            "Epoch 2205/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6253 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 1.0000\n",
            "Epoch 2206/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6390 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 1.0000\n",
            "Epoch 2207/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7191 - accuracy: 1.0000 - val_loss: 0.5931 - val_accuracy: 1.0000\n",
            "Epoch 2208/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5933 - accuracy: 1.0000 - val_loss: 0.6195 - val_accuracy: 1.0000\n",
            "Epoch 2209/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6201 - accuracy: 1.0000 - val_loss: 0.6627 - val_accuracy: 1.0000\n",
            "Epoch 2210/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6284 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 1.0000\n",
            "Epoch 2211/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6536 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 1.0000\n",
            "Epoch 2212/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6262 - accuracy: 1.0000 - val_loss: 0.5976 - val_accuracy: 1.0000\n",
            "Epoch 2213/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6255 - accuracy: 1.0000 - val_loss: 0.6035 - val_accuracy: 1.0000\n",
            "Epoch 2214/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6332 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
            "Epoch 2215/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6306 - accuracy: 1.0000 - val_loss: 0.6239 - val_accuracy: 1.0000\n",
            "Epoch 2216/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6347 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 2217/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6334 - accuracy: 1.0000 - val_loss: 0.6626 - val_accuracy: 1.0000\n",
            "Epoch 2218/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6296 - accuracy: 1.0000 - val_loss: 0.5950 - val_accuracy: 1.0000\n",
            "Epoch 2219/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6349 - accuracy: 1.0000 - val_loss: 0.6033 - val_accuracy: 1.0000\n",
            "Epoch 2220/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6295 - accuracy: 1.0000 - val_loss: 0.5963 - val_accuracy: 1.0000\n",
            "Epoch 2221/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6278 - accuracy: 1.0000 - val_loss: 0.6146 - val_accuracy: 1.0000\n",
            "Epoch 2222/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6685 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 1.0000\n",
            "Epoch 2223/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6031 - accuracy: 1.0000 - val_loss: 0.5907 - val_accuracy: 1.0000\n",
            "Epoch 2224/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6446 - accuracy: 1.0000 - val_loss: 0.6323 - val_accuracy: 1.0000\n",
            "Epoch 2225/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6229 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 1.0000\n",
            "Epoch 2226/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6217 - accuracy: 1.0000 - val_loss: 0.7486 - val_accuracy: 1.0000\n",
            "Epoch 2227/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6555 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 2228/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6221 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
            "Epoch 2229/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6333 - accuracy: 1.0000 - val_loss: 0.6034 - val_accuracy: 1.0000\n",
            "Epoch 2230/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6365 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
            "Epoch 2231/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.7503 - val_accuracy: 1.0000\n",
            "Epoch 2232/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6489 - accuracy: 1.0000 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
            "Epoch 2233/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6046 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 1.0000\n",
            "Epoch 2234/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6522 - accuracy: 1.0000 - val_loss: 0.6011 - val_accuracy: 1.0000\n",
            "Epoch 2235/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6227 - accuracy: 1.0000 - val_loss: 0.6391 - val_accuracy: 1.0000\n",
            "Epoch 2236/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6492 - accuracy: 1.0000 - val_loss: 0.8027 - val_accuracy: 1.0000\n",
            "Epoch 2237/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6194 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 1.0000\n",
            "Epoch 2238/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6232 - accuracy: 1.0000 - val_loss: 0.7498 - val_accuracy: 1.0000\n",
            "Epoch 2239/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6593 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
            "Epoch 2240/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6248 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 1.0000\n",
            "Epoch 2241/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6256 - accuracy: 1.0000 - val_loss: 0.7309 - val_accuracy: 1.0000\n",
            "Epoch 2242/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6593 - val_accuracy: 1.0000\n",
            "Epoch 2243/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6132 - accuracy: 1.0000 - val_loss: 0.5956 - val_accuracy: 1.0000\n",
            "Epoch 2244/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6375 - accuracy: 1.0000 - val_loss: 0.6701 - val_accuracy: 1.0000\n",
            "Epoch 2245/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6403 - accuracy: 1.0000 - val_loss: 0.5983 - val_accuracy: 1.0000\n",
            "Epoch 2246/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6306 - accuracy: 1.0000 - val_loss: 0.6318 - val_accuracy: 1.0000\n",
            "Epoch 2247/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6282 - accuracy: 1.0000 - val_loss: 0.6151 - val_accuracy: 1.0000\n",
            "Epoch 2248/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6263 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 1.0000\n",
            "Epoch 2249/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6422 - accuracy: 1.0000 - val_loss: 0.5919 - val_accuracy: 1.0000\n",
            "Epoch 2250/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6246 - accuracy: 1.0000 - val_loss: 0.6083 - val_accuracy: 1.0000\n",
            "Epoch 2251/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6253 - accuracy: 1.0000 - val_loss: 0.6597 - val_accuracy: 1.0000\n",
            "Epoch 2252/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6337 - accuracy: 1.0000 - val_loss: 0.6065 - val_accuracy: 1.0000\n",
            "Epoch 2253/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6399 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 1.0000\n",
            "Epoch 2254/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6229 - accuracy: 1.0000 - val_loss: 0.6033 - val_accuracy: 1.0000\n",
            "Epoch 2255/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6307 - accuracy: 1.0000 - val_loss: 0.7033 - val_accuracy: 1.0000\n",
            "Epoch 2256/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6375 - accuracy: 1.0000 - val_loss: 0.6591 - val_accuracy: 1.0000\n",
            "Epoch 2257/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6326 - accuracy: 1.0000 - val_loss: 0.6291 - val_accuracy: 1.0000\n",
            "Epoch 2258/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6431 - accuracy: 1.0000 - val_loss: 0.7947 - val_accuracy: 1.0000\n",
            "Epoch 2259/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6167 - accuracy: 1.0000 - val_loss: 0.5984 - val_accuracy: 1.0000\n",
            "Epoch 2260/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 1.0000\n",
            "Epoch 2261/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6366 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 2262/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6261 - accuracy: 1.0000 - val_loss: 0.6105 - val_accuracy: 1.0000\n",
            "Epoch 2263/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6459 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
            "Epoch 2264/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6225 - accuracy: 1.0000 - val_loss: 0.6388 - val_accuracy: 1.0000\n",
            "Epoch 2265/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6373 - accuracy: 1.0000 - val_loss: 0.6675 - val_accuracy: 1.0000\n",
            "Epoch 2266/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6699 - accuracy: 1.0000 - val_loss: 0.5937 - val_accuracy: 1.0000\n",
            "Epoch 2267/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.6383 - val_accuracy: 1.0000\n",
            "Epoch 2268/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6158 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 1.0000\n",
            "Epoch 2269/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 1.0000\n",
            "Epoch 2270/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6290 - accuracy: 1.0000 - val_loss: 0.7117 - val_accuracy: 1.0000\n",
            "Epoch 2271/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6305 - accuracy: 1.0000 - val_loss: 0.7798 - val_accuracy: 1.0000\n",
            "Epoch 2272/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6254 - accuracy: 1.0000 - val_loss: 0.6084 - val_accuracy: 1.0000\n",
            "Epoch 2273/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6323 - accuracy: 1.0000 - val_loss: 0.5984 - val_accuracy: 1.0000\n",
            "Epoch 2274/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6328 - accuracy: 1.0000 - val_loss: 0.6432 - val_accuracy: 1.0000\n",
            "Epoch 2275/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6340 - accuracy: 1.0000 - val_loss: 0.6752 - val_accuracy: 1.0000\n",
            "Epoch 2276/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6274 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 1.0000\n",
            "Epoch 2277/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6285 - accuracy: 1.0000 - val_loss: 0.6044 - val_accuracy: 1.0000\n",
            "Epoch 2278/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6305 - accuracy: 1.0000 - val_loss: 0.5940 - val_accuracy: 1.0000\n",
            "Epoch 2279/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6425 - accuracy: 1.0000 - val_loss: 0.6176 - val_accuracy: 1.0000\n",
            "Epoch 2280/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6270 - accuracy: 1.0000 - val_loss: 0.5910 - val_accuracy: 1.0000\n",
            "Epoch 2281/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6250 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 1.0000\n",
            "Epoch 2282/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6377 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
            "Epoch 2283/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6217 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 1.0000\n",
            "Epoch 2284/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6451 - accuracy: 1.0000 - val_loss: 0.5942 - val_accuracy: 1.0000\n",
            "Epoch 2285/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6457 - accuracy: 1.0000 - val_loss: 0.6007 - val_accuracy: 1.0000\n",
            "Epoch 2286/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 1.0000\n",
            "Epoch 2287/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6062 - accuracy: 1.0000 - val_loss: 0.6207 - val_accuracy: 1.0000\n",
            "Epoch 2288/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6561 - accuracy: 1.0000 - val_loss: 0.6255 - val_accuracy: 1.0000\n",
            "Epoch 2289/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6121 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 1.0000\n",
            "Epoch 2290/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6275 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 1.0000\n",
            "Epoch 2291/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6270 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 1.0000\n",
            "Epoch 2292/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6336 - accuracy: 1.0000 - val_loss: 0.6467 - val_accuracy: 1.0000\n",
            "Epoch 2293/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6691 - accuracy: 1.0000 - val_loss: 0.6787 - val_accuracy: 1.0000\n",
            "Epoch 2294/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5990 - accuracy: 1.0000 - val_loss: 0.5973 - val_accuracy: 1.0000\n",
            "Epoch 2295/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6251 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 2296/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7036 - accuracy: 1.0000 - val_loss: 0.6616 - val_accuracy: 1.0000\n",
            "Epoch 2297/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6130 - accuracy: 1.0000 - val_loss: 0.6253 - val_accuracy: 1.0000\n",
            "Epoch 2298/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6228 - accuracy: 1.0000 - val_loss: 0.6012 - val_accuracy: 1.0000\n",
            "Epoch 2299/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6293 - accuracy: 1.0000 - val_loss: 0.5966 - val_accuracy: 1.0000\n",
            "Epoch 2300/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6280 - accuracy: 1.0000 - val_loss: 0.5928 - val_accuracy: 1.0000\n",
            "Epoch 2301/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6276 - accuracy: 1.0000 - val_loss: 0.6009 - val_accuracy: 1.0000\n",
            "Epoch 2302/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6295 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 2303/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6348 - accuracy: 1.0000 - val_loss: 0.6318 - val_accuracy: 1.0000\n",
            "Epoch 2304/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6236 - accuracy: 1.0000 - val_loss: 0.6390 - val_accuracy: 1.0000\n",
            "Epoch 2305/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6314 - accuracy: 1.0000 - val_loss: 0.6052 - val_accuracy: 1.0000\n",
            "Epoch 2306/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6327 - accuracy: 1.0000 - val_loss: 1.1878 - val_accuracy: 1.0000\n",
            "Epoch 2307/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6733 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 1.0000\n",
            "Epoch 2308/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6409 - accuracy: 1.0000 - val_loss: 0.6309 - val_accuracy: 1.0000\n",
            "Epoch 2309/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6010 - accuracy: 1.0000 - val_loss: 0.5918 - val_accuracy: 1.0000\n",
            "Epoch 2310/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6432 - accuracy: 1.0000 - val_loss: 0.6763 - val_accuracy: 1.0000\n",
            "Epoch 2311/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6227 - accuracy: 1.0000 - val_loss: 0.5930 - val_accuracy: 1.0000\n",
            "Epoch 2312/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6333 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 1.0000\n",
            "Epoch 2313/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6319 - accuracy: 1.0000 - val_loss: 0.5939 - val_accuracy: 1.0000\n",
            "Epoch 2314/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6276 - accuracy: 1.0000 - val_loss: 0.6095 - val_accuracy: 1.0000\n",
            "Epoch 2315/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6219 - accuracy: 1.0000 - val_loss: 0.6033 - val_accuracy: 1.0000\n",
            "Epoch 2316/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6363 - accuracy: 1.0000 - val_loss: 0.6026 - val_accuracy: 1.0000\n",
            "Epoch 2317/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6463 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
            "Epoch 2318/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6233 - accuracy: 1.0000 - val_loss: 0.6136 - val_accuracy: 1.0000\n",
            "Epoch 2319/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6266 - accuracy: 1.0000 - val_loss: 0.7784 - val_accuracy: 1.0000\n",
            "Epoch 2320/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6363 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 1.0000\n",
            "Epoch 2321/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6265 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n",
            "Epoch 2322/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6390 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 1.0000\n",
            "Epoch 2323/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6511 - accuracy: 1.0000 - val_loss: 0.5954 - val_accuracy: 1.0000\n",
            "Epoch 2324/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6139 - accuracy: 1.0000 - val_loss: 0.6873 - val_accuracy: 1.0000\n",
            "Epoch 2325/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6390 - accuracy: 1.0000 - val_loss: 0.6042 - val_accuracy: 1.0000\n",
            "Epoch 2326/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6523 - accuracy: 1.0000 - val_loss: 0.6156 - val_accuracy: 1.0000\n",
            "Epoch 2327/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6009 - accuracy: 1.0000 - val_loss: 0.6408 - val_accuracy: 1.0000\n",
            "Epoch 2328/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6330 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 1.0000\n",
            "Epoch 2329/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6462 - accuracy: 1.0000 - val_loss: 0.5956 - val_accuracy: 1.0000\n",
            "Epoch 2330/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6394 - accuracy: 1.0000 - val_loss: 0.6000 - val_accuracy: 1.0000\n",
            "Epoch 2331/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6136 - accuracy: 1.0000 - val_loss: 0.7068 - val_accuracy: 1.0000\n",
            "Epoch 2332/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6375 - accuracy: 1.0000 - val_loss: 0.6011 - val_accuracy: 1.0000\n",
            "Epoch 2333/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6325 - accuracy: 1.0000 - val_loss: 0.5983 - val_accuracy: 1.0000\n",
            "Epoch 2334/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6379 - accuracy: 1.0000 - val_loss: 0.6529 - val_accuracy: 1.0000\n",
            "Epoch 2335/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6558 - accuracy: 1.0000 - val_loss: 0.6177 - val_accuracy: 1.0000\n",
            "Epoch 2336/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6183 - accuracy: 1.0000 - val_loss: 0.7913 - val_accuracy: 1.0000\n",
            "Epoch 2337/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6280 - accuracy: 1.0000 - val_loss: 0.6443 - val_accuracy: 1.0000\n",
            "Epoch 2338/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6317 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 1.0000\n",
            "Epoch 2339/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6581 - accuracy: 1.0000 - val_loss: 0.6059 - val_accuracy: 1.0000\n",
            "Epoch 2340/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6024 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 1.0000\n",
            "Epoch 2341/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6456 - accuracy: 1.0000 - val_loss: 0.5972 - val_accuracy: 1.0000\n",
            "Epoch 2342/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6196 - accuracy: 1.0000 - val_loss: 0.6496 - val_accuracy: 1.0000\n",
            "Epoch 2343/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6338 - accuracy: 1.0000 - val_loss: 0.5969 - val_accuracy: 1.0000\n",
            "Epoch 2344/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6035 - val_accuracy: 1.0000\n",
            "Epoch 2345/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6220 - accuracy: 1.0000 - val_loss: 0.6223 - val_accuracy: 1.0000\n",
            "Epoch 2346/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6248 - accuracy: 1.0000 - val_loss: 0.6106 - val_accuracy: 1.0000\n",
            "Epoch 2347/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6612 - accuracy: 1.0000 - val_loss: 0.5949 - val_accuracy: 1.0000\n",
            "Epoch 2348/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6119 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 1.0000\n",
            "Epoch 2349/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6249 - accuracy: 1.0000 - val_loss: 0.6837 - val_accuracy: 1.0000\n",
            "Epoch 2350/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6582 - accuracy: 1.0000 - val_loss: 0.5962 - val_accuracy: 1.0000\n",
            "Epoch 2351/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6304 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 1.0000\n",
            "Epoch 2352/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6104 - accuracy: 1.0000 - val_loss: 0.6760 - val_accuracy: 1.0000\n",
            "Epoch 2353/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6527 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 1.0000\n",
            "Epoch 2354/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6159 - accuracy: 1.0000 - val_loss: 0.8996 - val_accuracy: 1.0000\n",
            "Epoch 2355/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6528 - accuracy: 1.0000 - val_loss: 0.6109 - val_accuracy: 1.0000\n",
            "Epoch 2356/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6237 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 1.0000\n",
            "Epoch 2357/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6390 - accuracy: 1.0000 - val_loss: 0.5940 - val_accuracy: 1.0000\n",
            "Epoch 2358/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6165 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 1.0000\n",
            "Epoch 2359/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6396 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 1.0000\n",
            "Epoch 2360/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6249 - accuracy: 1.0000 - val_loss: 0.6285 - val_accuracy: 1.0000\n",
            "Epoch 2361/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6626 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 1.0000\n",
            "Epoch 2362/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6003 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 1.0000\n",
            "Epoch 2363/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6420 - accuracy: 1.0000 - val_loss: 0.5979 - val_accuracy: 1.0000\n",
            "Epoch 2364/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6279 - accuracy: 1.0000 - val_loss: 1.0475 - val_accuracy: 1.0000\n",
            "Epoch 2365/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 1.0000\n",
            "Epoch 2366/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6004 - accuracy: 1.0000 - val_loss: 0.8716 - val_accuracy: 1.0000\n",
            "Epoch 2367/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6400 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 1.0000\n",
            "Epoch 2368/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6269 - accuracy: 1.0000 - val_loss: 0.5982 - val_accuracy: 1.0000\n",
            "Epoch 2369/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6336 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 2370/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7389 - accuracy: 1.0000 - val_loss: 0.8645 - val_accuracy: 1.0000\n",
            "Epoch 2371/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6055 - accuracy: 1.0000 - val_loss: 0.5910 - val_accuracy: 1.0000\n",
            "Epoch 2372/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6091 - accuracy: 1.0000 - val_loss: 0.6353 - val_accuracy: 1.0000\n",
            "Epoch 2373/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6242 - accuracy: 1.0000 - val_loss: 0.6486 - val_accuracy: 1.0000\n",
            "Epoch 2374/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6348 - accuracy: 1.0000 - val_loss: 0.5976 - val_accuracy: 1.0000\n",
            "Epoch 2375/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6176 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
            "Epoch 2376/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6665 - accuracy: 1.0000 - val_loss: 0.7646 - val_accuracy: 1.0000\n",
            "Epoch 2377/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6024 - accuracy: 1.0000 - val_loss: 0.6118 - val_accuracy: 1.0000\n",
            "Epoch 2378/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.8265 - val_accuracy: 1.0000\n",
            "Epoch 2379/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6285 - accuracy: 1.0000 - val_loss: 0.6204 - val_accuracy: 1.0000\n",
            "Epoch 2380/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.5971 - val_accuracy: 1.0000\n",
            "Epoch 2381/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6218 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 1.0000\n",
            "Epoch 2382/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6435 - accuracy: 1.0000 - val_loss: 0.5985 - val_accuracy: 1.0000\n",
            "Epoch 2383/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6357 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 1.0000\n",
            "Epoch 2384/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6335 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 1.0000\n",
            "Epoch 2385/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6244 - accuracy: 1.0000 - val_loss: 0.6054 - val_accuracy: 1.0000\n",
            "Epoch 2386/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6225 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 1.0000\n",
            "Epoch 2387/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6341 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 1.0000\n",
            "Epoch 2388/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 1.0000\n",
            "Epoch 2389/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6300 - accuracy: 1.0000 - val_loss: 0.6010 - val_accuracy: 1.0000\n",
            "Epoch 2390/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6299 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 1.0000\n",
            "Epoch 2391/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6677 - accuracy: 1.0000 - val_loss: 0.6159 - val_accuracy: 1.0000\n",
            "Epoch 2392/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6074 - accuracy: 1.0000 - val_loss: 0.6199 - val_accuracy: 1.0000\n",
            "Epoch 2393/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6376 - accuracy: 1.0000 - val_loss: 1.2073 - val_accuracy: 1.0000\n",
            "Epoch 2394/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6253 - accuracy: 1.0000 - val_loss: 0.6394 - val_accuracy: 1.0000\n",
            "Epoch 2395/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6360 - accuracy: 1.0000 - val_loss: 0.5933 - val_accuracy: 1.0000\n",
            "Epoch 2396/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6406 - accuracy: 1.0000 - val_loss: 0.9021 - val_accuracy: 1.0000\n",
            "Epoch 2397/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6162 - accuracy: 1.0000 - val_loss: 0.7451 - val_accuracy: 1.0000\n",
            "Epoch 2398/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6290 - accuracy: 1.0000 - val_loss: 0.5982 - val_accuracy: 1.0000\n",
            "Epoch 2399/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6474 - accuracy: 1.0000 - val_loss: 0.6141 - val_accuracy: 1.0000\n",
            "Epoch 2400/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6260 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 1.0000\n",
            "Epoch 2401/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 1.0000\n",
            "Epoch 2402/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6028 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 1.0000\n",
            "Epoch 2403/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6335 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 1.0000\n",
            "Epoch 2404/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6931 - accuracy: 1.0000 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
            "Epoch 2405/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6149 - accuracy: 1.0000 - val_loss: 0.6171 - val_accuracy: 1.0000\n",
            "Epoch 2406/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6032 - accuracy: 1.0000 - val_loss: 0.6051 - val_accuracy: 1.0000\n",
            "Epoch 2407/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6249 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 1.0000\n",
            "Epoch 2408/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6894 - accuracy: 1.0000 - val_loss: 0.5942 - val_accuracy: 1.0000\n",
            "Epoch 2409/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5957 - accuracy: 1.0000 - val_loss: 0.7801 - val_accuracy: 1.0000\n",
            "Epoch 2410/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6335 - accuracy: 1.0000 - val_loss: 0.6069 - val_accuracy: 1.0000\n",
            "Epoch 2411/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6240 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 2412/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6429 - accuracy: 1.0000 - val_loss: 0.6497 - val_accuracy: 1.0000\n",
            "Epoch 2413/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6174 - accuracy: 1.0000 - val_loss: 0.6179 - val_accuracy: 1.0000\n",
            "Epoch 2414/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6609 - accuracy: 1.0000 - val_loss: 0.5965 - val_accuracy: 1.0000\n",
            "Epoch 2415/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6091 - accuracy: 1.0000 - val_loss: 0.5982 - val_accuracy: 1.0000\n",
            "Epoch 2416/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6304 - accuracy: 1.0000 - val_loss: 0.6326 - val_accuracy: 1.0000\n",
            "Epoch 2417/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6354 - accuracy: 1.0000 - val_loss: 0.6369 - val_accuracy: 1.0000\n",
            "Epoch 2418/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6328 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 1.0000\n",
            "Epoch 2419/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6175 - accuracy: 1.0000 - val_loss: 0.7482 - val_accuracy: 1.0000\n",
            "Epoch 2420/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6426 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 1.0000\n",
            "Epoch 2421/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6335 - accuracy: 1.0000 - val_loss: 0.6151 - val_accuracy: 1.0000\n",
            "Epoch 2422/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6306 - accuracy: 1.0000 - val_loss: 0.6779 - val_accuracy: 1.0000\n",
            "Epoch 2423/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6588 - accuracy: 1.0000 - val_loss: 0.5985 - val_accuracy: 1.0000\n",
            "Epoch 2424/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6023 - accuracy: 1.0000 - val_loss: 0.6122 - val_accuracy: 1.0000\n",
            "Epoch 2425/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6305 - accuracy: 1.0000 - val_loss: 0.5914 - val_accuracy: 1.0000\n",
            "Epoch 2426/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6332 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 1.0000\n",
            "Epoch 2427/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6256 - accuracy: 1.0000 - val_loss: 0.6673 - val_accuracy: 1.0000\n",
            "Epoch 2428/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6347 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 1.0000\n",
            "Epoch 2429/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6282 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 1.0000\n",
            "Epoch 2430/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6461 - accuracy: 1.0000 - val_loss: 0.6029 - val_accuracy: 1.0000\n",
            "Epoch 2431/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6164 - accuracy: 1.0000 - val_loss: 0.5938 - val_accuracy: 1.0000\n",
            "Epoch 2432/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6493 - accuracy: 1.0000 - val_loss: 0.6156 - val_accuracy: 1.0000\n",
            "Epoch 2433/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6221 - accuracy: 1.0000 - val_loss: 0.6811 - val_accuracy: 1.0000\n",
            "Epoch 2434/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6265 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 1.0000\n",
            "Epoch 2435/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7035 - accuracy: 1.0000 - val_loss: 0.5924 - val_accuracy: 1.0000\n",
            "Epoch 2436/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5987 - accuracy: 1.0000 - val_loss: 0.6651 - val_accuracy: 1.0000\n",
            "Epoch 2437/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6314 - accuracy: 1.0000 - val_loss: 0.5959 - val_accuracy: 1.0000\n",
            "Epoch 2438/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6218 - accuracy: 1.0000 - val_loss: 1.3215 - val_accuracy: 1.0000\n",
            "Epoch 2439/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6273 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 1.0000\n",
            "Epoch 2440/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6352 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 1.0000\n",
            "Epoch 2441/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6210 - accuracy: 1.0000 - val_loss: 0.7632 - val_accuracy: 1.0000\n",
            "Epoch 2442/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6443 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 1.0000\n",
            "Epoch 2443/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6209 - accuracy: 1.0000 - val_loss: 0.8211 - val_accuracy: 1.0000\n",
            "Epoch 2444/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6225 - accuracy: 1.0000 - val_loss: 0.5995 - val_accuracy: 1.0000\n",
            "Epoch 2445/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6433 - accuracy: 1.0000 - val_loss: 0.5929 - val_accuracy: 1.0000\n",
            "Epoch 2446/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6482 - accuracy: 1.0000 - val_loss: 0.6281 - val_accuracy: 1.0000\n",
            "Epoch 2447/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6452 - accuracy: 1.0000 - val_loss: 0.5946 - val_accuracy: 1.0000\n",
            "Epoch 2448/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6174 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 1.0000\n",
            "Epoch 2449/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6295 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 1.0000\n",
            "Epoch 2450/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6436 - accuracy: 1.0000 - val_loss: 0.6654 - val_accuracy: 1.0000\n",
            "Epoch 2451/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6253 - accuracy: 1.0000 - val_loss: 0.6145 - val_accuracy: 1.0000\n",
            "Epoch 2452/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6629 - accuracy: 1.0000 - val_loss: 0.6176 - val_accuracy: 1.0000\n",
            "Epoch 2453/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5964 - accuracy: 1.0000 - val_loss: 0.5944 - val_accuracy: 1.0000\n",
            "Epoch 2454/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6403 - accuracy: 1.0000 - val_loss: 0.6154 - val_accuracy: 1.0000\n",
            "Epoch 2455/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6276 - accuracy: 1.0000 - val_loss: 0.6084 - val_accuracy: 1.0000\n",
            "Epoch 2456/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6148 - accuracy: 1.0000 - val_loss: 0.6008 - val_accuracy: 1.0000\n",
            "Epoch 2457/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6327 - accuracy: 1.0000 - val_loss: 0.5957 - val_accuracy: 1.0000\n",
            "Epoch 2458/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.6276 - val_accuracy: 1.0000\n",
            "Epoch 2459/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6381 - accuracy: 1.0000 - val_loss: 1.1899 - val_accuracy: 1.0000\n",
            "Epoch 2460/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6354 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 1.0000\n",
            "Epoch 2461/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6310 - accuracy: 1.0000 - val_loss: 0.5939 - val_accuracy: 1.0000\n",
            "Epoch 2462/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6334 - accuracy: 1.0000 - val_loss: 0.6268 - val_accuracy: 1.0000\n",
            "Epoch 2463/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6210 - accuracy: 1.0000 - val_loss: 0.6770 - val_accuracy: 1.0000\n",
            "Epoch 2464/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6372 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 1.0000\n",
            "Epoch 2465/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6367 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 1.0000\n",
            "Epoch 2466/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6168 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 1.0000\n",
            "Epoch 2467/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6244 - accuracy: 1.0000 - val_loss: 0.5963 - val_accuracy: 1.0000\n",
            "Epoch 2468/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6490 - accuracy: 1.0000 - val_loss: 0.6205 - val_accuracy: 1.0000\n",
            "Epoch 2469/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6175 - accuracy: 1.0000 - val_loss: 0.6526 - val_accuracy: 1.0000\n",
            "Epoch 2470/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6254 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 1.0000\n",
            "Epoch 2471/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6429 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 1.0000\n",
            "Epoch 2472/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6191 - accuracy: 1.0000 - val_loss: 0.6154 - val_accuracy: 1.0000\n",
            "Epoch 2473/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6734 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 1.0000\n",
            "Epoch 2474/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5956 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 1.0000\n",
            "Epoch 2475/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6347 - accuracy: 1.0000 - val_loss: 0.6032 - val_accuracy: 1.0000\n",
            "Epoch 2476/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6350 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 1.0000\n",
            "Epoch 2477/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6342 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 1.0000\n",
            "Epoch 2478/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6474 - accuracy: 1.0000 - val_loss: 0.6205 - val_accuracy: 1.0000\n",
            "Epoch 2479/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6212 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 1.0000\n",
            "Epoch 2480/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6249 - accuracy: 1.0000 - val_loss: 0.6073 - val_accuracy: 1.0000\n",
            "Epoch 2481/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6236 - accuracy: 1.0000 - val_loss: 0.6076 - val_accuracy: 1.0000\n",
            "Epoch 2482/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6499 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 1.0000\n",
            "Epoch 2483/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6578 - accuracy: 1.0000 - val_loss: 0.6196 - val_accuracy: 1.0000\n",
            "Epoch 2484/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6966 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 1.0000\n",
            "Epoch 2485/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6609 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 1.0000\n",
            "Epoch 2486/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5964 - accuracy: 1.0000 - val_loss: 0.5970 - val_accuracy: 1.0000\n",
            "Epoch 2487/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6103 - accuracy: 1.0000 - val_loss: 0.5998 - val_accuracy: 1.0000\n",
            "Epoch 2488/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6281 - accuracy: 1.0000 - val_loss: 0.5916 - val_accuracy: 1.0000\n",
            "Epoch 2489/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6303 - accuracy: 1.0000 - val_loss: 0.5970 - val_accuracy: 1.0000\n",
            "Epoch 2490/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6393 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 1.0000\n",
            "Epoch 2491/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6190 - accuracy: 1.0000 - val_loss: 0.9187 - val_accuracy: 1.0000\n",
            "Epoch 2492/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6146 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 1.0000\n",
            "Epoch 2493/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6538 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 1.0000\n",
            "Epoch 2494/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6254 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 1.0000\n",
            "Epoch 2495/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6236 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 1.0000\n",
            "Epoch 2496/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6545 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 1.0000\n",
            "Epoch 2497/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6037 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 1.0000\n",
            "Epoch 2498/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6310 - accuracy: 1.0000 - val_loss: 0.6566 - val_accuracy: 1.0000\n",
            "Epoch 2499/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6854 - accuracy: 1.0000 - val_loss: 0.5932 - val_accuracy: 1.0000\n",
            "Epoch 2500/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5939 - accuracy: 1.0000 - val_loss: 0.6155 - val_accuracy: 1.0000\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "n_inputs=12\n",
        "Mod1(X_train, X_test, y_train, y_test,n_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jks0kP6HepZi"
      },
      "source": [
        "### **Passive Aggressive Regressor Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJGEsrJc8YYq"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import PassiveAggressiveRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9OYiK3l_Jsw",
        "outputId": "694a04f3-871c-434a-c652-a62e335a624e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Train (8011, 12) (8011, 1) Test (2671, 12) (2671, 1)\n",
            "(8011, 8)\n",
            "(2671, 8)\n",
            "Model saved succesfully!!!\n",
            "Loaded Model Sucessfully\n",
            "Regression  Model Training Score: -7.371\n"
          ]
        }
      ],
      "source": [
        "model =  PassiveAggressiveRegressor(max_iter=100)\n",
        "encoder = load_model('STAE1-4-encoder.h5')\n",
        "print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "X_val_encode = encoder.predict(X_val)\n",
        "print(X_train_encode.shape)\n",
        "print(X_test_encode.shape)\n",
        "model.fit(X_train_encode,y_train)\n",
        "filename = 'PAR.h5'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "print(\"Model saved succesfully!!!\")\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "print(\"Loaded Model Sucessfully\")\n",
        "yhat = loaded_model.predict(X_test_encode)\n",
        "print('Regression  Model Training Score: %.3f' % (r2_score(y_test, yhat)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7E41BZRc6Y1",
        "outputId": "715d4e6c-9da8-4e6d-e325-cadd28d447b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "encoder = load_model('STAE1-4-encoder.h5')\n",
        "filename = 'PAR.h5'\n",
        "X_val_encode = encoder.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjrC1Y0I_8Be",
        "outputId": "ff9f10d7-e52b-43e8-9a0f-8808fb20c3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Model Sucessfully\n"
          ]
        }
      ],
      "source": [
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "print(\"Loaded Model Sucessfully\")\n",
        "yhat1 = loaded_model.predict(X_val_encode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBlzhOBEexyj"
      },
      "source": [
        "## **Model Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "GKRkAdsKBn-F"
      },
      "outputs": [],
      "source": [
        "sam=pd.read_excel('Sample_submission.xlsx')\n",
        "y_val = sam.iloc[:,:].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REK7uIxG__My",
        "outputId": "29294e67-67bd-4a9c-966d-93a37b333a24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15998],\n",
              "       [16612],\n",
              "       [25572],\n",
              "       ...,\n",
              "       [22720],\n",
              "       [23544],\n",
              "       [27489]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZlAa1WDAA8z",
        "outputId": "3e823b13-4ca4-45e4-ca66-9512a4fdce99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19867.51331457],\n",
              "       [11698.64979108],\n",
              "       [32604.79477069],\n",
              "       ...,\n",
              "       [15477.12123052],\n",
              "       [24092.42496119],\n",
              "       [24079.83566476]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "yhat1=sc.inverse_transform(yhat1.reshape(len(yhat1),1))\n",
        "yhat1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkB6O7Eue2EQ"
      },
      "source": [
        "### **Model Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "NcvlK2o6DPz6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import max_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_poisson_deviance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJenV9mpImwH",
        "outputId": "ca511336-0ed1-4a19-c005-256cf7c91d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Explained Variance score: -3.711919397853385\n",
            "\n",
            " Max Score: 41981.75965752338\n",
            "\n",
            " Mean absolute error Score: 8905.253690627753\n",
            "\n",
            " Mean Squared log Error Score: 0.341349451312468\n",
            "\n",
            " Median absolute Score: 7883.385911993417\n",
            "\n",
            " Mean absolute Percentage Error Score: 0.45387153248081674\n",
            "\n",
            " R2 Score: -3.7556634975667507\n"
          ]
        }
      ],
      "source": [
        "print('\\n Explained Variance score:',explained_variance_score(y_val, yhat1))\n",
        "print('\\n Max Score:',max_error(y_val, yhat1))\n",
        "print('\\n Mean absolute error Score:',mean_absolute_error(y_val, yhat1))\n",
        "print('\\n Mean Squared log Error Score:',mean_squared_log_error(y_val, yhat1))\n",
        "print('\\n Median absolute Score:',median_absolute_error(y_val, yhat1))\n",
        "print('\\n Mean absolute Percentage Error Score:',mean_absolute_percentage_error(y_val, yhat1))\n",
        "print('\\n R2 Score:',r2_score(y_val, yhat1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Classification Training and Assesment**"
      ],
      "metadata": {
        "id": "UQ4KFIwo62xw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf_U0lAue6cd"
      },
      "source": [
        "## **Prediction of Airline Type using MLP Classifier with Stacked Autoencoder**\n",
        "\n",
        "**Note: To predict Airline Type , we first require the airline price based on which we predict/validate the same.Hence this model is termed as a Cascaded Model / Pipeline Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "TwTArHX8Mnby"
      },
      "outputs": [],
      "source": [
        "train_df=pd.read_csv('Train.csv')\n",
        "test_df=pd.read_csv('Test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "eWNMBdK6M6xz",
        "outputId": "7705cf66-1ab1-4ad9-e559-02f482a0e19d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-556c0af7-f4e3-493e-bdb5-39ae8ad57ef7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "      <th>Estimated Price</th>\n",
              "      <th>Airline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3897</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → IXR → BBI → BLR</td>\n",
              "      <td>445</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>7662</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → LKO → BOM → COK</td>\n",
              "      <td>1140</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>13882</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → NAG → BLR</td>\n",
              "      <td>325</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>30</td>\n",
              "      <td>6218</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → NAG → DEL</td>\n",
              "      <td>285</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>50</td>\n",
              "      <td>21</td>\n",
              "      <td>35</td>\n",
              "      <td>13302</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10677</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>150</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>4107</td>\n",
              "      <td>Air Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>45</td>\n",
              "      <td>23</td>\n",
              "      <td>20</td>\n",
              "      <td>4145</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>180</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>7229</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>New Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>160</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>12648</td>\n",
              "      <td>Vistara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → GOI → BOM → COK</td>\n",
              "      <td>500</td>\n",
              "      <td>2 stops</td>\n",
              "      <td>No info</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>11753</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-556c0af7-f4e3-493e-bdb5-39ae8ad57ef7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-556c0af7-f4e3-493e-bdb5-39ae8ad57ef7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-556c0af7-f4e3-493e-bdb5-39ae8ad57ef7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         Source Destination  ... Estimated Price      Airline\n",
              "0      Banglore   New Delhi  ...            3897       IndiGo\n",
              "1       Kolkata    Banglore  ...            7662    Air India\n",
              "2         Delhi      Cochin  ...           13882  Jet Airways\n",
              "3       Kolkata    Banglore  ...            6218       IndiGo\n",
              "4      Banglore   New Delhi  ...           13302       IndiGo\n",
              "...         ...         ...  ...             ...          ...\n",
              "10677   Kolkata    Banglore  ...            4107     Air Asia\n",
              "10678   Kolkata    Banglore  ...            4145    Air India\n",
              "10679  Banglore       Delhi  ...            7229  Jet Airways\n",
              "10680  Banglore   New Delhi  ...           12648      Vistara\n",
              "10681     Delhi      Cochin  ...           11753    Air India\n",
              "\n",
              "[10682 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "train_df['Airline']= train_df['Airline Type']\n",
        "train_df=train_df.drop(columns=['Unnamed: 0','Airline Type'])\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "FKMAvgyaM7ux",
        "outputId": "407921c8-d161-4e64-f5e0-a94d1aeb83f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3448aaef-407c-492e-8955-1fa64e0dccfb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "      <th>Price</th>\n",
              "      <th>Airline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>655</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>19868</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → MAA → BLR</td>\n",
              "      <td>240</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>11699</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>1425</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>In-flight meal not included</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>32605</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>780</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>20370</td>\n",
              "      <td>Multiple carriers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Banglore</td>\n",
              "      <td>Delhi</td>\n",
              "      <td>BLR → DEL</td>\n",
              "      <td>170</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "      <td>7754</td>\n",
              "      <td>Air Asia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → DEL → BLR</td>\n",
              "      <td>1435</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "      <td>34122</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>Kolkata</td>\n",
              "      <td>Banglore</td>\n",
              "      <td>CCU → BLR</td>\n",
              "      <td>155</td>\n",
              "      <td>non-stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>55</td>\n",
              "      <td>9853</td>\n",
              "      <td>IndiGo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>395</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>50</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>15477</td>\n",
              "      <td>Jet Airways</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>915</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>24092</td>\n",
              "      <td>Air India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>Delhi</td>\n",
              "      <td>Cochin</td>\n",
              "      <td>DEL → BOM → COK</td>\n",
              "      <td>860</td>\n",
              "      <td>1 stop</td>\n",
              "      <td>No info</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>24080</td>\n",
              "      <td>Multiple carriers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3448aaef-407c-492e-8955-1fa64e0dccfb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3448aaef-407c-492e-8955-1fa64e0dccfb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3448aaef-407c-492e-8955-1fa64e0dccfb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Source Destination  ...  Price            Airline\n",
              "0        Delhi      Cochin  ...  19868        Jet Airways\n",
              "1      Kolkata    Banglore  ...  11699             IndiGo\n",
              "2        Delhi      Cochin  ...  32605        Jet Airways\n",
              "3        Delhi      Cochin  ...  20370  Multiple carriers\n",
              "4     Banglore       Delhi  ...   7754           Air Asia\n",
              "...        ...         ...  ...    ...                ...\n",
              "2666   Kolkata    Banglore  ...  34122          Air India\n",
              "2667   Kolkata    Banglore  ...   9853             IndiGo\n",
              "2668     Delhi      Cochin  ...  15477        Jet Airways\n",
              "2669     Delhi      Cochin  ...  24092          Air India\n",
              "2670     Delhi      Cochin  ...  24080  Multiple carriers\n",
              "\n",
              "[2671 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "test_df['Price'] = np.round(yhat1). astype(int)\n",
        "test_df['Airline']= test_df['Airline Type']\n",
        "test_df = test_df.drop(columns=['Unnamed: 0','Airline Type'])\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8FTnp-tQNMPE"
      },
      "outputs": [],
      "source": [
        "def LE1(df):\n",
        "  le1= LabelEncoder()\n",
        "  le2= LabelEncoder()\n",
        "  le3= LabelEncoder()\n",
        "  le4= LabelEncoder()\n",
        "  le5= LabelEncoder()\n",
        "  le6 =LabelEncoder()\n",
        "  df['Source']=le1.fit_transform(df['Source'])\n",
        "  df['Destination']=le2.fit_transform(df['Destination'])\n",
        "  df['Route']=le3.fit_transform(df['Route'])\n",
        "  df['Total_Stops']=le4.fit_transform(df['Total_Stops'])\n",
        "  df['Additional_Info']=le5.fit_transform(df['Additional_Info'])\n",
        "  df['Airline']= le6.fit_transform(df['Airline'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "IXx8qyiQNT2V"
      },
      "outputs": [],
      "source": [
        "train_df= LE1(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DYzXa414O2kN",
        "outputId": "bb8876b0-224a-4744-efb8-b58eac443bb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-604e1381-c394-4f1e-a365-1177fad4a18e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "      <th>Estimated Price</th>\n",
              "      <th>Airline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>170</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3897</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>84</td>\n",
              "      <td>445</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>15</td>\n",
              "      <td>7662</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>118</td>\n",
              "      <td>1140</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>13882</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>91</td>\n",
              "      <td>325</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>30</td>\n",
              "      <td>6218</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>29</td>\n",
              "      <td>285</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>50</td>\n",
              "      <td>21</td>\n",
              "      <td>35</td>\n",
              "      <td>13302</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10677</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>150</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>4107</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10678</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>155</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>45</td>\n",
              "      <td>23</td>\n",
              "      <td>20</td>\n",
              "      <td>4145</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10679</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>180</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>27</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>7229</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10680</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>160</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>12648</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>108</td>\n",
              "      <td>500</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>11753</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10682 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-604e1381-c394-4f1e-a365-1177fad4a18e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-604e1381-c394-4f1e-a365-1177fad4a18e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-604e1381-c394-4f1e-a365-1177fad4a18e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Source  Destination  Route  ...  Arrival_min  Estimated Price  Airline\n",
              "0           0            5     18  ...           10             3897        3\n",
              "1           3            0     84  ...           15             7662        1\n",
              "2           2            1    118  ...           25            13882        4\n",
              "3           3            0     91  ...           30             6218        3\n",
              "4           0            5     29  ...           35            13302        3\n",
              "...       ...          ...    ...  ...          ...              ...      ...\n",
              "10677       3            0     64  ...           25             4107        0\n",
              "10678       3            0     64  ...           20             4145        1\n",
              "10679       0            2     18  ...           20             7229        4\n",
              "10680       0            5     18  ...           10            12648       10\n",
              "10681       2            1    108  ...           15            11753        1\n",
              "\n",
              "[10682 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "F-v_H7IkOUj_"
      },
      "outputs": [],
      "source": [
        "test_df=LE1(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aZ2Jl7jvZIJI",
        "outputId": "970e7b9f-8bde-4de9-ade8-ad07a189c138"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ae676903-c107-4678-8c12-48c018c91aa4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source</th>\n",
              "      <th>Destination</th>\n",
              "      <th>Route</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Total_Stops</th>\n",
              "      <th>Additional_Info</th>\n",
              "      <th>Journey_day</th>\n",
              "      <th>Journey_month</th>\n",
              "      <th>Dep_hour</th>\n",
              "      <th>Dep_min</th>\n",
              "      <th>Arrival_hour</th>\n",
              "      <th>Arrival_min</th>\n",
              "      <th>Price</th>\n",
              "      <th>Airline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>655</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>19868</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>65</td>\n",
              "      <td>240</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>11699</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>1425</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>32605</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>780</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>21</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>20370</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>170</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>24</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>55</td>\n",
              "      <td>2</td>\n",
              "      <td>45</td>\n",
              "      <td>7754</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>1435</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "      <td>34122</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>155</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>55</td>\n",
              "      <td>9853</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>395</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>50</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>15477</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>915</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>24092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>76</td>\n",
              "      <td>860</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>24080</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2671 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae676903-c107-4678-8c12-48c018c91aa4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae676903-c107-4678-8c12-48c018c91aa4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae676903-c107-4678-8c12-48c018c91aa4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Source  Destination  Route  ...  Arrival_min  Price  Airline\n",
              "0          2            1     76  ...           25  19868        4\n",
              "1          3            0     65  ...           20  11699        3\n",
              "2          2            1     76  ...            0  32605        4\n",
              "3          2            1     76  ...            0  20370        6\n",
              "4          0            2     16  ...           45   7754        0\n",
              "...      ...          ...    ...  ...          ...    ...      ...\n",
              "2666       3            0     51  ...           25  34122        1\n",
              "2667       3            0     43  ...           55   9853        3\n",
              "2668       2            1     76  ...           25  15477        4\n",
              "2669       2            1     76  ...           15  24092        1\n",
              "2670       2            1     76  ...           15  24080        6\n",
              "\n",
              "[2671 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "2Yvatn4ZPKDR"
      },
      "outputs": [],
      "source": [
        "X=train_df.iloc[:,:-1].values\n",
        "y=train_df.iloc[:,-1].values\n",
        "X_val=test_df.iloc[:,:-1].values\n",
        "y_val=test_df.iloc[:,-1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFeges7tPOay",
        "outputId": "3c3f0a9a-bd77-4c57-b7ff-6cf9228ed191"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     5,    18, ...,     1,    10,  3897],\n",
              "       [    3,     0,    84, ...,    13,    15,  7662],\n",
              "       [    2,     1,   118, ...,     4,    25, 13882],\n",
              "       ...,\n",
              "       [    0,     2,    18, ...,    11,    20,  7229],\n",
              "       [    0,     5,    18, ...,    14,    10, 12648],\n",
              "       [    2,     1,   108, ...,    19,    15, 11753]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "PmvHqJuXPSZU"
      },
      "outputs": [],
      "source": [
        "y=y.reshape(len(y),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTNiiwWfPS3n",
        "outputId": "f5cf1b8a-d4a8-4312-c7a7-2c7aa8570535"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3],\n",
              "       [ 1],\n",
              "       [ 4],\n",
              "       ...,\n",
              "       [ 4],\n",
              "       [10],\n",
              "       [ 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5jzVpLYP4Dd",
        "outputId": "9fbb8178-1d58-4771-86cf-29a0d310cb95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3897],\n",
              "       [ 7662],\n",
              "       [13882],\n",
              "       ...,\n",
              "       [ 7229],\n",
              "       [12648],\n",
              "       [11753]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "X[:,12:13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZltr40MPYtg",
        "outputId": "e36d95b5-6850-4d4f-bf47-6e69613c6132"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   5,  18, ...,   1,  10,  -1],\n",
              "       [  3,   0,  84, ...,  13,  15,   0],\n",
              "       [  2,   1, 118, ...,   4,  25,   1],\n",
              "       ...,\n",
              "       [  0,   2,  18, ...,  11,  20,   0],\n",
              "       [  0,   5,  18, ...,  14,  10,   0],\n",
              "       [  2,   1, 108, ...,  19,  15,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "sc = StandardScaler()\n",
        "X[:,12:13]=sc.fit_transform(X[:,12:13])\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHxSXSXAPcG3",
        "outputId": "139cdcf0-8fab-4986-fb8e-6509fe2cda23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10682, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P2o8XbdPdAD",
        "outputId": "9ca87a7c-c31f-4b99-cb94-af91e5973fa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10682, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLOQhUYSZoLB",
        "outputId": "8c0c18c1-86b6-4cbe-c24a-8f0dded4f7a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    2,     1,    76, ...,     4,    25, 19868],\n",
              "       [    3,     0,    65, ...,    10,    20, 11699],\n",
              "       [    2,     1,    76, ...,    19,     0, 32605],\n",
              "       ...,\n",
              "       [    2,     1,    76, ...,     4,    25, 15477],\n",
              "       [    2,     1,    76, ...,    19,    15, 24092],\n",
              "       [    2,     1,    76, ...,    19,    15, 24080]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "JFccCuv_ZpOD"
      },
      "outputs": [],
      "source": [
        "y_val=y_val.reshape(len(y_val),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyGeSXByZtX8",
        "outputId": "93bbe546-103e-43ba-d44d-cbb361679ad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4],\n",
              "       [3],\n",
              "       [4],\n",
              "       ...,\n",
              "       [4],\n",
              "       [1],\n",
              "       [6]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZanhjeBZuhH",
        "outputId": "a237cfb4-ba58-4b36-b16b-a965ecf8cbd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19868],\n",
              "       [11699],\n",
              "       [32605],\n",
              "       ...,\n",
              "       [15477],\n",
              "       [24092],\n",
              "       [24080]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "X_val[:,12:13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUIS1xQhZw1o",
        "outputId": "1276b95e-ce3a-4574-a562-f8b1dcf0283c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2,  1, 76, ...,  4, 25,  0],\n",
              "       [ 3,  0, 65, ..., 10, 20,  0],\n",
              "       [ 2,  1, 76, ..., 19,  0,  1],\n",
              "       ...,\n",
              "       [ 2,  1, 76, ...,  4, 25,  0],\n",
              "       [ 2,  1, 76, ..., 19, 15,  0],\n",
              "       [ 2,  1, 76, ..., 19, 15,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "sc = StandardScaler()\n",
        "X_val[:,12:13]=sc.fit_transform(X_val[:,12:13])\n",
        "X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv7TMIMXZ1ho",
        "outputId": "4e21d519-5983-4b98-f13a-3f3e7f551ada"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2671, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "X_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5uweeX8Z4bz",
        "outputId": "99d30b75-770b-4e72-e7ac-4667d8ed0aaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2671, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "tYBdzPqIPdko"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25 ,random_state = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_qI9ZbGfhOD"
      },
      "source": [
        "### **Feature Engineering Using Stacked Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9HNAbI6PJfg"
      },
      "outputs": [],
      "source": [
        "def Mod(X_train, X_test, y_train, y_test,n_inputs):\n",
        "    learning_rate = 1e-5\n",
        "    input_shape=Input(shape=(n_inputs,))\n",
        "    # Encoding layers\n",
        "    e1 = layers.Dense(n_inputs, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(input_shape)\n",
        "    e2 = layers.Dense(9, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e1)\n",
        "    e3 = layers.Dense(7, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e2)\n",
        "    e4 = layers.Dense(5, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e3)\n",
        "    e5 = layers.Dense(3, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e4)\n",
        "    #Decoding layers\n",
        "    d1 = layers.Dense(3, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(e3)\n",
        "    d2 = layers.Dense(5, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d1)\n",
        "    d3 = layers.Dense(7, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d2)\n",
        "    d4 = layers.Dense(9, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d3)\n",
        "    d5 = layers.Dense(n_inputs, activation='relu',activity_regularizer=regularizers.l1(learning_rate))(d4)\n",
        "    # output layer\n",
        "    output = layers.Dense(n_inputs, activation='linear',activity_regularizer=regularizers.l1(learning_rate))(d5)\n",
        "    autoencoder = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history=autoencoder.fit(X_train, X_train,epochs=2500,batch_size=2500,shuffle=True,validation_data=(X_test,X_test))\n",
        "    plt.plot(history.history['accuracy'], label='Encoder-1:train')\n",
        "    plt.plot(history.history['val_accuracy'], label='Encoder-1:test')\n",
        "    plt.title(\"Autoencoder-1 Results\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE1-41.png\")\n",
        "    autoencoder_1_input = autoencoder.predict(X_train)\n",
        "    autoencoder_1_input = np.concatenate((autoencoder_1_input , X_train))\n",
        "    autoencoder_1 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_1.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history1=autoencoder_1.fit(autoencoder_1_input,autoencoder_1_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_1_input,autoencoder_1_input))\n",
        "    plt.plot(history1.history['accuracy'], label='Encoder-2:train')\n",
        "    plt.plot(history1.history['val_accuracy'], label='Encoder-2:test')\n",
        "    plt.title(\"Autoencoder-2 Results\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE2-42.png\")\n",
        "    autoencoder_2_input = autoencoder_1.predict(autoencoder_1_input)\n",
        "    autoencoder_2_input = np.concatenate((autoencoder_2_input, autoencoder_1_input))\n",
        "    autoencoder_2 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_2.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history2=autoencoder_2.fit(autoencoder_2_input,autoencoder_2_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_2_input,autoencoder_2_input))\n",
        "    plt.plot(history2.history['accuracy'], label='Encoder-3:train')\n",
        "    plt.plot(history2.history['val_accuracy'], label='Encoder-3:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE3-43.png\")\n",
        "    autoencoder_3_input = autoencoder_2.predict(autoencoder_2_input)\n",
        "    autoencoder_3_input = np.concatenate((autoencoder_3_input, autoencoder_2_input))\n",
        "    autoencoder_3 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_3.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history3=autoencoder_3.fit(autoencoder_3_input,autoencoder_3_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_3_input,autoencoder_3_input))\n",
        "    plt.plot(history3.history['accuracy'], label='Encoder-4:train')\n",
        "    plt.plot(history3.history['val_accuracy'], label='Encoder-4:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE4-44.png\")\n",
        "    autoencoder_4_input = autoencoder_3.predict(autoencoder_3_input)\n",
        "    autoencoder_4_input = np.concatenate((autoencoder_4_input, autoencoder_3_input))\n",
        "    autoencoder_4 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_4.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history4=autoencoder_4.fit(autoencoder_4_input,autoencoder_4_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_4_input,autoencoder_4_input))\n",
        "    plt.plot(history4.history['accuracy'], label='Encoder-5:train')\n",
        "    plt.plot(history4.history['val_accuracy'], label='Encoder-5:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE5-45.png\")\n",
        "    autoencoder_5_input = autoencoder_4.predict(autoencoder_4_input)\n",
        "    autoencoder_5_input = np.concatenate((autoencoder_5_input, autoencoder_4_input))\n",
        "    autoencoder_5 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_5.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history5=autoencoder_5.fit(autoencoder_5_input,autoencoder_5_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_5_input,autoencoder_5_input))\n",
        "    plt.plot(history5.history['accuracy'], label='Encoder-6:train')\n",
        "    plt.plot(history5.history['val_accuracy'], label='Encoder-6:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE5-46.png\")\n",
        "    autoencoder_6_input = autoencoder_5.predict(autoencoder_5_input)\n",
        "    autoencoder_6_input = np.concatenate((autoencoder_6_input, autoencoder_5_input))\n",
        "    autoencoder_6 = Model(inputs=input_shape, outputs=output)\n",
        "    autoencoder_6.compile(metrics=['accuracy'],optimizer='adam', loss='mse')\n",
        "    history6=autoencoder_5.fit(autoencoder_6_input,autoencoder_6_input,epochs=2500,batch_size=2500,shuffle=True,validation_data=(autoencoder_6_input,autoencoder_6_input))\n",
        "    plt.plot(history6.history['accuracy'], label='Encoder-7:train')\n",
        "    plt.plot(history6.history['val_accuracy'], label='Encoder-7:test')\n",
        "    plt.title(\"Stacked Autoencoder Results (Layers:5)\")\n",
        "    plt.ylabel(\"Accuracy--->\")\n",
        "    plt.xlabel(\"Epochs--->\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"STAE5-47.png\")\n",
        "    encoder = Model(inputs=input_shape, outputs=e3)\n",
        "    encoder.save('STAE2-4-encoder.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xVprVirYQb-h",
        "outputId": "6f5f5d0e-32c5-4b23-9d5b-f43a53813ccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.8998 - val_accuracy: 1.0000\n",
            "Epoch 2/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 1.8920 - val_accuracy: 1.0000\n",
            "Epoch 3/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9099 - val_accuracy: 1.0000\n",
            "Epoch 4/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9305 - accuracy: 1.0000 - val_loss: 1.9207 - val_accuracy: 1.0000\n",
            "Epoch 5/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9127 - accuracy: 1.0000 - val_loss: 1.9155 - val_accuracy: 1.0000\n",
            "Epoch 6/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9230 - accuracy: 1.0000 - val_loss: 1.8930 - val_accuracy: 1.0000\n",
            "Epoch 7/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9403 - accuracy: 1.0000 - val_loss: 1.9828 - val_accuracy: 1.0000\n",
            "Epoch 8/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.9515 - val_accuracy: 1.0000\n",
            "Epoch 9/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 1.9187 - val_accuracy: 1.0000\n",
            "Epoch 10/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9265 - accuracy: 1.0000 - val_loss: 1.8988 - val_accuracy: 1.0000\n",
            "Epoch 11/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9277 - accuracy: 1.0000 - val_loss: 1.9260 - val_accuracy: 1.0000\n",
            "Epoch 12/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9337 - accuracy: 1.0000 - val_loss: 1.9036 - val_accuracy: 1.0000\n",
            "Epoch 13/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 2.0001 - val_accuracy: 1.0000\n",
            "Epoch 14/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9323 - accuracy: 1.0000 - val_loss: 1.9078 - val_accuracy: 1.0000\n",
            "Epoch 15/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 2.0917 - val_accuracy: 1.0000\n",
            "Epoch 16/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9325 - accuracy: 1.0000 - val_loss: 1.9724 - val_accuracy: 1.0000\n",
            "Epoch 17/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 1.9164 - val_accuracy: 1.0000\n",
            "Epoch 18/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9433 - val_accuracy: 1.0000\n",
            "Epoch 19/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9307 - accuracy: 1.0000 - val_loss: 1.9576 - val_accuracy: 1.0000\n",
            "Epoch 20/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.8919 - val_accuracy: 1.0000\n",
            "Epoch 21/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9341 - accuracy: 1.0000 - val_loss: 1.8950 - val_accuracy: 1.0000\n",
            "Epoch 22/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9321 - accuracy: 1.0000 - val_loss: 1.9306 - val_accuracy: 1.0000\n",
            "Epoch 23/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9194 - accuracy: 1.0000 - val_loss: 2.0945 - val_accuracy: 1.0000\n",
            "Epoch 24/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9318 - accuracy: 1.0000 - val_loss: 1.9218 - val_accuracy: 1.0000\n",
            "Epoch 25/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.9006 - val_accuracy: 1.0000\n",
            "Epoch 26/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9258 - accuracy: 1.0000 - val_loss: 2.2972 - val_accuracy: 1.0000\n",
            "Epoch 27/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9562 - accuracy: 1.0000 - val_loss: 1.9940 - val_accuracy: 1.0000\n",
            "Epoch 28/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 2.0610 - val_accuracy: 1.0000\n",
            "Epoch 29/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9297 - accuracy: 1.0000 - val_loss: 2.0187 - val_accuracy: 1.0000\n",
            "Epoch 30/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9302 - accuracy: 1.0000 - val_loss: 2.1923 - val_accuracy: 1.0000\n",
            "Epoch 31/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9313 - accuracy: 1.0000 - val_loss: 1.9234 - val_accuracy: 1.0000\n",
            "Epoch 32/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9142 - accuracy: 1.0000 - val_loss: 1.9251 - val_accuracy: 1.0000\n",
            "Epoch 33/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9305 - accuracy: 1.0000 - val_loss: 2.0565 - val_accuracy: 1.0000\n",
            "Epoch 34/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9355 - accuracy: 1.0000 - val_loss: 1.9926 - val_accuracy: 1.0000\n",
            "Epoch 35/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9233 - val_accuracy: 1.0000\n",
            "Epoch 36/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9409 - accuracy: 1.0000 - val_loss: 2.0019 - val_accuracy: 1.0000\n",
            "Epoch 37/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9079 - accuracy: 1.0000 - val_loss: 1.8942 - val_accuracy: 1.0000\n",
            "Epoch 38/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9312 - accuracy: 1.0000 - val_loss: 1.8975 - val_accuracy: 1.0000\n",
            "Epoch 39/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9285 - accuracy: 1.0000 - val_loss: 1.9055 - val_accuracy: 1.0000\n",
            "Epoch 40/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9160 - accuracy: 1.0000 - val_loss: 1.9662 - val_accuracy: 1.0000\n",
            "Epoch 41/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9241 - accuracy: 1.0000 - val_loss: 1.9544 - val_accuracy: 1.0000\n",
            "Epoch 42/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9252 - accuracy: 1.0000 - val_loss: 1.9222 - val_accuracy: 1.0000\n",
            "Epoch 43/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9269 - accuracy: 1.0000 - val_loss: 1.9696 - val_accuracy: 1.0000\n",
            "Epoch 44/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.9244 - val_accuracy: 1.0000\n",
            "Epoch 45/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9434 - accuracy: 1.0000 - val_loss: 1.9134 - val_accuracy: 1.0000\n",
            "Epoch 46/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.9075 - val_accuracy: 1.0000\n",
            "Epoch 47/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9323 - accuracy: 1.0000 - val_loss: 1.8980 - val_accuracy: 1.0000\n",
            "Epoch 48/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9880 - val_accuracy: 1.0000\n",
            "Epoch 49/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9609 - accuracy: 1.0000 - val_loss: 1.9048 - val_accuracy: 1.0000\n",
            "Epoch 50/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8968 - accuracy: 1.0000 - val_loss: 1.9957 - val_accuracy: 1.0000\n",
            "Epoch 51/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9386 - accuracy: 1.0000 - val_loss: 1.8869 - val_accuracy: 1.0000\n",
            "Epoch 52/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.9340 - val_accuracy: 1.0000\n",
            "Epoch 53/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9304 - accuracy: 1.0000 - val_loss: 1.9124 - val_accuracy: 1.0000\n",
            "Epoch 54/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.8997 - val_accuracy: 1.0000\n",
            "Epoch 55/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9277 - accuracy: 1.0000 - val_loss: 1.9433 - val_accuracy: 1.0000\n",
            "Epoch 56/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.9196 - val_accuracy: 1.0000\n",
            "Epoch 57/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9390 - accuracy: 1.0000 - val_loss: 1.9374 - val_accuracy: 1.0000\n",
            "Epoch 58/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9375 - accuracy: 1.0000 - val_loss: 2.1126 - val_accuracy: 1.0000\n",
            "Epoch 59/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9193 - val_accuracy: 1.0000\n",
            "Epoch 60/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9230 - accuracy: 1.0000 - val_loss: 1.9641 - val_accuracy: 1.0000\n",
            "Epoch 61/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9224 - accuracy: 1.0000 - val_loss: 1.9231 - val_accuracy: 1.0000\n",
            "Epoch 62/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9240 - accuracy: 1.0000 - val_loss: 1.9138 - val_accuracy: 1.0000\n",
            "Epoch 63/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9251 - accuracy: 1.0000 - val_loss: 1.9485 - val_accuracy: 1.0000\n",
            "Epoch 64/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9239 - accuracy: 1.0000 - val_loss: 1.9232 - val_accuracy: 1.0000\n",
            "Epoch 65/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9315 - accuracy: 1.0000 - val_loss: 2.3066 - val_accuracy: 1.0000\n",
            "Epoch 66/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9334 - accuracy: 1.0000 - val_loss: 1.9229 - val_accuracy: 1.0000\n",
            "Epoch 67/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9206 - accuracy: 1.0000 - val_loss: 1.9044 - val_accuracy: 1.0000\n",
            "Epoch 68/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9160 - accuracy: 1.0000 - val_loss: 1.8983 - val_accuracy: 1.0000\n",
            "Epoch 69/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9229 - accuracy: 1.0000 - val_loss: 2.0156 - val_accuracy: 1.0000\n",
            "Epoch 70/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9274 - accuracy: 1.0000 - val_loss: 1.9254 - val_accuracy: 1.0000\n",
            "Epoch 71/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9336 - accuracy: 1.0000 - val_loss: 1.9125 - val_accuracy: 1.0000\n",
            "Epoch 72/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9165 - accuracy: 1.0000 - val_loss: 1.9431 - val_accuracy: 1.0000\n",
            "Epoch 73/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9223 - accuracy: 1.0000 - val_loss: 1.9631 - val_accuracy: 1.0000\n",
            "Epoch 74/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9549 - accuracy: 1.0000 - val_loss: 1.9056 - val_accuracy: 1.0000\n",
            "Epoch 75/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9254 - accuracy: 1.0000 - val_loss: 1.9043 - val_accuracy: 1.0000\n",
            "Epoch 76/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9084 - accuracy: 1.0000 - val_loss: 1.8913 - val_accuracy: 1.0000\n",
            "Epoch 77/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9227 - val_accuracy: 1.0000\n",
            "Epoch 78/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9344 - accuracy: 1.0000 - val_loss: 2.0091 - val_accuracy: 1.0000\n",
            "Epoch 79/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9208 - accuracy: 1.0000 - val_loss: 1.9480 - val_accuracy: 1.0000\n",
            "Epoch 80/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9221 - accuracy: 1.0000 - val_loss: 2.0875 - val_accuracy: 1.0000\n",
            "Epoch 81/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9393 - accuracy: 1.0000 - val_loss: 1.9812 - val_accuracy: 1.0000\n",
            "Epoch 82/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9214 - accuracy: 1.0000 - val_loss: 2.0176 - val_accuracy: 1.0000\n",
            "Epoch 83/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9116 - val_accuracy: 1.0000\n",
            "Epoch 84/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9512 - accuracy: 1.0000 - val_loss: 1.9162 - val_accuracy: 1.0000\n",
            "Epoch 85/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9024 - accuracy: 1.0000 - val_loss: 1.9076 - val_accuracy: 1.0000\n",
            "Epoch 86/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9197 - accuracy: 1.0000 - val_loss: 1.8920 - val_accuracy: 1.0000\n",
            "Epoch 87/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9306 - accuracy: 1.0000 - val_loss: 1.9138 - val_accuracy: 1.0000\n",
            "Epoch 88/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.8981 - val_accuracy: 1.0000\n",
            "Epoch 89/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9252 - accuracy: 1.0000 - val_loss: 1.9075 - val_accuracy: 1.0000\n",
            "Epoch 90/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9354 - accuracy: 1.0000 - val_loss: 1.9165 - val_accuracy: 1.0000\n",
            "Epoch 91/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9311 - val_accuracy: 1.0000\n",
            "Epoch 92/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.8957 - val_accuracy: 1.0000\n",
            "Epoch 93/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9329 - accuracy: 1.0000 - val_loss: 1.9408 - val_accuracy: 1.0000\n",
            "Epoch 94/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9121 - val_accuracy: 1.0000\n",
            "Epoch 95/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9297 - accuracy: 1.0000 - val_loss: 1.9252 - val_accuracy: 1.0000\n",
            "Epoch 96/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9219 - accuracy: 1.0000 - val_loss: 1.9066 - val_accuracy: 1.0000\n",
            "Epoch 97/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 1.9302 - val_accuracy: 1.0000\n",
            "Epoch 98/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9146 - accuracy: 1.0000 - val_loss: 1.9805 - val_accuracy: 1.0000\n",
            "Epoch 99/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9217 - accuracy: 1.0000 - val_loss: 1.8953 - val_accuracy: 1.0000\n",
            "Epoch 100/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9321 - accuracy: 1.0000 - val_loss: 1.9997 - val_accuracy: 1.0000\n",
            "Epoch 101/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9190 - accuracy: 1.0000 - val_loss: 2.0992 - val_accuracy: 1.0000\n",
            "Epoch 102/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9308 - accuracy: 1.0000 - val_loss: 1.9181 - val_accuracy: 1.0000\n",
            "Epoch 103/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9368 - accuracy: 1.0000 - val_loss: 1.9876 - val_accuracy: 1.0000\n",
            "Epoch 104/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 2.0287 - val_accuracy: 1.0000\n",
            "Epoch 105/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9300 - accuracy: 1.0000 - val_loss: 1.8978 - val_accuracy: 1.0000\n",
            "Epoch 106/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9200 - accuracy: 1.0000 - val_loss: 1.9138 - val_accuracy: 1.0000\n",
            "Epoch 107/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9484 - accuracy: 1.0000 - val_loss: 1.9791 - val_accuracy: 1.0000\n",
            "Epoch 108/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9028 - accuracy: 1.0000 - val_loss: 1.9016 - val_accuracy: 1.0000\n",
            "Epoch 109/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9313 - accuracy: 1.0000 - val_loss: 1.9640 - val_accuracy: 1.0000\n",
            "Epoch 110/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 2.1578 - val_accuracy: 1.0000\n",
            "Epoch 111/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9268 - accuracy: 1.0000 - val_loss: 1.9003 - val_accuracy: 1.0000\n",
            "Epoch 112/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9257 - accuracy: 1.0000 - val_loss: 1.9060 - val_accuracy: 1.0000\n",
            "Epoch 113/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9273 - accuracy: 1.0000 - val_loss: 1.9527 - val_accuracy: 1.0000\n",
            "Epoch 114/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.9503 - val_accuracy: 1.0000\n",
            "Epoch 115/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9190 - accuracy: 1.0000 - val_loss: 1.9710 - val_accuracy: 1.0000\n",
            "Epoch 116/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9345 - accuracy: 1.0000 - val_loss: 1.9380 - val_accuracy: 1.0000\n",
            "Epoch 117/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 2.0047 - val_accuracy: 1.0000\n",
            "Epoch 118/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9633 - accuracy: 1.0000 - val_loss: 1.9907 - val_accuracy: 1.0000\n",
            "Epoch 119/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9034 - accuracy: 1.0000 - val_loss: 1.8879 - val_accuracy: 1.0000\n",
            "Epoch 120/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9269 - accuracy: 1.0000 - val_loss: 1.9451 - val_accuracy: 1.0000\n",
            "Epoch 121/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.9020 - val_accuracy: 1.0000\n",
            "Epoch 122/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9251 - accuracy: 1.0000 - val_loss: 1.9764 - val_accuracy: 1.0000\n",
            "Epoch 123/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9193 - accuracy: 1.0000 - val_loss: 1.9309 - val_accuracy: 1.0000\n",
            "Epoch 124/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9337 - accuracy: 1.0000 - val_loss: 1.9145 - val_accuracy: 1.0000\n",
            "Epoch 125/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9190 - accuracy: 1.0000 - val_loss: 1.9053 - val_accuracy: 1.0000\n",
            "Epoch 126/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9282 - accuracy: 1.0000 - val_loss: 1.9411 - val_accuracy: 1.0000\n",
            "Epoch 127/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 128/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9251 - accuracy: 1.0000 - val_loss: 1.9279 - val_accuracy: 1.0000\n",
            "Epoch 129/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9154 - accuracy: 1.0000 - val_loss: 1.9678 - val_accuracy: 1.0000\n",
            "Epoch 130/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9394 - accuracy: 1.0000 - val_loss: 1.9514 - val_accuracy: 1.0000\n",
            "Epoch 131/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.9826 - val_accuracy: 1.0000\n",
            "Epoch 132/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9287 - accuracy: 1.0000 - val_loss: 1.9417 - val_accuracy: 1.0000\n",
            "Epoch 133/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9347 - accuracy: 1.0000 - val_loss: 1.8929 - val_accuracy: 1.0000\n",
            "Epoch 134/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9039 - accuracy: 1.0000 - val_loss: 1.9426 - val_accuracy: 1.0000\n",
            "Epoch 135/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9372 - accuracy: 1.0000 - val_loss: 1.9695 - val_accuracy: 1.0000\n",
            "Epoch 136/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9354 - accuracy: 1.0000 - val_loss: 1.9069 - val_accuracy: 1.0000\n",
            "Epoch 137/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9469 - val_accuracy: 1.0000\n",
            "Epoch 138/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.9072 - val_accuracy: 1.0000\n",
            "Epoch 139/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9289 - accuracy: 1.0000 - val_loss: 1.9307 - val_accuracy: 1.0000\n",
            "Epoch 140/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9273 - accuracy: 1.0000 - val_loss: 1.9174 - val_accuracy: 1.0000\n",
            "Epoch 141/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9506 - val_accuracy: 1.0000\n",
            "Epoch 142/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9365 - accuracy: 1.0000 - val_loss: 1.9264 - val_accuracy: 1.0000\n",
            "Epoch 143/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9221 - accuracy: 1.0000 - val_loss: 1.9280 - val_accuracy: 1.0000\n",
            "Epoch 144/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9211 - accuracy: 1.0000 - val_loss: 1.9244 - val_accuracy: 1.0000\n",
            "Epoch 145/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9266 - accuracy: 1.0000 - val_loss: 1.9126 - val_accuracy: 1.0000\n",
            "Epoch 146/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9347 - accuracy: 1.0000 - val_loss: 2.1060 - val_accuracy: 1.0000\n",
            "Epoch 147/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9196 - accuracy: 1.0000 - val_loss: 1.9806 - val_accuracy: 1.0000\n",
            "Epoch 148/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9215 - accuracy: 1.0000 - val_loss: 1.9297 - val_accuracy: 1.0000\n",
            "Epoch 149/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9197 - accuracy: 1.0000 - val_loss: 1.9347 - val_accuracy: 1.0000\n",
            "Epoch 150/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.9491 - val_accuracy: 1.0000\n",
            "Epoch 151/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9378 - accuracy: 1.0000 - val_loss: 1.9458 - val_accuracy: 1.0000\n",
            "Epoch 152/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9196 - accuracy: 1.0000 - val_loss: 1.9304 - val_accuracy: 1.0000\n",
            "Epoch 153/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9438 - accuracy: 1.0000 - val_loss: 1.9363 - val_accuracy: 1.0000\n",
            "Epoch 154/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9034 - accuracy: 1.0000 - val_loss: 1.8951 - val_accuracy: 1.0000\n",
            "Epoch 155/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9199 - accuracy: 1.0000 - val_loss: 1.9440 - val_accuracy: 1.0000\n",
            "Epoch 156/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9211 - accuracy: 1.0000 - val_loss: 1.8963 - val_accuracy: 1.0000\n",
            "Epoch 157/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9352 - accuracy: 1.0000 - val_loss: 1.9332 - val_accuracy: 1.0000\n",
            "Epoch 158/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9042 - val_accuracy: 1.0000\n",
            "Epoch 159/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 1.9840 - val_accuracy: 1.0000\n",
            "Epoch 160/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9264 - accuracy: 1.0000 - val_loss: 1.9405 - val_accuracy: 1.0000\n",
            "Epoch 161/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9206 - accuracy: 1.0000 - val_loss: 2.1207 - val_accuracy: 1.0000\n",
            "Epoch 162/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9295 - accuracy: 1.0000 - val_loss: 2.0213 - val_accuracy: 1.0000\n",
            "Epoch 163/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9254 - val_accuracy: 1.0000\n",
            "Epoch 164/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9351 - accuracy: 1.0000 - val_loss: 1.9051 - val_accuracy: 1.0000\n",
            "Epoch 165/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9179 - accuracy: 1.0000 - val_loss: 1.9166 - val_accuracy: 1.0000\n",
            "Epoch 166/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9188 - accuracy: 1.0000 - val_loss: 2.3290 - val_accuracy: 1.0000\n",
            "Epoch 167/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9526 - accuracy: 1.0000 - val_loss: 1.9226 - val_accuracy: 1.0000\n",
            "Epoch 168/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9036 - accuracy: 1.0000 - val_loss: 1.8990 - val_accuracy: 1.0000\n",
            "Epoch 169/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9343 - accuracy: 1.0000 - val_loss: 1.9030 - val_accuracy: 1.0000\n",
            "Epoch 170/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.9028 - val_accuracy: 1.0000\n",
            "Epoch 171/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9233 - accuracy: 1.0000 - val_loss: 1.8969 - val_accuracy: 1.0000\n",
            "Epoch 172/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9276 - accuracy: 1.0000 - val_loss: 1.9299 - val_accuracy: 1.0000\n",
            "Epoch 173/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.9460 - val_accuracy: 1.0000\n",
            "Epoch 174/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9254 - accuracy: 1.0000 - val_loss: 2.0095 - val_accuracy: 1.0000\n",
            "Epoch 175/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9206 - accuracy: 1.0000 - val_loss: 2.0761 - val_accuracy: 1.0000\n",
            "Epoch 176/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9258 - accuracy: 1.0000 - val_loss: 1.9236 - val_accuracy: 1.0000\n",
            "Epoch 177/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.8991 - val_accuracy: 1.0000\n",
            "Epoch 178/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9274 - accuracy: 1.0000 - val_loss: 1.9164 - val_accuracy: 1.0000\n",
            "Epoch 179/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9145 - val_accuracy: 1.0000\n",
            "Epoch 180/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9253 - accuracy: 1.0000 - val_loss: 1.9006 - val_accuracy: 1.0000\n",
            "Epoch 181/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9258 - accuracy: 1.0000 - val_loss: 1.9279 - val_accuracy: 1.0000\n",
            "Epoch 182/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9343 - accuracy: 1.0000 - val_loss: 1.9191 - val_accuracy: 1.0000\n",
            "Epoch 183/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9051 - accuracy: 1.0000 - val_loss: 1.9156 - val_accuracy: 1.0000\n",
            "Epoch 184/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9306 - accuracy: 1.0000 - val_loss: 1.9167 - val_accuracy: 1.0000\n",
            "Epoch 185/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9232 - accuracy: 1.0000 - val_loss: 2.0224 - val_accuracy: 1.0000\n",
            "Epoch 186/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9266 - accuracy: 1.0000 - val_loss: 1.9153 - val_accuracy: 1.0000\n",
            "Epoch 187/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9574 - accuracy: 1.0000 - val_loss: 1.9214 - val_accuracy: 1.0000\n",
            "Epoch 188/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8997 - accuracy: 1.0000 - val_loss: 1.9015 - val_accuracy: 1.0000\n",
            "Epoch 189/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9227 - accuracy: 1.0000 - val_loss: 1.9808 - val_accuracy: 1.0000\n",
            "Epoch 190/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 1.8939 - val_accuracy: 1.0000\n",
            "Epoch 191/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9170 - accuracy: 1.0000 - val_loss: 1.9195 - val_accuracy: 1.0000\n",
            "Epoch 192/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9278 - accuracy: 1.0000 - val_loss: 1.8931 - val_accuracy: 1.0000\n",
            "Epoch 193/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9234 - accuracy: 1.0000 - val_loss: 2.1288 - val_accuracy: 1.0000\n",
            "Epoch 194/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9279 - accuracy: 1.0000 - val_loss: 1.9064 - val_accuracy: 1.0000\n",
            "Epoch 195/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9205 - accuracy: 1.0000 - val_loss: 1.9151 - val_accuracy: 1.0000\n",
            "Epoch 196/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9189 - accuracy: 1.0000 - val_loss: 1.9455 - val_accuracy: 1.0000\n",
            "Epoch 197/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9066 - val_accuracy: 1.0000\n",
            "Epoch 198/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9290 - accuracy: 1.0000 - val_loss: 2.0677 - val_accuracy: 1.0000\n",
            "Epoch 199/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9258 - accuracy: 1.0000 - val_loss: 1.9286 - val_accuracy: 1.0000\n",
            "Epoch 200/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9160 - accuracy: 1.0000 - val_loss: 2.0128 - val_accuracy: 1.0000\n",
            "Epoch 201/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9215 - accuracy: 1.0000 - val_loss: 1.8979 - val_accuracy: 1.0000\n",
            "Epoch 202/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9242 - accuracy: 1.0000 - val_loss: 1.9037 - val_accuracy: 1.0000\n",
            "Epoch 203/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.9814 - val_accuracy: 1.0000\n",
            "Epoch 204/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9469 - accuracy: 1.0000 - val_loss: 1.8977 - val_accuracy: 1.0000\n",
            "Epoch 205/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.9545 - val_accuracy: 1.0000\n",
            "Epoch 206/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9200 - accuracy: 1.0000 - val_loss: 2.0902 - val_accuracy: 1.0000\n",
            "Epoch 207/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9275 - accuracy: 1.0000 - val_loss: 1.9302 - val_accuracy: 1.0000\n",
            "Epoch 208/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9225 - accuracy: 1.0000 - val_loss: 2.0350 - val_accuracy: 1.0000\n",
            "Epoch 209/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9169 - accuracy: 1.0000 - val_loss: 1.9478 - val_accuracy: 1.0000\n",
            "Epoch 210/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9300 - accuracy: 1.0000 - val_loss: 2.3534 - val_accuracy: 1.0000\n",
            "Epoch 211/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9263 - accuracy: 1.0000 - val_loss: 2.0124 - val_accuracy: 1.0000\n",
            "Epoch 212/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9389 - accuracy: 1.0000 - val_loss: 1.9227 - val_accuracy: 1.0000\n",
            "Epoch 213/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.8965 - val_accuracy: 1.0000\n",
            "Epoch 214/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9080 - accuracy: 1.0000 - val_loss: 1.9197 - val_accuracy: 1.0000\n",
            "Epoch 215/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9271 - accuracy: 1.0000 - val_loss: 2.0462 - val_accuracy: 1.0000\n",
            "Epoch 216/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.9978 - val_accuracy: 1.0000\n",
            "Epoch 217/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9104 - val_accuracy: 1.0000\n",
            "Epoch 218/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9254 - accuracy: 1.0000 - val_loss: 1.8987 - val_accuracy: 1.0000\n",
            "Epoch 219/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9391 - accuracy: 1.0000 - val_loss: 1.9036 - val_accuracy: 1.0000\n",
            "Epoch 220/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9378 - accuracy: 1.0000 - val_loss: 1.9129 - val_accuracy: 1.0000\n",
            "Epoch 221/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9034 - accuracy: 1.0000 - val_loss: 1.9117 - val_accuracy: 1.0000\n",
            "Epoch 222/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9201 - accuracy: 1.0000 - val_loss: 1.9353 - val_accuracy: 1.0000\n",
            "Epoch 223/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9395 - accuracy: 1.0000 - val_loss: 1.9015 - val_accuracy: 1.0000\n",
            "Epoch 224/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9030 - val_accuracy: 1.0000\n",
            "Epoch 225/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9316 - accuracy: 1.0000 - val_loss: 1.9728 - val_accuracy: 1.0000\n",
            "Epoch 226/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9242 - accuracy: 1.0000 - val_loss: 1.9155 - val_accuracy: 1.0000\n",
            "Epoch 227/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9123 - accuracy: 1.0000 - val_loss: 2.0922 - val_accuracy: 1.0000\n",
            "Epoch 228/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9402 - accuracy: 1.0000 - val_loss: 2.2056 - val_accuracy: 1.0000\n",
            "Epoch 229/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9308 - accuracy: 1.0000 - val_loss: 2.0522 - val_accuracy: 1.0000\n",
            "Epoch 230/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9439 - accuracy: 1.0000 - val_loss: 1.9313 - val_accuracy: 1.0000\n",
            "Epoch 231/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9231 - accuracy: 1.0000 - val_loss: 1.8996 - val_accuracy: 1.0000\n",
            "Epoch 232/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9089 - accuracy: 1.0000 - val_loss: 1.8988 - val_accuracy: 1.0000\n",
            "Epoch 233/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9309 - accuracy: 1.0000 - val_loss: 1.9172 - val_accuracy: 1.0000\n",
            "Epoch 234/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9133 - accuracy: 1.0000 - val_loss: 2.0998 - val_accuracy: 1.0000\n",
            "Epoch 235/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9297 - accuracy: 1.0000 - val_loss: 1.9735 - val_accuracy: 1.0000\n",
            "Epoch 236/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9286 - accuracy: 1.0000 - val_loss: 1.8943 - val_accuracy: 1.0000\n",
            "Epoch 237/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9283 - accuracy: 1.0000 - val_loss: 1.9001 - val_accuracy: 1.0000\n",
            "Epoch 238/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9047 - accuracy: 1.0000 - val_loss: 1.9488 - val_accuracy: 1.0000\n",
            "Epoch 239/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9346 - accuracy: 1.0000 - val_loss: 1.9642 - val_accuracy: 1.0000\n",
            "Epoch 240/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9418 - accuracy: 1.0000 - val_loss: 1.9594 - val_accuracy: 1.0000\n",
            "Epoch 241/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9006 - accuracy: 1.0000 - val_loss: 1.9101 - val_accuracy: 1.0000\n",
            "Epoch 242/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9320 - accuracy: 1.0000 - val_loss: 1.9270 - val_accuracy: 1.0000\n",
            "Epoch 243/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 1.9061 - val_accuracy: 1.0000\n",
            "Epoch 244/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9215 - accuracy: 1.0000 - val_loss: 2.0248 - val_accuracy: 1.0000\n",
            "Epoch 245/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9334 - accuracy: 1.0000 - val_loss: 1.9097 - val_accuracy: 1.0000\n",
            "Epoch 246/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.9757 - val_accuracy: 1.0000\n",
            "Epoch 247/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9225 - accuracy: 1.0000 - val_loss: 2.1181 - val_accuracy: 1.0000\n",
            "Epoch 248/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 2.0033 - val_accuracy: 1.0000\n",
            "Epoch 249/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9280 - accuracy: 1.0000 - val_loss: 1.9110 - val_accuracy: 1.0000\n",
            "Epoch 250/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.9022 - val_accuracy: 1.0000\n",
            "Epoch 251/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.9081 - val_accuracy: 1.0000\n",
            "Epoch 252/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9076 - val_accuracy: 1.0000\n",
            "Epoch 253/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9433 - accuracy: 1.0000 - val_loss: 1.9310 - val_accuracy: 1.0000\n",
            "Epoch 254/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9196 - accuracy: 1.0000 - val_loss: 1.8951 - val_accuracy: 1.0000\n",
            "Epoch 255/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.9644 - val_accuracy: 1.0000\n",
            "Epoch 256/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9229 - accuracy: 1.0000 - val_loss: 1.9393 - val_accuracy: 1.0000\n",
            "Epoch 257/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9384 - accuracy: 1.0000 - val_loss: 2.0043 - val_accuracy: 1.0000\n",
            "Epoch 258/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9176 - accuracy: 1.0000 - val_loss: 1.9024 - val_accuracy: 1.0000\n",
            "Epoch 259/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9208 - accuracy: 1.0000 - val_loss: 1.9020 - val_accuracy: 1.0000\n",
            "Epoch 260/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.8894 - val_accuracy: 1.0000\n",
            "Epoch 261/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9670 - val_accuracy: 1.0000\n",
            "Epoch 262/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9186 - val_accuracy: 1.0000\n",
            "Epoch 263/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9311 - accuracy: 1.0000 - val_loss: 2.0513 - val_accuracy: 1.0000\n",
            "Epoch 264/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9008 - val_accuracy: 1.0000\n",
            "Epoch 265/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9212 - accuracy: 1.0000 - val_loss: 1.9288 - val_accuracy: 1.0000\n",
            "Epoch 266/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9393 - accuracy: 1.0000 - val_loss: 1.9013 - val_accuracy: 1.0000\n",
            "Epoch 267/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 2.0003 - val_accuracy: 1.0000\n",
            "Epoch 268/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9262 - accuracy: 1.0000 - val_loss: 1.9081 - val_accuracy: 1.0000\n",
            "Epoch 269/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 2.0556 - val_accuracy: 1.0000\n",
            "Epoch 270/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9110 - accuracy: 1.0000 - val_loss: 1.9204 - val_accuracy: 1.0000\n",
            "Epoch 271/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9297 - accuracy: 1.0000 - val_loss: 2.1309 - val_accuracy: 1.0000\n",
            "Epoch 272/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9259 - accuracy: 1.0000 - val_loss: 1.8978 - val_accuracy: 1.0000\n",
            "Epoch 273/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9244 - accuracy: 1.0000 - val_loss: 1.9683 - val_accuracy: 1.0000\n",
            "Epoch 274/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9205 - accuracy: 1.0000 - val_loss: 1.8918 - val_accuracy: 1.0000\n",
            "Epoch 275/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 2.0450 - val_accuracy: 1.0000\n",
            "Epoch 276/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9178 - accuracy: 1.0000 - val_loss: 1.9048 - val_accuracy: 1.0000\n",
            "Epoch 277/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9595 - accuracy: 1.0000 - val_loss: 1.9283 - val_accuracy: 1.0000\n",
            "Epoch 278/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9027 - accuracy: 1.0000 - val_loss: 1.9238 - val_accuracy: 1.0000\n",
            "Epoch 279/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9095 - accuracy: 1.0000 - val_loss: 1.9033 - val_accuracy: 1.0000\n",
            "Epoch 280/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9205 - accuracy: 1.0000 - val_loss: 1.8991 - val_accuracy: 1.0000\n",
            "Epoch 281/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9303 - accuracy: 1.0000 - val_loss: 1.9086 - val_accuracy: 1.0000\n",
            "Epoch 282/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9274 - accuracy: 1.0000 - val_loss: 1.9043 - val_accuracy: 1.0000\n",
            "Epoch 283/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.9539 - val_accuracy: 1.0000\n",
            "Epoch 284/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9459 - accuracy: 1.0000 - val_loss: 1.8941 - val_accuracy: 1.0000\n",
            "Epoch 285/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.8996 - accuracy: 1.0000 - val_loss: 1.9352 - val_accuracy: 1.0000\n",
            "Epoch 286/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9176 - accuracy: 1.0000 - val_loss: 1.9881 - val_accuracy: 1.0000\n",
            "Epoch 287/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9310 - accuracy: 1.0000 - val_loss: 1.9754 - val_accuracy: 1.0000\n",
            "Epoch 288/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 1.8958 - val_accuracy: 1.0000\n",
            "Epoch 289/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 2.1332 - val_accuracy: 1.0000\n",
            "Epoch 290/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9552 - accuracy: 1.0000 - val_loss: 1.9169 - val_accuracy: 1.0000\n",
            "Epoch 291/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9065 - accuracy: 1.0000 - val_loss: 2.1629 - val_accuracy: 1.0000\n",
            "Epoch 292/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9203 - accuracy: 1.0000 - val_loss: 1.9277 - val_accuracy: 1.0000\n",
            "Epoch 293/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9453 - accuracy: 1.0000 - val_loss: 1.8860 - val_accuracy: 1.0000\n",
            "Epoch 294/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9044 - accuracy: 1.0000 - val_loss: 1.9975 - val_accuracy: 1.0000\n",
            "Epoch 295/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9364 - accuracy: 1.0000 - val_loss: 2.0024 - val_accuracy: 1.0000\n",
            "Epoch 296/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9108 - val_accuracy: 1.0000\n",
            "Epoch 297/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.8924 - val_accuracy: 1.0000\n",
            "Epoch 298/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9360 - accuracy: 1.0000 - val_loss: 1.9027 - val_accuracy: 1.0000\n",
            "Epoch 299/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 1.9248 - val_accuracy: 1.0000\n",
            "Epoch 300/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.9042 - val_accuracy: 1.0000\n",
            "Epoch 301/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 1.9482 - val_accuracy: 1.0000\n",
            "Epoch 302/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9182 - accuracy: 1.0000 - val_loss: 1.9519 - val_accuracy: 1.0000\n",
            "Epoch 303/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9568 - accuracy: 1.0000 - val_loss: 1.9016 - val_accuracy: 1.0000\n",
            "Epoch 304/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.8978 - accuracy: 1.0000 - val_loss: 1.9028 - val_accuracy: 1.0000\n",
            "Epoch 305/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9264 - accuracy: 1.0000 - val_loss: 1.9222 - val_accuracy: 1.0000\n",
            "Epoch 306/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9114 - accuracy: 1.0000 - val_loss: 1.9079 - val_accuracy: 1.0000\n",
            "Epoch 307/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9288 - accuracy: 1.0000 - val_loss: 1.9269 - val_accuracy: 1.0000\n",
            "Epoch 308/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9058 - accuracy: 1.0000 - val_loss: 1.9683 - val_accuracy: 1.0000\n",
            "Epoch 309/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9272 - accuracy: 1.0000 - val_loss: 1.9183 - val_accuracy: 1.0000\n",
            "Epoch 310/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 2.4268 - val_accuracy: 1.0000\n",
            "Epoch 311/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9295 - accuracy: 1.0000 - val_loss: 1.9157 - val_accuracy: 1.0000\n",
            "Epoch 312/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.8937 - val_accuracy: 1.0000\n",
            "Epoch 313/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9284 - accuracy: 1.0000 - val_loss: 1.9311 - val_accuracy: 1.0000\n",
            "Epoch 314/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9154 - accuracy: 1.0000 - val_loss: 1.9187 - val_accuracy: 1.0000\n",
            "Epoch 315/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9352 - accuracy: 1.0000 - val_loss: 1.9351 - val_accuracy: 1.0000\n",
            "Epoch 316/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.9904 - val_accuracy: 1.0000\n",
            "Epoch 317/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9248 - accuracy: 1.0000 - val_loss: 1.9518 - val_accuracy: 1.0000\n",
            "Epoch 318/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9284 - accuracy: 1.0000 - val_loss: 1.9376 - val_accuracy: 1.0000\n",
            "Epoch 319/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.9923 - val_accuracy: 1.0000\n",
            "Epoch 320/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9224 - accuracy: 1.0000 - val_loss: 1.8881 - val_accuracy: 1.0000\n",
            "Epoch 321/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9199 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 1.0000\n",
            "Epoch 322/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9212 - accuracy: 1.0000 - val_loss: 1.9420 - val_accuracy: 1.0000\n",
            "Epoch 323/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9389 - accuracy: 1.0000 - val_loss: 1.9860 - val_accuracy: 1.0000\n",
            "Epoch 324/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9044 - accuracy: 1.0000 - val_loss: 1.9031 - val_accuracy: 1.0000\n",
            "Epoch 325/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9294 - accuracy: 1.0000 - val_loss: 1.9314 - val_accuracy: 1.0000\n",
            "Epoch 326/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9086 - accuracy: 1.0000 - val_loss: 1.8934 - val_accuracy: 1.0000\n",
            "Epoch 327/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9222 - accuracy: 1.0000 - val_loss: 1.9420 - val_accuracy: 1.0000\n",
            "Epoch 328/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9175 - accuracy: 1.0000 - val_loss: 2.0123 - val_accuracy: 1.0000\n",
            "Epoch 329/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9272 - accuracy: 1.0000 - val_loss: 2.0306 - val_accuracy: 1.0000\n",
            "Epoch 330/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 1.9624 - val_accuracy: 1.0000\n",
            "Epoch 331/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.9877 - val_accuracy: 1.0000\n",
            "Epoch 332/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9271 - accuracy: 1.0000 - val_loss: 1.9059 - val_accuracy: 1.0000\n",
            "Epoch 333/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9345 - accuracy: 1.0000 - val_loss: 1.9677 - val_accuracy: 1.0000\n",
            "Epoch 334/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 1.9522 - val_accuracy: 1.0000\n",
            "Epoch 335/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9259 - accuracy: 1.0000 - val_loss: 1.8972 - val_accuracy: 1.0000\n",
            "Epoch 336/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9163 - val_accuracy: 1.0000\n",
            "Epoch 337/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.8971 - val_accuracy: 1.0000\n",
            "Epoch 338/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9263 - accuracy: 1.0000 - val_loss: 1.9856 - val_accuracy: 1.0000\n",
            "Epoch 339/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9217 - accuracy: 1.0000 - val_loss: 1.8967 - val_accuracy: 1.0000\n",
            "Epoch 340/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9308 - accuracy: 1.0000 - val_loss: 1.9080 - val_accuracy: 1.0000\n",
            "Epoch 341/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.9768 - val_accuracy: 1.0000\n",
            "Epoch 342/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9307 - accuracy: 1.0000 - val_loss: 1.9085 - val_accuracy: 1.0000\n",
            "Epoch 343/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 1.9284 - val_accuracy: 1.0000\n",
            "Epoch 344/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.8942 - val_accuracy: 1.0000\n",
            "Epoch 345/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9324 - accuracy: 1.0000 - val_loss: 1.9383 - val_accuracy: 1.0000\n",
            "Epoch 346/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9221 - accuracy: 1.0000 - val_loss: 1.9808 - val_accuracy: 1.0000\n",
            "Epoch 347/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9195 - accuracy: 1.0000 - val_loss: 1.9140 - val_accuracy: 1.0000\n",
            "Epoch 348/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.9994 - val_accuracy: 1.0000\n",
            "Epoch 349/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.9597 - val_accuracy: 1.0000\n",
            "Epoch 350/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9281 - accuracy: 1.0000 - val_loss: 1.9167 - val_accuracy: 1.0000\n",
            "Epoch 351/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9280 - accuracy: 1.0000 - val_loss: 1.9353 - val_accuracy: 1.0000\n",
            "Epoch 352/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 2.0511 - val_accuracy: 1.0000\n",
            "Epoch 353/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9357 - accuracy: 1.0000 - val_loss: 1.9491 - val_accuracy: 1.0000\n",
            "Epoch 354/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9771 - val_accuracy: 1.0000\n",
            "Epoch 355/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9114 - accuracy: 1.0000 - val_loss: 1.9153 - val_accuracy: 1.0000\n",
            "Epoch 356/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 1.9591 - val_accuracy: 1.0000\n",
            "Epoch 357/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9260 - accuracy: 1.0000 - val_loss: 1.9009 - val_accuracy: 1.0000\n",
            "Epoch 358/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9282 - accuracy: 1.0000 - val_loss: 1.9538 - val_accuracy: 1.0000\n",
            "Epoch 359/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9300 - accuracy: 1.0000 - val_loss: 1.9632 - val_accuracy: 1.0000\n",
            "Epoch 360/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9083 - accuracy: 1.0000 - val_loss: 1.9250 - val_accuracy: 1.0000\n",
            "Epoch 361/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9227 - accuracy: 1.0000 - val_loss: 1.8959 - val_accuracy: 1.0000\n",
            "Epoch 362/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9240 - accuracy: 1.0000 - val_loss: 2.0279 - val_accuracy: 1.0000\n",
            "Epoch 363/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9522 - val_accuracy: 1.0000\n",
            "Epoch 364/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.8983 - val_accuracy: 1.0000\n",
            "Epoch 365/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9387 - accuracy: 1.0000 - val_loss: 1.9026 - val_accuracy: 1.0000\n",
            "Epoch 366/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9366 - val_accuracy: 1.0000\n",
            "Epoch 367/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 1.9103 - val_accuracy: 1.0000\n",
            "Epoch 368/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 1.9141 - val_accuracy: 1.0000\n",
            "Epoch 369/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9257 - accuracy: 1.0000 - val_loss: 1.9196 - val_accuracy: 1.0000\n",
            "Epoch 370/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9225 - accuracy: 1.0000 - val_loss: 1.9604 - val_accuracy: 1.0000\n",
            "Epoch 371/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.9030 - val_accuracy: 1.0000\n",
            "Epoch 372/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9260 - accuracy: 1.0000 - val_loss: 1.8970 - val_accuracy: 1.0000\n",
            "Epoch 373/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9067 - accuracy: 1.0000 - val_loss: 1.9047 - val_accuracy: 1.0000\n",
            "Epoch 374/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9214 - accuracy: 1.0000 - val_loss: 1.9671 - val_accuracy: 1.0000\n",
            "Epoch 375/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9195 - accuracy: 1.0000 - val_loss: 1.9204 - val_accuracy: 1.0000\n",
            "Epoch 376/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9269 - accuracy: 1.0000 - val_loss: 1.9133 - val_accuracy: 1.0000\n",
            "Epoch 377/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9124 - val_accuracy: 1.0000\n",
            "Epoch 378/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9217 - accuracy: 1.0000 - val_loss: 1.9328 - val_accuracy: 1.0000\n",
            "Epoch 379/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9262 - accuracy: 1.0000 - val_loss: 1.9140 - val_accuracy: 1.0000\n",
            "Epoch 380/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9193 - accuracy: 1.0000 - val_loss: 1.9093 - val_accuracy: 1.0000\n",
            "Epoch 381/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9180 - accuracy: 1.0000 - val_loss: 1.9016 - val_accuracy: 1.0000\n",
            "Epoch 382/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 1.9116 - val_accuracy: 1.0000\n",
            "Epoch 383/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9382 - accuracy: 1.0000 - val_loss: 2.1521 - val_accuracy: 1.0000\n",
            "Epoch 384/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.9240 - val_accuracy: 1.0000\n",
            "Epoch 385/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9179 - accuracy: 1.0000 - val_loss: 2.0147 - val_accuracy: 1.0000\n",
            "Epoch 386/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9260 - accuracy: 1.0000 - val_loss: 1.8919 - val_accuracy: 1.0000\n",
            "Epoch 387/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9296 - accuracy: 1.0000 - val_loss: 1.9280 - val_accuracy: 1.0000\n",
            "Epoch 388/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9084 - accuracy: 1.0000 - val_loss: 1.9473 - val_accuracy: 1.0000\n",
            "Epoch 389/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9294 - accuracy: 1.0000 - val_loss: 1.8885 - val_accuracy: 1.0000\n",
            "Epoch 390/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9108 - accuracy: 1.0000 - val_loss: 1.9029 - val_accuracy: 1.0000\n",
            "Epoch 391/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9169 - accuracy: 1.0000 - val_loss: 1.9370 - val_accuracy: 1.0000\n",
            "Epoch 392/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9273 - accuracy: 1.0000 - val_loss: 1.9228 - val_accuracy: 1.0000\n",
            "Epoch 393/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9068 - val_accuracy: 1.0000\n",
            "Epoch 394/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9290 - accuracy: 1.0000 - val_loss: 1.9458 - val_accuracy: 1.0000\n",
            "Epoch 395/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9203 - accuracy: 1.0000 - val_loss: 1.9488 - val_accuracy: 1.0000\n",
            "Epoch 396/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 2.0070 - val_accuracy: 1.0000\n",
            "Epoch 397/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9293 - accuracy: 1.0000 - val_loss: 1.8920 - val_accuracy: 1.0000\n",
            "Epoch 398/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 1.9007 - val_accuracy: 1.0000\n",
            "Epoch 399/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9224 - accuracy: 1.0000 - val_loss: 1.9576 - val_accuracy: 1.0000\n",
            "Epoch 400/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.9140 - val_accuracy: 1.0000\n",
            "Epoch 401/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9182 - accuracy: 1.0000 - val_loss: 1.9030 - val_accuracy: 1.0000\n",
            "Epoch 402/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9246 - accuracy: 1.0000 - val_loss: 1.9616 - val_accuracy: 1.0000\n",
            "Epoch 403/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9196 - accuracy: 1.0000 - val_loss: 1.9883 - val_accuracy: 1.0000\n",
            "Epoch 404/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9199 - accuracy: 1.0000 - val_loss: 1.9427 - val_accuracy: 1.0000\n",
            "Epoch 405/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.9161 - val_accuracy: 1.0000\n",
            "Epoch 406/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9359 - accuracy: 1.0000 - val_loss: 1.9335 - val_accuracy: 1.0000\n",
            "Epoch 407/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.9361 - val_accuracy: 1.0000\n",
            "Epoch 408/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 2.0894 - val_accuracy: 1.0000\n",
            "Epoch 409/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9218 - accuracy: 1.0000 - val_loss: 1.9046 - val_accuracy: 1.0000\n",
            "Epoch 410/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9309 - accuracy: 1.0000 - val_loss: 1.9036 - val_accuracy: 1.0000\n",
            "Epoch 411/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.9261 - val_accuracy: 1.0000\n",
            "Epoch 412/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9268 - accuracy: 1.0000 - val_loss: 1.9113 - val_accuracy: 1.0000\n",
            "Epoch 413/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9159 - accuracy: 1.0000 - val_loss: 1.9045 - val_accuracy: 1.0000\n",
            "Epoch 414/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9211 - accuracy: 1.0000 - val_loss: 2.1527 - val_accuracy: 1.0000\n",
            "Epoch 415/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9229 - accuracy: 1.0000 - val_loss: 1.9264 - val_accuracy: 1.0000\n",
            "Epoch 416/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9211 - accuracy: 1.0000 - val_loss: 1.9143 - val_accuracy: 1.0000\n",
            "Epoch 417/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9404 - accuracy: 1.0000 - val_loss: 1.9560 - val_accuracy: 1.0000\n",
            "Epoch 418/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9024 - accuracy: 1.0000 - val_loss: 1.9790 - val_accuracy: 1.0000\n",
            "Epoch 419/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9218 - accuracy: 1.0000 - val_loss: 1.9152 - val_accuracy: 1.0000\n",
            "Epoch 420/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9222 - accuracy: 1.0000 - val_loss: 1.9020 - val_accuracy: 1.0000\n",
            "Epoch 421/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9162 - accuracy: 1.0000 - val_loss: 1.9086 - val_accuracy: 1.0000\n",
            "Epoch 422/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9489 - val_accuracy: 1.0000\n",
            "Epoch 423/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9169 - accuracy: 1.0000 - val_loss: 1.9099 - val_accuracy: 1.0000\n",
            "Epoch 424/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9152 - val_accuracy: 1.0000\n",
            "Epoch 425/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.9313 - val_accuracy: 1.0000\n",
            "Epoch 426/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.8976 - val_accuracy: 1.0000\n",
            "Epoch 427/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9299 - accuracy: 1.0000 - val_loss: 1.9058 - val_accuracy: 1.0000\n",
            "Epoch 428/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 4.6007 - val_accuracy: 1.0000\n",
            "Epoch 429/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.1837 - accuracy: 1.0000 - val_loss: 2.0616 - val_accuracy: 1.0000\n",
            "Epoch 430/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 2.0659 - accuracy: 1.0000 - val_loss: 2.0818 - val_accuracy: 1.0000\n",
            "Epoch 431/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0540 - accuracy: 1.0000 - val_loss: 1.9773 - val_accuracy: 1.0000\n",
            "Epoch 432/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9534 - accuracy: 1.0000 - val_loss: 1.9065 - val_accuracy: 1.0000\n",
            "Epoch 433/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9170 - accuracy: 1.0000 - val_loss: 1.9769 - val_accuracy: 1.0000\n",
            "Epoch 434/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.8965 - val_accuracy: 1.0000\n",
            "Epoch 435/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 2.0286 - val_accuracy: 1.0000\n",
            "Epoch 436/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9356 - accuracy: 1.0000 - val_loss: 1.9943 - val_accuracy: 1.0000\n",
            "Epoch 437/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9123 - accuracy: 1.0000 - val_loss: 1.9585 - val_accuracy: 1.0000\n",
            "Epoch 438/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9272 - accuracy: 1.0000 - val_loss: 1.9226 - val_accuracy: 1.0000\n",
            "Epoch 439/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9272 - accuracy: 1.0000 - val_loss: 1.9015 - val_accuracy: 1.0000\n",
            "Epoch 440/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 2.0036 - val_accuracy: 1.0000\n",
            "Epoch 441/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9146 - accuracy: 1.0000 - val_loss: 1.9823 - val_accuracy: 1.0000\n",
            "Epoch 442/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9204 - accuracy: 1.0000 - val_loss: 1.9039 - val_accuracy: 1.0000\n",
            "Epoch 443/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9233 - accuracy: 1.0000 - val_loss: 1.9912 - val_accuracy: 1.0000\n",
            "Epoch 444/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.8952 - val_accuracy: 1.0000\n",
            "Epoch 445/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9129 - accuracy: 1.0000 - val_loss: 1.9434 - val_accuracy: 1.0000\n",
            "Epoch 446/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9280 - accuracy: 1.0000 - val_loss: 1.9533 - val_accuracy: 1.0000\n",
            "Epoch 447/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9237 - accuracy: 1.0000 - val_loss: 1.9216 - val_accuracy: 1.0000\n",
            "Epoch 448/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.9065 - val_accuracy: 1.0000\n",
            "Epoch 449/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9199 - accuracy: 1.0000 - val_loss: 1.9781 - val_accuracy: 1.0000\n",
            "Epoch 450/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9194 - accuracy: 1.0000 - val_loss: 1.9796 - val_accuracy: 1.0000\n",
            "Epoch 451/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9219 - accuracy: 1.0000 - val_loss: 1.9238 - val_accuracy: 1.0000\n",
            "Epoch 452/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9289 - accuracy: 1.0000 - val_loss: 1.8841 - val_accuracy: 1.0000\n",
            "Epoch 453/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9026 - accuracy: 1.0000 - val_loss: 1.8902 - val_accuracy: 1.0000\n",
            "Epoch 454/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9204 - accuracy: 1.0000 - val_loss: 1.9909 - val_accuracy: 1.0000\n",
            "Epoch 455/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9340 - val_accuracy: 1.0000\n",
            "Epoch 456/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.8996 - val_accuracy: 1.0000\n",
            "Epoch 457/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9252 - accuracy: 1.0000 - val_loss: 1.9216 - val_accuracy: 1.0000\n",
            "Epoch 458/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.9123 - val_accuracy: 1.0000\n",
            "Epoch 459/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9252 - accuracy: 1.0000 - val_loss: 1.9540 - val_accuracy: 1.0000\n",
            "Epoch 460/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 1.9271 - val_accuracy: 1.0000\n",
            "Epoch 461/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9187 - accuracy: 1.0000 - val_loss: 1.9719 - val_accuracy: 1.0000\n",
            "Epoch 462/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9173 - accuracy: 1.0000 - val_loss: 1.9238 - val_accuracy: 1.0000\n",
            "Epoch 463/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9302 - accuracy: 1.0000 - val_loss: 1.8909 - val_accuracy: 1.0000\n",
            "Epoch 464/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9141 - accuracy: 1.0000 - val_loss: 1.9635 - val_accuracy: 1.0000\n",
            "Epoch 465/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9230 - accuracy: 1.0000 - val_loss: 1.9765 - val_accuracy: 1.0000\n",
            "Epoch 466/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.8950 - val_accuracy: 1.0000\n",
            "Epoch 467/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 2.0323 - val_accuracy: 1.0000\n",
            "Epoch 468/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.9012 - val_accuracy: 1.0000\n",
            "Epoch 469/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9280 - accuracy: 1.0000 - val_loss: 1.9156 - val_accuracy: 1.0000\n",
            "Epoch 470/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9266 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 1.0000\n",
            "Epoch 471/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.9274 - val_accuracy: 1.0000\n",
            "Epoch 472/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 2.1020 - val_accuracy: 1.0000\n",
            "Epoch 473/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.8983 - val_accuracy: 1.0000\n",
            "Epoch 474/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9281 - accuracy: 1.0000 - val_loss: 1.8866 - val_accuracy: 1.0000\n",
            "Epoch 475/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 2.0487 - val_accuracy: 1.0000\n",
            "Epoch 476/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9087 - accuracy: 1.0000 - val_loss: 2.1067 - val_accuracy: 1.0000\n",
            "Epoch 477/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9297 - accuracy: 1.0000 - val_loss: 1.9467 - val_accuracy: 1.0000\n",
            "Epoch 478/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.8969 - val_accuracy: 1.0000\n",
            "Epoch 479/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9427 - accuracy: 1.0000 - val_loss: 1.8948 - val_accuracy: 1.0000\n",
            "Epoch 480/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9070 - accuracy: 1.0000 - val_loss: 1.8931 - val_accuracy: 1.0000\n",
            "Epoch 481/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 1.9372 - val_accuracy: 1.0000\n",
            "Epoch 482/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9262 - accuracy: 1.0000 - val_loss: 1.9011 - val_accuracy: 1.0000\n",
            "Epoch 483/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9133 - accuracy: 1.0000 - val_loss: 1.9322 - val_accuracy: 1.0000\n",
            "Epoch 484/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9208 - accuracy: 1.0000 - val_loss: 1.9611 - val_accuracy: 1.0000\n",
            "Epoch 485/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.9188 - val_accuracy: 1.0000\n",
            "Epoch 486/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.9016 - val_accuracy: 1.0000\n",
            "Epoch 487/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 1.9117 - val_accuracy: 1.0000\n",
            "Epoch 488/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9280 - accuracy: 1.0000 - val_loss: 1.9094 - val_accuracy: 1.0000\n",
            "Epoch 489/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9055 - accuracy: 1.0000 - val_loss: 1.9159 - val_accuracy: 1.0000\n",
            "Epoch 490/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9279 - accuracy: 1.0000 - val_loss: 1.9844 - val_accuracy: 1.0000\n",
            "Epoch 491/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9189 - accuracy: 1.0000 - val_loss: 1.9227 - val_accuracy: 1.0000\n",
            "Epoch 492/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9254 - accuracy: 1.0000 - val_loss: 1.9097 - val_accuracy: 1.0000\n",
            "Epoch 493/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 1.9603 - val_accuracy: 1.0000\n",
            "Epoch 494/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9258 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 495/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 1.9359 - val_accuracy: 1.0000\n",
            "Epoch 496/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9221 - accuracy: 1.0000 - val_loss: 1.9144 - val_accuracy: 1.0000\n",
            "Epoch 497/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 1.9193 - val_accuracy: 1.0000\n",
            "Epoch 498/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 2.0207 - val_accuracy: 1.0000\n",
            "Epoch 499/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9179 - accuracy: 1.0000 - val_loss: 1.9021 - val_accuracy: 1.0000\n",
            "Epoch 500/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 2.0287 - val_accuracy: 1.0000\n",
            "Epoch 501/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9218 - accuracy: 1.0000 - val_loss: 1.9165 - val_accuracy: 1.0000\n",
            "Epoch 502/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9039 - val_accuracy: 1.0000\n",
            "Epoch 503/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 1.9107 - val_accuracy: 1.0000\n",
            "Epoch 504/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9150 - accuracy: 1.0000 - val_loss: 1.8862 - val_accuracy: 1.0000\n",
            "Epoch 505/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9270 - accuracy: 1.0000 - val_loss: 1.9988 - val_accuracy: 1.0000\n",
            "Epoch 506/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9183 - accuracy: 1.0000 - val_loss: 1.8935 - val_accuracy: 1.0000\n",
            "Epoch 507/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.9112 - val_accuracy: 1.0000\n",
            "Epoch 508/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9150 - accuracy: 1.0000 - val_loss: 1.9109 - val_accuracy: 1.0000\n",
            "Epoch 509/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9843 - val_accuracy: 1.0000\n",
            "Epoch 510/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9054 - val_accuracy: 1.0000\n",
            "Epoch 511/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9308 - accuracy: 1.0000 - val_loss: 1.9094 - val_accuracy: 1.0000\n",
            "Epoch 512/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9080 - accuracy: 1.0000 - val_loss: 1.9185 - val_accuracy: 1.0000\n",
            "Epoch 513/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9159 - accuracy: 1.0000 - val_loss: 1.8874 - val_accuracy: 1.0000\n",
            "Epoch 514/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9192 - accuracy: 1.0000 - val_loss: 1.9185 - val_accuracy: 1.0000\n",
            "Epoch 515/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 2.2376 - val_accuracy: 1.0000\n",
            "Epoch 516/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9253 - accuracy: 1.0000 - val_loss: 1.9456 - val_accuracy: 1.0000\n",
            "Epoch 517/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9221 - accuracy: 1.0000 - val_loss: 1.9819 - val_accuracy: 1.0000\n",
            "Epoch 518/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.8961 - val_accuracy: 1.0000\n",
            "Epoch 519/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9128 - accuracy: 1.0000 - val_loss: 1.8969 - val_accuracy: 1.0000\n",
            "Epoch 520/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9429 - accuracy: 1.0000 - val_loss: 1.9124 - val_accuracy: 1.0000\n",
            "Epoch 521/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9040 - accuracy: 1.0000 - val_loss: 1.9824 - val_accuracy: 1.0000\n",
            "Epoch 522/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9236 - accuracy: 1.0000 - val_loss: 2.0573 - val_accuracy: 1.0000\n",
            "Epoch 523/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9213 - accuracy: 1.0000 - val_loss: 2.0125 - val_accuracy: 1.0000\n",
            "Epoch 524/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9269 - accuracy: 1.0000 - val_loss: 1.9892 - val_accuracy: 1.0000\n",
            "Epoch 525/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.9030 - val_accuracy: 1.0000\n",
            "Epoch 526/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9176 - accuracy: 1.0000 - val_loss: 1.8896 - val_accuracy: 1.0000\n",
            "Epoch 527/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 1.8912 - val_accuracy: 1.0000\n",
            "Epoch 528/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9269 - accuracy: 1.0000 - val_loss: 1.8976 - val_accuracy: 1.0000\n",
            "Epoch 529/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9097 - accuracy: 1.0000 - val_loss: 1.8915 - val_accuracy: 1.0000\n",
            "Epoch 530/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9145 - accuracy: 1.0000 - val_loss: 1.8991 - val_accuracy: 1.0000\n",
            "Epoch 531/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 2.1699 - val_accuracy: 1.0000\n",
            "Epoch 532/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9251 - accuracy: 1.0000 - val_loss: 1.9480 - val_accuracy: 1.0000\n",
            "Epoch 533/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9195 - accuracy: 1.0000 - val_loss: 1.8914 - val_accuracy: 1.0000\n",
            "Epoch 534/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9232 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 1.0000\n",
            "Epoch 535/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9127 - accuracy: 1.0000 - val_loss: 1.9179 - val_accuracy: 1.0000\n",
            "Epoch 536/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9197 - accuracy: 1.0000 - val_loss: 1.8993 - val_accuracy: 1.0000\n",
            "Epoch 537/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9126 - accuracy: 1.0000 - val_loss: 1.9196 - val_accuracy: 1.0000\n",
            "Epoch 538/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 1.0000\n",
            "Epoch 539/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9188 - accuracy: 1.0000 - val_loss: 1.9377 - val_accuracy: 1.0000\n",
            "Epoch 540/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9213 - accuracy: 1.0000 - val_loss: 1.9097 - val_accuracy: 1.0000\n",
            "Epoch 541/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9205 - accuracy: 1.0000 - val_loss: 1.8902 - val_accuracy: 1.0000\n",
            "Epoch 542/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9141 - accuracy: 1.0000 - val_loss: 1.8962 - val_accuracy: 1.0000\n",
            "Epoch 543/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9184 - accuracy: 1.0000 - val_loss: 1.9074 - val_accuracy: 1.0000\n",
            "Epoch 544/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 1.9568 - val_accuracy: 1.0000\n",
            "Epoch 545/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.8926 - val_accuracy: 1.0000\n",
            "Epoch 546/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9181 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 1.0000\n",
            "Epoch 547/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 548/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9253 - accuracy: 1.0000 - val_loss: 1.9847 - val_accuracy: 1.0000\n",
            "Epoch 549/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9207 - val_accuracy: 1.0000\n",
            "Epoch 550/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 1.8898 - val_accuracy: 1.0000\n",
            "Epoch 551/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 1.9107 - val_accuracy: 1.0000\n",
            "Epoch 552/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 1.8946 - val_accuracy: 1.0000\n",
            "Epoch 553/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.8951 - val_accuracy: 1.0000\n",
            "Epoch 554/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9018 - accuracy: 1.0000 - val_loss: 1.9195 - val_accuracy: 1.0000\n",
            "Epoch 555/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9307 - accuracy: 1.0000 - val_loss: 1.9319 - val_accuracy: 1.0000\n",
            "Epoch 556/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9535 - val_accuracy: 1.0000\n",
            "Epoch 557/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9257 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 1.0000\n",
            "Epoch 558/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.8898 - val_accuracy: 1.0000\n",
            "Epoch 559/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9255 - accuracy: 1.0000 - val_loss: 1.8886 - val_accuracy: 1.0000\n",
            "Epoch 560/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8991 - accuracy: 1.0000 - val_loss: 1.9295 - val_accuracy: 1.0000\n",
            "Epoch 561/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9295 - accuracy: 1.0000 - val_loss: 2.0053 - val_accuracy: 1.0000\n",
            "Epoch 562/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9207 - accuracy: 1.0000 - val_loss: 1.9322 - val_accuracy: 1.0000\n",
            "Epoch 563/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.9429 - val_accuracy: 1.0000\n",
            "Epoch 564/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9208 - accuracy: 1.0000 - val_loss: 1.9135 - val_accuracy: 1.0000\n",
            "Epoch 565/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9277 - accuracy: 1.0000 - val_loss: 1.9045 - val_accuracy: 1.0000\n",
            "Epoch 566/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.9031 - val_accuracy: 1.0000\n",
            "Epoch 567/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9367 - accuracy: 1.0000 - val_loss: 1.9232 - val_accuracy: 1.0000\n",
            "Epoch 568/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9034 - accuracy: 1.0000 - val_loss: 1.9489 - val_accuracy: 1.0000\n",
            "Epoch 569/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.9085 - val_accuracy: 1.0000\n",
            "Epoch 570/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9281 - accuracy: 1.0000 - val_loss: 1.9041 - val_accuracy: 1.0000\n",
            "Epoch 571/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9044 - accuracy: 1.0000 - val_loss: 3.8783 - val_accuracy: 1.0000\n",
            "Epoch 572/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9599 - accuracy: 1.0000 - val_loss: 1.8864 - val_accuracy: 1.0000\n",
            "Epoch 573/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9251 - accuracy: 1.0000 - val_loss: 1.9235 - val_accuracy: 1.0000\n",
            "Epoch 574/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9058 - accuracy: 1.0000 - val_loss: 2.0038 - val_accuracy: 1.0000\n",
            "Epoch 575/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.9154 - val_accuracy: 1.0000\n",
            "Epoch 576/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9187 - accuracy: 1.0000 - val_loss: 1.9132 - val_accuracy: 1.0000\n",
            "Epoch 577/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 2.0266 - val_accuracy: 1.0000\n",
            "Epoch 578/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9284 - accuracy: 1.0000 - val_loss: 1.9178 - val_accuracy: 1.0000\n",
            "Epoch 579/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9124 - accuracy: 1.0000 - val_loss: 1.9549 - val_accuracy: 1.0000\n",
            "Epoch 580/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.9347 - val_accuracy: 1.0000\n",
            "Epoch 581/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 1.9399 - val_accuracy: 1.0000\n",
            "Epoch 582/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9282 - accuracy: 1.0000 - val_loss: 1.9206 - val_accuracy: 1.0000\n",
            "Epoch 583/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.9813 - val_accuracy: 1.0000\n",
            "Epoch 584/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9298 - accuracy: 1.0000 - val_loss: 1.9070 - val_accuracy: 1.0000\n",
            "Epoch 585/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9080 - accuracy: 1.0000 - val_loss: 1.9206 - val_accuracy: 1.0000\n",
            "Epoch 586/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.8862 - val_accuracy: 1.0000\n",
            "Epoch 587/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.9146 - val_accuracy: 1.0000\n",
            "Epoch 588/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9288 - accuracy: 1.0000 - val_loss: 1.9587 - val_accuracy: 1.0000\n",
            "Epoch 589/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9307 - accuracy: 1.0000 - val_loss: 2.0828 - val_accuracy: 1.0000\n",
            "Epoch 590/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9034 - accuracy: 1.0000 - val_loss: 1.9095 - val_accuracy: 1.0000\n",
            "Epoch 591/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9256 - accuracy: 1.0000 - val_loss: 1.9700 - val_accuracy: 1.0000\n",
            "Epoch 592/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.8912 - val_accuracy: 1.0000\n",
            "Epoch 593/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.8937 - val_accuracy: 1.0000\n",
            "Epoch 594/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9421 - val_accuracy: 1.0000\n",
            "Epoch 595/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9219 - accuracy: 1.0000 - val_loss: 2.0141 - val_accuracy: 1.0000\n",
            "Epoch 596/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 2.0139 - val_accuracy: 1.0000\n",
            "Epoch 597/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9374 - accuracy: 1.0000 - val_loss: 1.8965 - val_accuracy: 1.0000\n",
            "Epoch 598/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9629 - val_accuracy: 1.0000\n",
            "Epoch 599/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.9139 - val_accuracy: 1.0000\n",
            "Epoch 600/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.9783 - val_accuracy: 1.0000\n",
            "Epoch 601/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9178 - accuracy: 1.0000 - val_loss: 1.9198 - val_accuracy: 1.0000\n",
            "Epoch 602/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9246 - accuracy: 1.0000 - val_loss: 1.9350 - val_accuracy: 1.0000\n",
            "Epoch 603/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9447 - accuracy: 1.0000 - val_loss: 1.8997 - val_accuracy: 1.0000\n",
            "Epoch 604/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9032 - accuracy: 1.0000 - val_loss: 1.8881 - val_accuracy: 1.0000\n",
            "Epoch 605/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.9198 - val_accuracy: 1.0000\n",
            "Epoch 606/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9079 - accuracy: 1.0000 - val_loss: 1.8938 - val_accuracy: 1.0000\n",
            "Epoch 607/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9188 - accuracy: 1.0000 - val_loss: 1.9071 - val_accuracy: 1.0000\n",
            "Epoch 608/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9223 - accuracy: 1.0000 - val_loss: 1.8997 - val_accuracy: 1.0000\n",
            "Epoch 609/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9150 - accuracy: 1.0000 - val_loss: 2.1658 - val_accuracy: 1.0000\n",
            "Epoch 610/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9180 - accuracy: 1.0000 - val_loss: 1.8929 - val_accuracy: 1.0000\n",
            "Epoch 611/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9196 - accuracy: 1.0000 - val_loss: 2.0439 - val_accuracy: 1.0000\n",
            "Epoch 612/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9674 - val_accuracy: 1.0000\n",
            "Epoch 613/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9259 - accuracy: 1.0000 - val_loss: 1.8982 - val_accuracy: 1.0000\n",
            "Epoch 614/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9094 - accuracy: 1.0000 - val_loss: 2.0190 - val_accuracy: 1.0000\n",
            "Epoch 615/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9208 - accuracy: 1.0000 - val_loss: 1.9169 - val_accuracy: 1.0000\n",
            "Epoch 616/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9394 - accuracy: 1.0000 - val_loss: 1.9151 - val_accuracy: 1.0000\n",
            "Epoch 617/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9089 - accuracy: 1.0000 - val_loss: 1.8894 - val_accuracy: 1.0000\n",
            "Epoch 618/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9140 - accuracy: 1.0000 - val_loss: 1.9277 - val_accuracy: 1.0000\n",
            "Epoch 619/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.9078 - val_accuracy: 1.0000\n",
            "Epoch 620/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 1.9964 - val_accuracy: 1.0000\n",
            "Epoch 621/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9265 - accuracy: 1.0000 - val_loss: 1.9929 - val_accuracy: 1.0000\n",
            "Epoch 622/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.9303 - val_accuracy: 1.0000\n",
            "Epoch 623/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9223 - accuracy: 1.0000 - val_loss: 1.9022 - val_accuracy: 1.0000\n",
            "Epoch 624/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 1.9535 - val_accuracy: 1.0000\n",
            "Epoch 625/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9106 - accuracy: 1.0000 - val_loss: 1.8974 - val_accuracy: 1.0000\n",
            "Epoch 626/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9288 - accuracy: 1.0000 - val_loss: 1.9135 - val_accuracy: 1.0000\n",
            "Epoch 627/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 1.9067 - val_accuracy: 1.0000\n",
            "Epoch 628/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9009 - val_accuracy: 1.0000\n",
            "Epoch 629/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9200 - accuracy: 1.0000 - val_loss: 1.9313 - val_accuracy: 1.0000\n",
            "Epoch 630/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.8958 - val_accuracy: 1.0000\n",
            "Epoch 631/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9189 - accuracy: 1.0000 - val_loss: 1.9238 - val_accuracy: 1.0000\n",
            "Epoch 632/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9023 - val_accuracy: 1.0000\n",
            "Epoch 633/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.9193 - val_accuracy: 1.0000\n",
            "Epoch 634/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9206 - accuracy: 1.0000 - val_loss: 1.9230 - val_accuracy: 1.0000\n",
            "Epoch 635/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.9657 - val_accuracy: 1.0000\n",
            "Epoch 636/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 1.9404 - val_accuracy: 1.0000\n",
            "Epoch 637/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9182 - accuracy: 1.0000 - val_loss: 1.9807 - val_accuracy: 1.0000\n",
            "Epoch 638/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 1.9215 - val_accuracy: 1.0000\n",
            "Epoch 639/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 1.9561 - val_accuracy: 1.0000\n",
            "Epoch 640/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.9826 - val_accuracy: 1.0000\n",
            "Epoch 641/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 2.1017 - val_accuracy: 1.0000\n",
            "Epoch 642/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9361 - accuracy: 1.0000 - val_loss: 1.9205 - val_accuracy: 1.0000\n",
            "Epoch 643/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9197 - accuracy: 1.0000 - val_loss: 1.9436 - val_accuracy: 1.0000\n",
            "Epoch 644/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9051 - accuracy: 1.0000 - val_loss: 1.9270 - val_accuracy: 1.0000\n",
            "Epoch 645/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 2.0060 - val_accuracy: 1.0000\n",
            "Epoch 646/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9183 - accuracy: 1.0000 - val_loss: 1.8975 - val_accuracy: 1.0000\n",
            "Epoch 647/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.9225 - val_accuracy: 1.0000\n",
            "Epoch 648/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 1.9108 - val_accuracy: 1.0000\n",
            "Epoch 649/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9102 - accuracy: 1.0000 - val_loss: 1.8988 - val_accuracy: 1.0000\n",
            "Epoch 650/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9116 - accuracy: 1.0000 - val_loss: 1.9103 - val_accuracy: 1.0000\n",
            "Epoch 651/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9276 - accuracy: 1.0000 - val_loss: 1.9193 - val_accuracy: 1.0000\n",
            "Epoch 652/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9038 - accuracy: 1.0000 - val_loss: 2.0045 - val_accuracy: 1.0000\n",
            "Epoch 653/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9217 - accuracy: 1.0000 - val_loss: 1.9096 - val_accuracy: 1.0000\n",
            "Epoch 654/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.9482 - val_accuracy: 1.0000\n",
            "Epoch 655/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9181 - accuracy: 1.0000 - val_loss: 2.0274 - val_accuracy: 1.0000\n",
            "Epoch 656/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9272 - accuracy: 1.0000 - val_loss: 1.9093 - val_accuracy: 1.0000\n",
            "Epoch 657/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.9554 - val_accuracy: 1.0000\n",
            "Epoch 658/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9187 - accuracy: 1.0000 - val_loss: 1.8958 - val_accuracy: 1.0000\n",
            "Epoch 659/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9154 - accuracy: 1.0000 - val_loss: 2.0475 - val_accuracy: 1.0000\n",
            "Epoch 660/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 1.8902 - val_accuracy: 1.0000\n",
            "Epoch 661/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9494 - val_accuracy: 1.0000\n",
            "Epoch 662/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9256 - accuracy: 1.0000 - val_loss: 1.9098 - val_accuracy: 1.0000\n",
            "Epoch 663/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9096 - accuracy: 1.0000 - val_loss: 1.9074 - val_accuracy: 1.0000\n",
            "Epoch 664/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9105 - accuracy: 1.0000 - val_loss: 1.8910 - val_accuracy: 1.0000\n",
            "Epoch 665/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 1.8861 - val_accuracy: 1.0000\n",
            "Epoch 666/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.9033 - val_accuracy: 1.0000\n",
            "Epoch 667/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9175 - accuracy: 1.0000 - val_loss: 1.9296 - val_accuracy: 1.0000\n",
            "Epoch 668/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9230 - accuracy: 1.0000 - val_loss: 2.0128 - val_accuracy: 1.0000\n",
            "Epoch 669/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.9176 - val_accuracy: 1.0000\n",
            "Epoch 670/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9309 - val_accuracy: 1.0000\n",
            "Epoch 671/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.9176 - val_accuracy: 1.0000\n",
            "Epoch 672/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9280 - accuracy: 1.0000 - val_loss: 1.9479 - val_accuracy: 1.0000\n",
            "Epoch 673/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9187 - accuracy: 1.0000 - val_loss: 1.9055 - val_accuracy: 1.0000\n",
            "Epoch 674/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9536 - val_accuracy: 1.0000\n",
            "Epoch 675/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9382 - accuracy: 1.0000 - val_loss: 1.8984 - val_accuracy: 1.0000\n",
            "Epoch 676/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8952 - accuracy: 1.0000 - val_loss: 1.9663 - val_accuracy: 1.0000\n",
            "Epoch 677/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.9011 - val_accuracy: 1.0000\n",
            "Epoch 678/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9092 - val_accuracy: 1.0000\n",
            "Epoch 679/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9377 - accuracy: 1.0000 - val_loss: 1.8916 - val_accuracy: 1.0000\n",
            "Epoch 680/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.8999 - accuracy: 1.0000 - val_loss: 1.9027 - val_accuracy: 1.0000\n",
            "Epoch 681/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 2.0306 - val_accuracy: 1.0000\n",
            "Epoch 682/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 1.9177 - val_accuracy: 1.0000\n",
            "Epoch 683/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9249 - accuracy: 1.0000 - val_loss: 1.9133 - val_accuracy: 1.0000\n",
            "Epoch 684/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9173 - accuracy: 1.0000 - val_loss: 1.8909 - val_accuracy: 1.0000\n",
            "Epoch 685/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9139 - accuracy: 1.0000 - val_loss: 2.0553 - val_accuracy: 1.0000\n",
            "Epoch 686/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.8866 - val_accuracy: 1.0000\n",
            "Epoch 687/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9375 - accuracy: 1.0000 - val_loss: 1.8928 - val_accuracy: 1.0000\n",
            "Epoch 688/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8968 - accuracy: 1.0000 - val_loss: 1.9510 - val_accuracy: 1.0000\n",
            "Epoch 689/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9461 - val_accuracy: 1.0000\n",
            "Epoch 690/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.8891 - val_accuracy: 1.0000\n",
            "Epoch 691/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9066 - accuracy: 1.0000 - val_loss: 1.9241 - val_accuracy: 1.0000\n",
            "Epoch 692/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9179 - accuracy: 1.0000 - val_loss: 1.9099 - val_accuracy: 1.0000\n",
            "Epoch 693/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 2.0213 - val_accuracy: 1.0000\n",
            "Epoch 694/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9316 - accuracy: 1.0000 - val_loss: 2.1050 - val_accuracy: 1.0000\n",
            "Epoch 695/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9123 - accuracy: 1.0000 - val_loss: 1.9541 - val_accuracy: 1.0000\n",
            "Epoch 696/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9142 - accuracy: 1.0000 - val_loss: 1.9095 - val_accuracy: 1.0000\n",
            "Epoch 697/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9189 - accuracy: 1.0000 - val_loss: 1.9077 - val_accuracy: 1.0000\n",
            "Epoch 698/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9291 - val_accuracy: 1.0000\n",
            "Epoch 699/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9113 - accuracy: 1.0000 - val_loss: 1.9391 - val_accuracy: 1.0000\n",
            "Epoch 700/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9272 - accuracy: 1.0000 - val_loss: 1.9055 - val_accuracy: 1.0000\n",
            "Epoch 701/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9196 - accuracy: 1.0000 - val_loss: 1.9029 - val_accuracy: 1.0000\n",
            "Epoch 702/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.8918 - val_accuracy: 1.0000\n",
            "Epoch 703/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9083 - accuracy: 1.0000 - val_loss: 1.9188 - val_accuracy: 1.0000\n",
            "Epoch 704/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9214 - accuracy: 1.0000 - val_loss: 1.9502 - val_accuracy: 1.0000\n",
            "Epoch 705/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9180 - accuracy: 1.0000 - val_loss: 1.8883 - val_accuracy: 1.0000\n",
            "Epoch 706/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9188 - accuracy: 1.0000 - val_loss: 1.9054 - val_accuracy: 1.0000\n",
            "Epoch 707/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.9274 - val_accuracy: 1.0000\n",
            "Epoch 708/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.9257 - val_accuracy: 1.0000\n",
            "Epoch 709/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.9401 - val_accuracy: 1.0000\n",
            "Epoch 710/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9122 - accuracy: 1.0000 - val_loss: 1.9335 - val_accuracy: 1.0000\n",
            "Epoch 711/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9392 - accuracy: 1.0000 - val_loss: 2.0884 - val_accuracy: 1.0000\n",
            "Epoch 712/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9017 - accuracy: 1.0000 - val_loss: 1.9652 - val_accuracy: 1.0000\n",
            "Epoch 713/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9139 - accuracy: 1.0000 - val_loss: 1.8906 - val_accuracy: 1.0000\n",
            "Epoch 714/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9383 - accuracy: 1.0000 - val_loss: 1.8873 - val_accuracy: 1.0000\n",
            "Epoch 715/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8981 - accuracy: 1.0000 - val_loss: 1.9008 - val_accuracy: 1.0000\n",
            "Epoch 716/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9483 - val_accuracy: 1.0000\n",
            "Epoch 717/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 1.8992 - val_accuracy: 1.0000\n",
            "Epoch 718/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9200 - accuracy: 1.0000 - val_loss: 1.8886 - val_accuracy: 1.0000\n",
            "Epoch 719/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.8977 - val_accuracy: 1.0000\n",
            "Epoch 720/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9097 - accuracy: 1.0000 - val_loss: 2.0356 - val_accuracy: 1.0000\n",
            "Epoch 721/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9237 - accuracy: 1.0000 - val_loss: 1.9058 - val_accuracy: 1.0000\n",
            "Epoch 722/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9195 - accuracy: 1.0000 - val_loss: 1.8992 - val_accuracy: 1.0000\n",
            "Epoch 723/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9113 - accuracy: 1.0000 - val_loss: 1.8998 - val_accuracy: 1.0000\n",
            "Epoch 724/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9243 - accuracy: 1.0000 - val_loss: 2.0098 - val_accuracy: 1.0000\n",
            "Epoch 725/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9197 - accuracy: 1.0000 - val_loss: 2.1536 - val_accuracy: 1.0000\n",
            "Epoch 726/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9488 - accuracy: 1.0000 - val_loss: 1.9603 - val_accuracy: 1.0000\n",
            "Epoch 727/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9016 - accuracy: 1.0000 - val_loss: 1.8926 - val_accuracy: 1.0000\n",
            "Epoch 728/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9006 - accuracy: 1.0000 - val_loss: 1.9574 - val_accuracy: 1.0000\n",
            "Epoch 729/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9260 - accuracy: 1.0000 - val_loss: 1.9497 - val_accuracy: 1.0000\n",
            "Epoch 730/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 2.9553 - val_accuracy: 1.0000\n",
            "Epoch 731/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9571 - accuracy: 1.0000 - val_loss: 1.8826 - val_accuracy: 1.0000\n",
            "Epoch 732/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9075 - accuracy: 1.0000 - val_loss: 1.9607 - val_accuracy: 1.0000\n",
            "Epoch 733/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.9247 - val_accuracy: 1.0000\n",
            "Epoch 734/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9092 - accuracy: 1.0000 - val_loss: 1.9470 - val_accuracy: 1.0000\n",
            "Epoch 735/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 1.8967 - val_accuracy: 1.0000\n",
            "Epoch 736/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9182 - accuracy: 1.0000 - val_loss: 1.9116 - val_accuracy: 1.0000\n",
            "Epoch 737/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.9586 - val_accuracy: 1.0000\n",
            "Epoch 738/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9227 - accuracy: 1.0000 - val_loss: 1.9749 - val_accuracy: 1.0000\n",
            "Epoch 739/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9246 - accuracy: 1.0000 - val_loss: 1.9725 - val_accuracy: 1.0000\n",
            "Epoch 740/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.9576 - val_accuracy: 1.0000\n",
            "Epoch 741/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9122 - accuracy: 1.0000 - val_loss: 1.8919 - val_accuracy: 1.0000\n",
            "Epoch 742/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9141 - accuracy: 1.0000 - val_loss: 2.0276 - val_accuracy: 1.0000\n",
            "Epoch 743/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9189 - accuracy: 1.0000 - val_loss: 1.8908 - val_accuracy: 1.0000\n",
            "Epoch 744/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9333 - val_accuracy: 1.0000\n",
            "Epoch 745/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 2.0528 - val_accuracy: 1.0000\n",
            "Epoch 746/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9078 - accuracy: 1.0000 - val_loss: 1.8947 - val_accuracy: 1.0000\n",
            "Epoch 747/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 2.0117 - val_accuracy: 1.0000\n",
            "Epoch 748/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 2.1177 - val_accuracy: 1.0000\n",
            "Epoch 749/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9133 - accuracy: 1.0000 - val_loss: 1.9007 - val_accuracy: 1.0000\n",
            "Epoch 750/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9098 - accuracy: 1.0000 - val_loss: 1.9228 - val_accuracy: 1.0000\n",
            "Epoch 751/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9173 - accuracy: 1.0000 - val_loss: 1.9010 - val_accuracy: 1.0000\n",
            "Epoch 752/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9129 - accuracy: 1.0000 - val_loss: 1.9216 - val_accuracy: 1.0000\n",
            "Epoch 753/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9296 - accuracy: 1.0000 - val_loss: 1.9020 - val_accuracy: 1.0000\n",
            "Epoch 754/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.9147 - val_accuracy: 1.0000\n",
            "Epoch 755/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9240 - accuracy: 1.0000 - val_loss: 2.1606 - val_accuracy: 1.0000\n",
            "Epoch 756/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 1.8872 - val_accuracy: 1.0000\n",
            "Epoch 757/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9091 - val_accuracy: 1.0000\n",
            "Epoch 758/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9437 - val_accuracy: 1.0000\n",
            "Epoch 759/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 2.1011 - val_accuracy: 1.0000\n",
            "Epoch 760/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9139 - accuracy: 1.0000 - val_loss: 1.9217 - val_accuracy: 1.0000\n",
            "Epoch 761/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9142 - accuracy: 1.0000 - val_loss: 1.9476 - val_accuracy: 1.0000\n",
            "Epoch 762/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9170 - accuracy: 1.0000 - val_loss: 1.9147 - val_accuracy: 1.0000\n",
            "Epoch 763/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.9330 - val_accuracy: 1.0000\n",
            "Epoch 764/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9187 - accuracy: 1.0000 - val_loss: 1.9117 - val_accuracy: 1.0000\n",
            "Epoch 765/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9126 - accuracy: 1.0000 - val_loss: 1.9559 - val_accuracy: 1.0000\n",
            "Epoch 766/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.9018 - val_accuracy: 1.0000\n",
            "Epoch 767/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9175 - accuracy: 1.0000 - val_loss: 1.8895 - val_accuracy: 1.0000\n",
            "Epoch 768/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.9138 - val_accuracy: 1.0000\n",
            "Epoch 769/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 1.8986 - val_accuracy: 1.0000\n",
            "Epoch 770/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9252 - accuracy: 1.0000 - val_loss: 1.8948 - val_accuracy: 1.0000\n",
            "Epoch 771/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9078 - accuracy: 1.0000 - val_loss: 1.9217 - val_accuracy: 1.0000\n",
            "Epoch 772/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.9018 - val_accuracy: 1.0000\n",
            "Epoch 773/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9122 - accuracy: 1.0000 - val_loss: 1.9424 - val_accuracy: 1.0000\n",
            "Epoch 774/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9224 - accuracy: 1.0000 - val_loss: 2.0114 - val_accuracy: 1.0000\n",
            "Epoch 775/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.8870 - val_accuracy: 1.0000\n",
            "Epoch 776/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.9965 - val_accuracy: 1.0000\n",
            "Epoch 777/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9366 - val_accuracy: 1.0000\n",
            "Epoch 778/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9127 - accuracy: 1.0000 - val_loss: 1.9090 - val_accuracy: 1.0000\n",
            "Epoch 779/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9128 - accuracy: 1.0000 - val_loss: 1.8858 - val_accuracy: 1.0000\n",
            "Epoch 780/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9229 - accuracy: 1.0000 - val_loss: 1.8869 - val_accuracy: 1.0000\n",
            "Epoch 781/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9407 - val_accuracy: 1.0000\n",
            "Epoch 782/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9180 - accuracy: 1.0000 - val_loss: 1.8967 - val_accuracy: 1.0000\n",
            "Epoch 783/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9140 - accuracy: 1.0000 - val_loss: 1.9430 - val_accuracy: 1.0000\n",
            "Epoch 784/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9173 - accuracy: 1.0000 - val_loss: 1.9292 - val_accuracy: 1.0000\n",
            "Epoch 785/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9290 - accuracy: 1.0000 - val_loss: 1.8992 - val_accuracy: 1.0000\n",
            "Epoch 786/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9039 - accuracy: 1.0000 - val_loss: 1.9129 - val_accuracy: 1.0000\n",
            "Epoch 787/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.9228 - val_accuracy: 1.0000\n",
            "Epoch 788/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.8924 - val_accuracy: 1.0000\n",
            "Epoch 789/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9243 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 1.0000\n",
            "Epoch 790/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9017 - accuracy: 1.0000 - val_loss: 1.9050 - val_accuracy: 1.0000\n",
            "Epoch 791/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9251 - accuracy: 1.0000 - val_loss: 1.9478 - val_accuracy: 1.0000\n",
            "Epoch 792/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9076 - accuracy: 1.0000 - val_loss: 1.9034 - val_accuracy: 1.0000\n",
            "Epoch 793/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9169 - accuracy: 1.0000 - val_loss: 2.0514 - val_accuracy: 1.0000\n",
            "Epoch 794/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9071 - accuracy: 1.0000 - val_loss: 2.0141 - val_accuracy: 1.0000\n",
            "Epoch 795/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 2.0268 - val_accuracy: 1.0000\n",
            "Epoch 796/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.8837 - val_accuracy: 1.0000\n",
            "Epoch 797/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9140 - accuracy: 1.0000 - val_loss: 2.0020 - val_accuracy: 1.0000\n",
            "Epoch 798/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9324 - accuracy: 1.0000 - val_loss: 1.9451 - val_accuracy: 1.0000\n",
            "Epoch 799/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9114 - accuracy: 1.0000 - val_loss: 2.0866 - val_accuracy: 1.0000\n",
            "Epoch 800/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9255 - accuracy: 1.0000 - val_loss: 1.9383 - val_accuracy: 1.0000\n",
            "Epoch 801/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9075 - accuracy: 1.0000 - val_loss: 1.9509 - val_accuracy: 1.0000\n",
            "Epoch 802/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9211 - val_accuracy: 1.0000\n",
            "Epoch 803/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9107 - accuracy: 1.0000 - val_loss: 1.9205 - val_accuracy: 1.0000\n",
            "Epoch 804/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9078 - accuracy: 1.0000 - val_loss: 2.0798 - val_accuracy: 1.0000\n",
            "Epoch 805/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9232 - accuracy: 1.0000 - val_loss: 1.9307 - val_accuracy: 1.0000\n",
            "Epoch 806/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9213 - accuracy: 1.0000 - val_loss: 1.9075 - val_accuracy: 1.0000\n",
            "Epoch 807/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9088 - accuracy: 1.0000 - val_loss: 1.9186 - val_accuracy: 1.0000\n",
            "Epoch 808/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9218 - accuracy: 1.0000 - val_loss: 2.0134 - val_accuracy: 1.0000\n",
            "Epoch 809/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 2.0126 - val_accuracy: 1.0000\n",
            "Epoch 810/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 1.9111 - val_accuracy: 1.0000\n",
            "Epoch 811/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9192 - accuracy: 1.0000 - val_loss: 1.9291 - val_accuracy: 1.0000\n",
            "Epoch 812/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.9037 - val_accuracy: 1.0000\n",
            "Epoch 813/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9092 - accuracy: 1.0000 - val_loss: 1.9234 - val_accuracy: 1.0000\n",
            "Epoch 814/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.9296 - val_accuracy: 1.0000\n",
            "Epoch 815/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9389 - val_accuracy: 1.0000\n",
            "Epoch 816/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9238 - accuracy: 1.0000 - val_loss: 1.8993 - val_accuracy: 1.0000\n",
            "Epoch 817/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9295 - accuracy: 1.0000 - val_loss: 1.8911 - val_accuracy: 1.0000\n",
            "Epoch 818/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8923 - accuracy: 1.0000 - val_loss: 1.8876 - val_accuracy: 1.0000\n",
            "Epoch 819/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9300 - accuracy: 1.0000 - val_loss: 1.9234 - val_accuracy: 1.0000\n",
            "Epoch 820/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9193 - accuracy: 1.0000 - val_loss: 1.9433 - val_accuracy: 1.0000\n",
            "Epoch 821/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.9093 - val_accuracy: 1.0000\n",
            "Epoch 822/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.9411 - val_accuracy: 1.0000\n",
            "Epoch 823/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.9342 - val_accuracy: 1.0000\n",
            "Epoch 824/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9048 - accuracy: 1.0000 - val_loss: 2.0175 - val_accuracy: 1.0000\n",
            "Epoch 825/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9270 - accuracy: 1.0000 - val_loss: 2.2737 - val_accuracy: 1.0000\n",
            "Epoch 826/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9097 - accuracy: 1.0000 - val_loss: 1.9140 - val_accuracy: 1.0000\n",
            "Epoch 827/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9086 - accuracy: 1.0000 - val_loss: 1.9749 - val_accuracy: 1.0000\n",
            "Epoch 828/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9184 - accuracy: 1.0000 - val_loss: 1.9043 - val_accuracy: 1.0000\n",
            "Epoch 829/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9201 - accuracy: 1.0000 - val_loss: 1.9278 - val_accuracy: 1.0000\n",
            "Epoch 830/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9150 - accuracy: 1.0000 - val_loss: 1.9504 - val_accuracy: 1.0000\n",
            "Epoch 831/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9165 - accuracy: 1.0000 - val_loss: 1.8975 - val_accuracy: 1.0000\n",
            "Epoch 832/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.9319 - val_accuracy: 1.0000\n",
            "Epoch 833/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9094 - accuracy: 1.0000 - val_loss: 2.0274 - val_accuracy: 1.0000\n",
            "Epoch 834/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9203 - accuracy: 1.0000 - val_loss: 1.9588 - val_accuracy: 1.0000\n",
            "Epoch 835/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 2.0334 - val_accuracy: 1.0000\n",
            "Epoch 836/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9203 - accuracy: 1.0000 - val_loss: 1.8855 - val_accuracy: 1.0000\n",
            "Epoch 837/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9373 - val_accuracy: 1.0000\n",
            "Epoch 838/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.9232 - val_accuracy: 1.0000\n",
            "Epoch 839/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9094 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 1.0000\n",
            "Epoch 840/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9232 - accuracy: 1.0000 - val_loss: 1.8996 - val_accuracy: 1.0000\n",
            "Epoch 841/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9036 - accuracy: 1.0000 - val_loss: 2.0150 - val_accuracy: 1.0000\n",
            "Epoch 842/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9269 - accuracy: 1.0000 - val_loss: 1.9431 - val_accuracy: 1.0000\n",
            "Epoch 843/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9098 - accuracy: 1.0000 - val_loss: 1.9325 - val_accuracy: 1.0000\n",
            "Epoch 844/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9143 - val_accuracy: 1.0000\n",
            "Epoch 845/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9195 - accuracy: 1.0000 - val_loss: 1.9622 - val_accuracy: 1.0000\n",
            "Epoch 846/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9183 - accuracy: 1.0000 - val_loss: 1.8916 - val_accuracy: 1.0000\n",
            "Epoch 847/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9354 - accuracy: 1.0000 - val_loss: 1.9006 - val_accuracy: 1.0000\n",
            "Epoch 848/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9008 - accuracy: 1.0000 - val_loss: 1.8980 - val_accuracy: 1.0000\n",
            "Epoch 849/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9087 - accuracy: 1.0000 - val_loss: 1.8997 - val_accuracy: 1.0000\n",
            "Epoch 850/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9176 - accuracy: 1.0000 - val_loss: 1.8987 - val_accuracy: 1.0000\n",
            "Epoch 851/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9083 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 1.0000\n",
            "Epoch 852/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 1.8983 - val_accuracy: 1.0000\n",
            "Epoch 853/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9124 - accuracy: 1.0000 - val_loss: 1.9391 - val_accuracy: 1.0000\n",
            "Epoch 854/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9310 - accuracy: 1.0000 - val_loss: 1.9748 - val_accuracy: 1.0000\n",
            "Epoch 855/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9035 - accuracy: 1.0000 - val_loss: 1.8831 - val_accuracy: 1.0000\n",
            "Epoch 856/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 2.0632 - val_accuracy: 1.0000\n",
            "Epoch 857/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9149 - val_accuracy: 1.0000\n",
            "Epoch 858/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9076 - accuracy: 1.0000 - val_loss: 1.9528 - val_accuracy: 1.0000\n",
            "Epoch 859/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9141 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 860/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 1.8904 - val_accuracy: 1.0000\n",
            "Epoch 861/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.8868 - val_accuracy: 1.0000\n",
            "Epoch 862/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.9082 - val_accuracy: 1.0000\n",
            "Epoch 863/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.9795 - val_accuracy: 1.0000\n",
            "Epoch 864/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9113 - accuracy: 1.0000 - val_loss: 1.9073 - val_accuracy: 1.0000\n",
            "Epoch 865/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9190 - accuracy: 1.0000 - val_loss: 2.0442 - val_accuracy: 1.0000\n",
            "Epoch 866/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.9108 - val_accuracy: 1.0000\n",
            "Epoch 867/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9181 - accuracy: 1.0000 - val_loss: 1.9841 - val_accuracy: 1.0000\n",
            "Epoch 868/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9224 - accuracy: 1.0000 - val_loss: 1.8826 - val_accuracy: 1.0000\n",
            "Epoch 869/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9036 - val_accuracy: 1.0000\n",
            "Epoch 870/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.9290 - val_accuracy: 1.0000\n",
            "Epoch 871/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9108 - accuracy: 1.0000 - val_loss: 1.9698 - val_accuracy: 1.0000\n",
            "Epoch 872/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 1.9751 - val_accuracy: 1.0000\n",
            "Epoch 873/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9087 - accuracy: 1.0000 - val_loss: 1.9937 - val_accuracy: 1.0000\n",
            "Epoch 874/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.8971 - val_accuracy: 1.0000\n",
            "Epoch 875/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9670 - val_accuracy: 1.0000\n",
            "Epoch 876/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.9010 - val_accuracy: 1.0000\n",
            "Epoch 877/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9214 - accuracy: 1.0000 - val_loss: 2.1014 - val_accuracy: 1.0000\n",
            "Epoch 878/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9366 - accuracy: 1.0000 - val_loss: 1.8993 - val_accuracy: 1.0000\n",
            "Epoch 879/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8971 - accuracy: 1.0000 - val_loss: 1.9145 - val_accuracy: 1.0000\n",
            "Epoch 880/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.9198 - val_accuracy: 1.0000\n",
            "Epoch 881/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9274 - accuracy: 1.0000 - val_loss: 1.9148 - val_accuracy: 1.0000\n",
            "Epoch 882/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9041 - accuracy: 1.0000 - val_loss: 1.9028 - val_accuracy: 1.0000\n",
            "Epoch 883/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9212 - accuracy: 1.0000 - val_loss: 1.8891 - val_accuracy: 1.0000\n",
            "Epoch 884/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 1.9284 - val_accuracy: 1.0000\n",
            "Epoch 885/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9091 - accuracy: 1.0000 - val_loss: 1.8932 - val_accuracy: 1.0000\n",
            "Epoch 886/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9278 - val_accuracy: 1.0000\n",
            "Epoch 887/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.9217 - val_accuracy: 1.0000\n",
            "Epoch 888/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9123 - val_accuracy: 1.0000\n",
            "Epoch 889/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9096 - accuracy: 1.0000 - val_loss: 1.8937 - val_accuracy: 1.0000\n",
            "Epoch 890/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 1.8990 - val_accuracy: 1.0000\n",
            "Epoch 891/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9041 - accuracy: 1.0000 - val_loss: 1.9264 - val_accuracy: 1.0000\n",
            "Epoch 892/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9212 - accuracy: 1.0000 - val_loss: 1.9940 - val_accuracy: 1.0000\n",
            "Epoch 893/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9368 - val_accuracy: 1.0000\n",
            "Epoch 894/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.8977 - val_accuracy: 1.0000\n",
            "Epoch 895/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9211 - accuracy: 1.0000 - val_loss: 1.9722 - val_accuracy: 1.0000\n",
            "Epoch 896/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9000 - accuracy: 1.0000 - val_loss: 2.0603 - val_accuracy: 1.0000\n",
            "Epoch 897/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9170 - accuracy: 1.0000 - val_loss: 1.9216 - val_accuracy: 1.0000\n",
            "Epoch 898/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.9206 - val_accuracy: 1.0000\n",
            "Epoch 899/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9239 - accuracy: 1.0000 - val_loss: 1.9165 - val_accuracy: 1.0000\n",
            "Epoch 900/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9015 - accuracy: 1.0000 - val_loss: 1.8972 - val_accuracy: 1.0000\n",
            "Epoch 901/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 1.9030 - val_accuracy: 1.0000\n",
            "Epoch 902/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9215 - accuracy: 1.0000 - val_loss: 2.1285 - val_accuracy: 1.0000\n",
            "Epoch 903/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9142 - accuracy: 1.0000 - val_loss: 1.9355 - val_accuracy: 1.0000\n",
            "Epoch 904/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9071 - accuracy: 1.0000 - val_loss: 1.8955 - val_accuracy: 1.0000\n",
            "Epoch 905/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 2.0363 - val_accuracy: 1.0000\n",
            "Epoch 906/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.9242 - val_accuracy: 1.0000\n",
            "Epoch 907/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9139 - accuracy: 1.0000 - val_loss: 1.8838 - val_accuracy: 1.0000\n",
            "Epoch 908/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.8853 - val_accuracy: 1.0000\n",
            "Epoch 909/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9345 - accuracy: 1.0000 - val_loss: 1.9283 - val_accuracy: 1.0000\n",
            "Epoch 910/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8931 - accuracy: 1.0000 - val_loss: 1.9576 - val_accuracy: 1.0000\n",
            "Epoch 911/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 2.3724 - val_accuracy: 1.0000\n",
            "Epoch 912/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 1.9596 - val_accuracy: 1.0000\n",
            "Epoch 913/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9026 - val_accuracy: 1.0000\n",
            "Epoch 914/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9095 - accuracy: 1.0000 - val_loss: 1.9153 - val_accuracy: 1.0000\n",
            "Epoch 915/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9106 - accuracy: 1.0000 - val_loss: 1.8921 - val_accuracy: 1.0000\n",
            "Epoch 916/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 1.8840 - val_accuracy: 1.0000\n",
            "Epoch 917/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9192 - accuracy: 1.0000 - val_loss: 1.9287 - val_accuracy: 1.0000\n",
            "Epoch 918/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9338 - accuracy: 1.0000 - val_loss: 2.1693 - val_accuracy: 1.0000\n",
            "Epoch 919/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9042 - accuracy: 1.0000 - val_loss: 1.9470 - val_accuracy: 1.0000\n",
            "Epoch 920/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9076 - accuracy: 1.0000 - val_loss: 1.9319 - val_accuracy: 1.0000\n",
            "Epoch 921/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9256 - accuracy: 1.0000 - val_loss: 1.8925 - val_accuracy: 1.0000\n",
            "Epoch 922/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9056 - accuracy: 1.0000 - val_loss: 1.9467 - val_accuracy: 1.0000\n",
            "Epoch 923/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.9233 - val_accuracy: 1.0000\n",
            "Epoch 924/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9144 - val_accuracy: 1.0000\n",
            "Epoch 925/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.9292 - val_accuracy: 1.0000\n",
            "Epoch 926/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9194 - accuracy: 1.0000 - val_loss: 1.8930 - val_accuracy: 1.0000\n",
            "Epoch 927/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9160 - accuracy: 1.0000 - val_loss: 1.9101 - val_accuracy: 1.0000\n",
            "Epoch 928/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.9229 - val_accuracy: 1.0000\n",
            "Epoch 929/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9193 - accuracy: 1.0000 - val_loss: 1.8860 - val_accuracy: 1.0000\n",
            "Epoch 930/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 1.9084 - val_accuracy: 1.0000\n",
            "Epoch 931/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9145 - accuracy: 1.0000 - val_loss: 2.0100 - val_accuracy: 1.0000\n",
            "Epoch 932/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 2.1095 - val_accuracy: 1.0000\n",
            "Epoch 933/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.8907 - val_accuracy: 1.0000\n",
            "Epoch 934/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9285 - accuracy: 1.0000 - val_loss: 1.9444 - val_accuracy: 1.0000\n",
            "Epoch 935/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.8993 - accuracy: 1.0000 - val_loss: 1.9148 - val_accuracy: 1.0000\n",
            "Epoch 936/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.9437 - val_accuracy: 1.0000\n",
            "Epoch 937/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9165 - accuracy: 1.0000 - val_loss: 1.8969 - val_accuracy: 1.0000\n",
            "Epoch 938/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 1.9205 - val_accuracy: 1.0000\n",
            "Epoch 939/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.9975 - val_accuracy: 1.0000\n",
            "Epoch 940/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9175 - accuracy: 1.0000 - val_loss: 1.9567 - val_accuracy: 1.0000\n",
            "Epoch 941/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.9477 - val_accuracy: 1.0000\n",
            "Epoch 942/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9230 - accuracy: 1.0000 - val_loss: 1.9922 - val_accuracy: 1.0000\n",
            "Epoch 943/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 1.9439 - val_accuracy: 1.0000\n",
            "Epoch 944/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.9747 - val_accuracy: 1.0000\n",
            "Epoch 945/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9222 - accuracy: 1.0000 - val_loss: 1.8914 - val_accuracy: 1.0000\n",
            "Epoch 946/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9055 - accuracy: 1.0000 - val_loss: 1.9203 - val_accuracy: 1.0000\n",
            "Epoch 947/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9159 - accuracy: 1.0000 - val_loss: 1.8913 - val_accuracy: 1.0000\n",
            "Epoch 948/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.8912 - val_accuracy: 1.0000\n",
            "Epoch 949/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9192 - accuracy: 1.0000 - val_loss: 1.9414 - val_accuracy: 1.0000\n",
            "Epoch 950/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9258 - accuracy: 1.0000 - val_loss: 1.9248 - val_accuracy: 1.0000\n",
            "Epoch 951/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 2.1936 - val_accuracy: 1.0000\n",
            "Epoch 952/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9020 - accuracy: 1.0000 - val_loss: 2.1258 - val_accuracy: 1.0000\n",
            "Epoch 953/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9236 - accuracy: 1.0000 - val_loss: 1.9374 - val_accuracy: 1.0000\n",
            "Epoch 954/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9128 - accuracy: 1.0000 - val_loss: 1.9202 - val_accuracy: 1.0000\n",
            "Epoch 955/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9323 - accuracy: 1.0000 - val_loss: 2.0868 - val_accuracy: 1.0000\n",
            "Epoch 956/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9021 - accuracy: 1.0000 - val_loss: 1.8902 - val_accuracy: 1.0000\n",
            "Epoch 957/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.8886 - val_accuracy: 1.0000\n",
            "Epoch 958/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 1.9204 - val_accuracy: 1.0000\n",
            "Epoch 959/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9331 - val_accuracy: 1.0000\n",
            "Epoch 960/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9180 - accuracy: 1.0000 - val_loss: 1.9103 - val_accuracy: 1.0000\n",
            "Epoch 961/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9079 - accuracy: 1.0000 - val_loss: 1.8968 - val_accuracy: 1.0000\n",
            "Epoch 962/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.9074 - val_accuracy: 1.0000\n",
            "Epoch 963/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.9888 - val_accuracy: 1.0000\n",
            "Epoch 964/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.8873 - val_accuracy: 1.0000\n",
            "Epoch 965/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9203 - accuracy: 1.0000 - val_loss: 1.8938 - val_accuracy: 1.0000\n",
            "Epoch 966/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8975 - accuracy: 1.0000 - val_loss: 3.8461 - val_accuracy: 1.0000\n",
            "Epoch 967/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 2.0646 - accuracy: 1.0000 - val_loss: 1.9135 - val_accuracy: 1.0000\n",
            "Epoch 968/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8918 - accuracy: 1.0000 - val_loss: 1.9403 - val_accuracy: 1.0000\n",
            "Epoch 969/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.9073 - val_accuracy: 1.0000\n",
            "Epoch 970/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 1.8931 - val_accuracy: 1.0000\n",
            "Epoch 971/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9092 - accuracy: 1.0000 - val_loss: 1.8909 - val_accuracy: 1.0000\n",
            "Epoch 972/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.9178 - val_accuracy: 1.0000\n",
            "Epoch 973/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9197 - accuracy: 1.0000 - val_loss: 1.9068 - val_accuracy: 1.0000\n",
            "Epoch 974/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8997 - accuracy: 1.0000 - val_loss: 1.8940 - val_accuracy: 1.0000\n",
            "Epoch 975/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9273 - accuracy: 1.0000 - val_loss: 1.8986 - val_accuracy: 1.0000\n",
            "Epoch 976/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8980 - accuracy: 1.0000 - val_loss: 1.9136 - val_accuracy: 1.0000\n",
            "Epoch 977/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.8933 - val_accuracy: 1.0000\n",
            "Epoch 978/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 1.9076 - val_accuracy: 1.0000\n",
            "Epoch 979/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9148 - val_accuracy: 1.0000\n",
            "Epoch 980/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9050 - accuracy: 1.0000 - val_loss: 1.9128 - val_accuracy: 1.0000\n",
            "Epoch 981/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9194 - accuracy: 1.0000 - val_loss: 1.9328 - val_accuracy: 1.0000\n",
            "Epoch 982/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 2.0392 - val_accuracy: 1.0000\n",
            "Epoch 983/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9211 - accuracy: 1.0000 - val_loss: 1.9096 - val_accuracy: 1.0000\n",
            "Epoch 984/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9906 - val_accuracy: 1.0000\n",
            "Epoch 985/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.9175 - val_accuracy: 1.0000\n",
            "Epoch 986/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.8830 - val_accuracy: 1.0000\n",
            "Epoch 987/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9102 - accuracy: 1.0000 - val_loss: 2.0111 - val_accuracy: 1.0000\n",
            "Epoch 988/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9217 - accuracy: 1.0000 - val_loss: 2.5677 - val_accuracy: 1.0000\n",
            "Epoch 989/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.8925 - val_accuracy: 1.0000\n",
            "Epoch 990/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8986 - accuracy: 1.0000 - val_loss: 1.8830 - val_accuracy: 1.0000\n",
            "Epoch 991/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9307 - val_accuracy: 1.0000\n",
            "Epoch 992/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9296 - accuracy: 1.0000 - val_loss: 1.8942 - val_accuracy: 1.0000\n",
            "Epoch 993/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8979 - accuracy: 1.0000 - val_loss: 2.0244 - val_accuracy: 1.0000\n",
            "Epoch 994/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9251 - accuracy: 1.0000 - val_loss: 1.9667 - val_accuracy: 1.0000\n",
            "Epoch 995/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9183 - accuracy: 1.0000 - val_loss: 1.9264 - val_accuracy: 1.0000\n",
            "Epoch 996/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9007 - accuracy: 1.0000 - val_loss: 1.8861 - val_accuracy: 1.0000\n",
            "Epoch 997/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9232 - accuracy: 1.0000 - val_loss: 1.8990 - val_accuracy: 1.0000\n",
            "Epoch 998/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9110 - accuracy: 1.0000 - val_loss: 1.9005 - val_accuracy: 1.0000\n",
            "Epoch 999/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.8939 - val_accuracy: 1.0000\n",
            "Epoch 1000/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.8985 - val_accuracy: 1.0000\n",
            "Epoch 1001/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9270 - accuracy: 1.0000 - val_loss: 2.0385 - val_accuracy: 1.0000\n",
            "Epoch 1002/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9079 - accuracy: 1.0000 - val_loss: 1.9023 - val_accuracy: 1.0000\n",
            "Epoch 1003/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.9750 - val_accuracy: 1.0000\n",
            "Epoch 1004/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9162 - accuracy: 1.0000 - val_loss: 1.9636 - val_accuracy: 1.0000\n",
            "Epoch 1005/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 1.9099 - val_accuracy: 1.0000\n",
            "Epoch 1006/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9432 - val_accuracy: 1.0000\n",
            "Epoch 1007/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9110 - accuracy: 1.0000 - val_loss: 1.9370 - val_accuracy: 1.0000\n",
            "Epoch 1008/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9357 - accuracy: 1.0000 - val_loss: 1.9051 - val_accuracy: 1.0000\n",
            "Epoch 1009/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8964 - accuracy: 1.0000 - val_loss: 1.9203 - val_accuracy: 1.0000\n",
            "Epoch 1010/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 1.9449 - val_accuracy: 1.0000\n",
            "Epoch 1011/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9270 - accuracy: 1.0000 - val_loss: 2.0619 - val_accuracy: 1.0000\n",
            "Epoch 1012/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9096 - accuracy: 1.0000 - val_loss: 1.9509 - val_accuracy: 1.0000\n",
            "Epoch 1013/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9219 - accuracy: 1.0000 - val_loss: 1.8878 - val_accuracy: 1.0000\n",
            "Epoch 1014/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9146 - accuracy: 1.0000 - val_loss: 1.8906 - val_accuracy: 1.0000\n",
            "Epoch 1015/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9004 - accuracy: 1.0000 - val_loss: 1.9197 - val_accuracy: 1.0000\n",
            "Epoch 1016/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9212 - accuracy: 1.0000 - val_loss: 1.8994 - val_accuracy: 1.0000\n",
            "Epoch 1017/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.8859 - val_accuracy: 1.0000\n",
            "Epoch 1018/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9274 - accuracy: 1.0000 - val_loss: 1.9488 - val_accuracy: 1.0000\n",
            "Epoch 1019/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9011 - accuracy: 1.0000 - val_loss: 1.9229 - val_accuracy: 1.0000\n",
            "Epoch 1020/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.9303 - val_accuracy: 1.0000\n",
            "Epoch 1021/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9074 - accuracy: 1.0000 - val_loss: 2.0135 - val_accuracy: 1.0000\n",
            "Epoch 1022/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9142 - accuracy: 1.0000 - val_loss: 1.9119 - val_accuracy: 1.0000\n",
            "Epoch 1023/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.8944 - val_accuracy: 1.0000\n",
            "Epoch 1024/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.9018 - val_accuracy: 1.0000\n",
            "Epoch 1025/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 2.0129 - val_accuracy: 1.0000\n",
            "Epoch 1026/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9107 - accuracy: 1.0000 - val_loss: 1.8923 - val_accuracy: 1.0000\n",
            "Epoch 1027/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9087 - accuracy: 1.0000 - val_loss: 2.0188 - val_accuracy: 1.0000\n",
            "Epoch 1028/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 1.0000\n",
            "Epoch 1029/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9297 - accuracy: 1.0000 - val_loss: 1.8891 - val_accuracy: 1.0000\n",
            "Epoch 1030/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8999 - accuracy: 1.0000 - val_loss: 1.8933 - val_accuracy: 1.0000\n",
            "Epoch 1031/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.9168 - val_accuracy: 1.0000\n",
            "Epoch 1032/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 2.1477 - val_accuracy: 1.0000\n",
            "Epoch 1033/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9135 - val_accuracy: 1.0000\n",
            "Epoch 1034/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9062 - accuracy: 1.0000 - val_loss: 1.9249 - val_accuracy: 1.0000\n",
            "Epoch 1035/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 1.9393 - val_accuracy: 1.0000\n",
            "Epoch 1036/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9073 - accuracy: 1.0000 - val_loss: 1.9551 - val_accuracy: 1.0000\n",
            "Epoch 1037/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.8839 - val_accuracy: 1.0000\n",
            "Epoch 1038/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9073 - accuracy: 1.0000 - val_loss: 1.9946 - val_accuracy: 1.0000\n",
            "Epoch 1039/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9128 - accuracy: 1.0000 - val_loss: 1.9124 - val_accuracy: 1.0000\n",
            "Epoch 1040/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9027 - accuracy: 1.0000 - val_loss: 1.9624 - val_accuracy: 1.0000\n",
            "Epoch 1041/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9201 - accuracy: 1.0000 - val_loss: 1.8941 - val_accuracy: 1.0000\n",
            "Epoch 1042/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9071 - accuracy: 1.0000 - val_loss: 1.8921 - val_accuracy: 1.0000\n",
            "Epoch 1043/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9145 - accuracy: 1.0000 - val_loss: 1.8958 - val_accuracy: 1.0000\n",
            "Epoch 1044/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9223 - accuracy: 1.0000 - val_loss: 1.9265 - val_accuracy: 1.0000\n",
            "Epoch 1045/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9054 - accuracy: 1.0000 - val_loss: 1.9079 - val_accuracy: 1.0000\n",
            "Epoch 1046/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 1.9001 - val_accuracy: 1.0000\n",
            "Epoch 1047/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9380 - accuracy: 1.0000 - val_loss: 1.9167 - val_accuracy: 1.0000\n",
            "Epoch 1048/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8945 - accuracy: 1.0000 - val_loss: 1.9246 - val_accuracy: 1.0000\n",
            "Epoch 1049/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9181 - accuracy: 1.0000 - val_loss: 1.8935 - val_accuracy: 1.0000\n",
            "Epoch 1050/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9081 - accuracy: 1.0000 - val_loss: 1.8856 - val_accuracy: 1.0000\n",
            "Epoch 1051/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.8820 - val_accuracy: 1.0000\n",
            "Epoch 1052/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 1.9612 - val_accuracy: 1.0000\n",
            "Epoch 1053/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9028 - accuracy: 1.0000 - val_loss: 1.9563 - val_accuracy: 1.0000\n",
            "Epoch 1054/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9109 - accuracy: 1.0000 - val_loss: 2.0181 - val_accuracy: 1.0000\n",
            "Epoch 1055/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.9382 - val_accuracy: 1.0000\n",
            "Epoch 1056/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9095 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 1.0000\n",
            "Epoch 1057/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9382 - val_accuracy: 1.0000\n",
            "Epoch 1058/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9124 - accuracy: 1.0000 - val_loss: 2.0141 - val_accuracy: 1.0000\n",
            "Epoch 1059/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9222 - accuracy: 1.0000 - val_loss: 1.9233 - val_accuracy: 1.0000\n",
            "Epoch 1060/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9017 - accuracy: 1.0000 - val_loss: 1.8879 - val_accuracy: 1.0000\n",
            "Epoch 1061/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9507 - accuracy: 1.0000 - val_loss: 1.9051 - val_accuracy: 1.0000\n",
            "Epoch 1062/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8911 - accuracy: 1.0000 - val_loss: 1.8945 - val_accuracy: 1.0000\n",
            "Epoch 1063/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8996 - accuracy: 1.0000 - val_loss: 1.9062 - val_accuracy: 1.0000\n",
            "Epoch 1064/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.9523 - val_accuracy: 1.0000\n",
            "Epoch 1065/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.9263 - val_accuracy: 1.0000\n",
            "Epoch 1066/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.9160 - val_accuracy: 1.0000\n",
            "Epoch 1067/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.9162 - val_accuracy: 1.0000\n",
            "Epoch 1068/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9037 - val_accuracy: 1.0000\n",
            "Epoch 1069/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 1.9821 - val_accuracy: 1.0000\n",
            "Epoch 1070/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.9018 - val_accuracy: 1.0000\n",
            "Epoch 1071/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.9064 - val_accuracy: 1.0000\n",
            "Epoch 1072/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.8901 - val_accuracy: 1.0000\n",
            "Epoch 1073/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9074 - accuracy: 1.0000 - val_loss: 2.0689 - val_accuracy: 1.0000\n",
            "Epoch 1074/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9280 - accuracy: 1.0000 - val_loss: 2.1147 - val_accuracy: 1.0000\n",
            "Epoch 1075/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9038 - accuracy: 1.0000 - val_loss: 1.8874 - val_accuracy: 1.0000\n",
            "Epoch 1076/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.9256 - val_accuracy: 1.0000\n",
            "Epoch 1077/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9094 - accuracy: 1.0000 - val_loss: 1.8945 - val_accuracy: 1.0000\n",
            "Epoch 1078/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9047 - accuracy: 1.0000 - val_loss: 1.9097 - val_accuracy: 1.0000\n",
            "Epoch 1079/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.9142 - val_accuracy: 1.0000\n",
            "Epoch 1080/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9078 - accuracy: 1.0000 - val_loss: 1.9301 - val_accuracy: 1.0000\n",
            "Epoch 1081/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9289 - accuracy: 1.0000 - val_loss: 1.9568 - val_accuracy: 1.0000\n",
            "Epoch 1082/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9031 - accuracy: 1.0000 - val_loss: 1.9602 - val_accuracy: 1.0000\n",
            "Epoch 1083/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 2.0115 - val_accuracy: 1.0000\n",
            "Epoch 1084/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.9442 - val_accuracy: 1.0000\n",
            "Epoch 1085/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9116 - accuracy: 1.0000 - val_loss: 1.9039 - val_accuracy: 1.0000\n",
            "Epoch 1086/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9215 - accuracy: 1.0000 - val_loss: 1.9009 - val_accuracy: 1.0000\n",
            "Epoch 1087/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9002 - accuracy: 1.0000 - val_loss: 1.9240 - val_accuracy: 1.0000\n",
            "Epoch 1088/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9257 - accuracy: 1.0000 - val_loss: 1.9026 - val_accuracy: 1.0000\n",
            "Epoch 1089/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9018 - accuracy: 1.0000 - val_loss: 2.0410 - val_accuracy: 1.0000\n",
            "Epoch 1090/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.9070 - val_accuracy: 1.0000\n",
            "Epoch 1091/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.8916 - val_accuracy: 1.0000\n",
            "Epoch 1092/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9178 - accuracy: 1.0000 - val_loss: 1.9093 - val_accuracy: 1.0000\n",
            "Epoch 1093/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9097 - accuracy: 1.0000 - val_loss: 1.9292 - val_accuracy: 1.0000\n",
            "Epoch 1094/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 2.0139 - val_accuracy: 1.0000\n",
            "Epoch 1095/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.8848 - val_accuracy: 1.0000\n",
            "Epoch 1096/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9067 - accuracy: 1.0000 - val_loss: 1.8861 - val_accuracy: 1.0000\n",
            "Epoch 1097/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.9457 - val_accuracy: 1.0000\n",
            "Epoch 1098/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9123 - accuracy: 1.0000 - val_loss: 1.9703 - val_accuracy: 1.0000\n",
            "Epoch 1099/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.8972 - val_accuracy: 1.0000\n",
            "Epoch 1100/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.8843 - val_accuracy: 1.0000\n",
            "Epoch 1101/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9230 - accuracy: 1.0000 - val_loss: 1.9397 - val_accuracy: 1.0000\n",
            "Epoch 1102/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9639 - val_accuracy: 1.0000\n",
            "Epoch 1103/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9066 - accuracy: 1.0000 - val_loss: 1.9098 - val_accuracy: 1.0000\n",
            "Epoch 1104/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9146 - accuracy: 1.0000 - val_loss: 1.9188 - val_accuracy: 1.0000\n",
            "Epoch 1105/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.9664 - val_accuracy: 1.0000\n",
            "Epoch 1106/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9040 - accuracy: 1.0000 - val_loss: 2.0531 - val_accuracy: 1.0000\n",
            "Epoch 1107/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.8861 - val_accuracy: 1.0000\n",
            "Epoch 1108/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.8921 - val_accuracy: 1.0000\n",
            "Epoch 1109/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8973 - accuracy: 1.0000 - val_loss: 1.9182 - val_accuracy: 1.0000\n",
            "Epoch 1110/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9162 - accuracy: 1.0000 - val_loss: 1.9423 - val_accuracy: 1.0000\n",
            "Epoch 1111/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.9144 - val_accuracy: 1.0000\n",
            "Epoch 1112/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 1.8871 - val_accuracy: 1.0000\n",
            "Epoch 1113/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9056 - accuracy: 1.0000 - val_loss: 1.8818 - val_accuracy: 1.0000\n",
            "Epoch 1114/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9113 - accuracy: 1.0000 - val_loss: 1.8874 - val_accuracy: 1.0000\n",
            "Epoch 1115/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9197 - accuracy: 1.0000 - val_loss: 1.9674 - val_accuracy: 1.0000\n",
            "Epoch 1116/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9045 - accuracy: 1.0000 - val_loss: 1.8889 - val_accuracy: 1.0000\n",
            "Epoch 1117/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9175 - accuracy: 1.0000 - val_loss: 1.9164 - val_accuracy: 1.0000\n",
            "Epoch 1118/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9122 - accuracy: 1.0000 - val_loss: 1.9303 - val_accuracy: 1.0000\n",
            "Epoch 1119/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9274 - accuracy: 1.0000 - val_loss: 1.9347 - val_accuracy: 1.0000\n",
            "Epoch 1120/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8947 - accuracy: 1.0000 - val_loss: 1.9101 - val_accuracy: 1.0000\n",
            "Epoch 1121/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9343 - val_accuracy: 1.0000\n",
            "Epoch 1122/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9109 - accuracy: 1.0000 - val_loss: 1.8818 - val_accuracy: 1.0000\n",
            "Epoch 1123/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9088 - accuracy: 1.0000 - val_loss: 1.9090 - val_accuracy: 1.0000\n",
            "Epoch 1124/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 2.0552 - val_accuracy: 1.0000\n",
            "Epoch 1125/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.9577 - val_accuracy: 1.0000\n",
            "Epoch 1126/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.8962 - val_accuracy: 1.0000\n",
            "Epoch 1127/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9092 - accuracy: 1.0000 - val_loss: 1.9167 - val_accuracy: 1.0000\n",
            "Epoch 1128/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9081 - accuracy: 1.0000 - val_loss: 1.9185 - val_accuracy: 1.0000\n",
            "Epoch 1129/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.9065 - val_accuracy: 1.0000\n",
            "Epoch 1130/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.8856 - val_accuracy: 1.0000\n",
            "Epoch 1131/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9091 - accuracy: 1.0000 - val_loss: 1.8862 - val_accuracy: 1.0000\n",
            "Epoch 1132/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9124 - accuracy: 1.0000 - val_loss: 2.2085 - val_accuracy: 1.0000\n",
            "Epoch 1133/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9376 - accuracy: 1.0000 - val_loss: 1.9016 - val_accuracy: 1.0000\n",
            "Epoch 1134/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9095 - accuracy: 1.0000 - val_loss: 1.8821 - val_accuracy: 1.0000\n",
            "Epoch 1135/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 1.9061 - val_accuracy: 1.0000\n",
            "Epoch 1136/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.9605 - val_accuracy: 1.0000\n",
            "Epoch 1137/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9040 - accuracy: 1.0000 - val_loss: 1.9145 - val_accuracy: 1.0000\n",
            "Epoch 1138/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.8870 - val_accuracy: 1.0000\n",
            "Epoch 1139/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9098 - accuracy: 1.0000 - val_loss: 1.8891 - val_accuracy: 1.0000\n",
            "Epoch 1140/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.8943 - val_accuracy: 1.0000\n",
            "Epoch 1141/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9084 - accuracy: 1.0000 - val_loss: 1.9131 - val_accuracy: 1.0000\n",
            "Epoch 1142/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.9076 - val_accuracy: 1.0000\n",
            "Epoch 1143/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9095 - accuracy: 1.0000 - val_loss: 1.9274 - val_accuracy: 1.0000\n",
            "Epoch 1144/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9233 - accuracy: 1.0000 - val_loss: 1.9088 - val_accuracy: 1.0000\n",
            "Epoch 1145/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 2.0687 - val_accuracy: 1.0000\n",
            "Epoch 1146/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9020 - accuracy: 1.0000 - val_loss: 1.9157 - val_accuracy: 1.0000\n",
            "Epoch 1147/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.9155 - val_accuracy: 1.0000\n",
            "Epoch 1148/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9044 - val_accuracy: 1.0000\n",
            "Epoch 1149/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9089 - accuracy: 1.0000 - val_loss: 1.9775 - val_accuracy: 1.0000\n",
            "Epoch 1150/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9223 - accuracy: 1.0000 - val_loss: 2.3557 - val_accuracy: 1.0000\n",
            "Epoch 1151/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9182 - accuracy: 1.0000 - val_loss: 1.9071 - val_accuracy: 1.0000\n",
            "Epoch 1152/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 1.0000\n",
            "Epoch 1153/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.8958 - val_accuracy: 1.0000\n",
            "Epoch 1154/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9050 - accuracy: 1.0000 - val_loss: 1.9064 - val_accuracy: 1.0000\n",
            "Epoch 1155/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.9823 - val_accuracy: 1.0000\n",
            "Epoch 1156/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9107 - accuracy: 1.0000 - val_loss: 1.9313 - val_accuracy: 1.0000\n",
            "Epoch 1157/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9180 - accuracy: 1.0000 - val_loss: 1.9282 - val_accuracy: 1.0000\n",
            "Epoch 1158/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9129 - accuracy: 1.0000 - val_loss: 1.9333 - val_accuracy: 1.0000\n",
            "Epoch 1159/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9128 - accuracy: 1.0000 - val_loss: 1.8879 - val_accuracy: 1.0000\n",
            "Epoch 1160/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.8995 - val_accuracy: 1.0000\n",
            "Epoch 1161/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9067 - accuracy: 1.0000 - val_loss: 1.8996 - val_accuracy: 1.0000\n",
            "Epoch 1162/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.9397 - val_accuracy: 1.0000\n",
            "Epoch 1163/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8913 - accuracy: 1.0000 - val_loss: 1.8946 - val_accuracy: 1.0000\n",
            "Epoch 1164/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9207 - accuracy: 1.0000 - val_loss: 1.9995 - val_accuracy: 1.0000\n",
            "Epoch 1165/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 1.9384 - val_accuracy: 1.0000\n",
            "Epoch 1166/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9035 - val_accuracy: 1.0000\n",
            "Epoch 1167/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9082 - accuracy: 1.0000 - val_loss: 1.9102 - val_accuracy: 1.0000\n",
            "Epoch 1168/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.9829 - val_accuracy: 1.0000\n",
            "Epoch 1169/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9047 - val_accuracy: 1.0000\n",
            "Epoch 1170/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9124 - val_accuracy: 1.0000\n",
            "Epoch 1171/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 1.9261 - val_accuracy: 1.0000\n",
            "Epoch 1172/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.9791 - val_accuracy: 1.0000\n",
            "Epoch 1173/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.9061 - val_accuracy: 1.0000\n",
            "Epoch 1174/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9291 - accuracy: 1.0000 - val_loss: 1.9035 - val_accuracy: 1.0000\n",
            "Epoch 1175/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8933 - accuracy: 1.0000 - val_loss: 1.9299 - val_accuracy: 1.0000\n",
            "Epoch 1176/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9884 - val_accuracy: 1.0000\n",
            "Epoch 1177/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.9004 - val_accuracy: 1.0000\n",
            "Epoch 1178/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9127 - accuracy: 1.0000 - val_loss: 1.9657 - val_accuracy: 1.0000\n",
            "Epoch 1179/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.9699 - val_accuracy: 1.0000\n",
            "Epoch 1180/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.9140 - val_accuracy: 1.0000\n",
            "Epoch 1181/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9021 - accuracy: 1.0000 - val_loss: 1.8901 - val_accuracy: 1.0000\n",
            "Epoch 1182/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.8958 - val_accuracy: 1.0000\n",
            "Epoch 1183/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9335 - val_accuracy: 1.0000\n",
            "Epoch 1184/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9159 - accuracy: 1.0000 - val_loss: 1.8969 - val_accuracy: 1.0000\n",
            "Epoch 1185/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9083 - accuracy: 1.0000 - val_loss: 1.9863 - val_accuracy: 1.0000\n",
            "Epoch 1186/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.9154 - val_accuracy: 1.0000\n",
            "Epoch 1187/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9084 - accuracy: 1.0000 - val_loss: 2.1312 - val_accuracy: 1.0000\n",
            "Epoch 1188/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 1.9029 - val_accuracy: 1.0000\n",
            "Epoch 1189/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 2.1075 - val_accuracy: 1.0000\n",
            "Epoch 1190/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 1.9207 - val_accuracy: 1.0000\n",
            "Epoch 1191/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 2.0535 - val_accuracy: 1.0000\n",
            "Epoch 1192/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.9342 - val_accuracy: 1.0000\n",
            "Epoch 1193/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9170 - accuracy: 1.0000 - val_loss: 1.9558 - val_accuracy: 1.0000\n",
            "Epoch 1194/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.8902 - val_accuracy: 1.0000\n",
            "Epoch 1195/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.9094 - val_accuracy: 1.0000\n",
            "Epoch 1196/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9074 - accuracy: 1.0000 - val_loss: 1.9376 - val_accuracy: 1.0000\n",
            "Epoch 1197/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9204 - val_accuracy: 1.0000\n",
            "Epoch 1198/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.8938 - val_accuracy: 1.0000\n",
            "Epoch 1199/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9258 - accuracy: 1.0000 - val_loss: 2.0824 - val_accuracy: 1.0000\n",
            "Epoch 1200/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8968 - accuracy: 1.0000 - val_loss: 1.9069 - val_accuracy: 1.0000\n",
            "Epoch 1201/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 1.9463 - val_accuracy: 1.0000\n",
            "Epoch 1202/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 1.8955 - val_accuracy: 1.0000\n",
            "Epoch 1203/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.8866 - val_accuracy: 1.0000\n",
            "Epoch 1204/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.8873 - val_accuracy: 1.0000\n",
            "Epoch 1205/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9183 - accuracy: 1.0000 - val_loss: 1.8833 - val_accuracy: 1.0000\n",
            "Epoch 1206/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9054 - accuracy: 1.0000 - val_loss: 1.9004 - val_accuracy: 1.0000\n",
            "Epoch 1207/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9052 - val_accuracy: 1.0000\n",
            "Epoch 1208/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9107 - accuracy: 1.0000 - val_loss: 1.9085 - val_accuracy: 1.0000\n",
            "Epoch 1209/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9073 - accuracy: 1.0000 - val_loss: 1.9417 - val_accuracy: 1.0000\n",
            "Epoch 1210/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9151 - val_accuracy: 1.0000\n",
            "Epoch 1211/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 1.9114 - val_accuracy: 1.0000\n",
            "Epoch 1212/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9074 - accuracy: 1.0000 - val_loss: 1.9107 - val_accuracy: 1.0000\n",
            "Epoch 1213/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 1.9063 - val_accuracy: 1.0000\n",
            "Epoch 1214/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9055 - accuracy: 1.0000 - val_loss: 1.9097 - val_accuracy: 1.0000\n",
            "Epoch 1215/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.9017 - val_accuracy: 1.0000\n",
            "Epoch 1216/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9081 - accuracy: 1.0000 - val_loss: 1.9020 - val_accuracy: 1.0000\n",
            "Epoch 1217/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9239 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 1.0000\n",
            "Epoch 1218/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8976 - accuracy: 1.0000 - val_loss: 1.9100 - val_accuracy: 1.0000\n",
            "Epoch 1219/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9084 - accuracy: 1.0000 - val_loss: 1.8902 - val_accuracy: 1.0000\n",
            "Epoch 1220/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9133 - accuracy: 1.0000 - val_loss: 1.9793 - val_accuracy: 1.0000\n",
            "Epoch 1221/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9070 - accuracy: 1.0000 - val_loss: 1.9139 - val_accuracy: 1.0000\n",
            "Epoch 1222/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.9185 - val_accuracy: 1.0000\n",
            "Epoch 1223/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.9136 - val_accuracy: 1.0000\n",
            "Epoch 1224/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.9896 - val_accuracy: 1.0000\n",
            "Epoch 1225/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9146 - accuracy: 1.0000 - val_loss: 2.1068 - val_accuracy: 1.0000\n",
            "Epoch 1226/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9098 - accuracy: 1.0000 - val_loss: 1.9811 - val_accuracy: 1.0000\n",
            "Epoch 1227/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9851 - val_accuracy: 1.0000\n",
            "Epoch 1228/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.8938 - val_accuracy: 1.0000\n",
            "Epoch 1229/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9106 - accuracy: 1.0000 - val_loss: 1.9880 - val_accuracy: 1.0000\n",
            "Epoch 1230/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9215 - accuracy: 1.0000 - val_loss: 1.9065 - val_accuracy: 1.0000\n",
            "Epoch 1231/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.8978 - accuracy: 1.0000 - val_loss: 1.9347 - val_accuracy: 1.0000\n",
            "Epoch 1232/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.9013 - val_accuracy: 1.0000\n",
            "Epoch 1233/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.9356 - val_accuracy: 1.0000\n",
            "Epoch 1234/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9142 - accuracy: 1.0000 - val_loss: 1.9022 - val_accuracy: 1.0000\n",
            "Epoch 1235/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9037 - accuracy: 1.0000 - val_loss: 1.9645 - val_accuracy: 1.0000\n",
            "Epoch 1236/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9096 - accuracy: 1.0000 - val_loss: 1.9658 - val_accuracy: 1.0000\n",
            "Epoch 1237/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9160 - accuracy: 1.0000 - val_loss: 1.9044 - val_accuracy: 1.0000\n",
            "Epoch 1238/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.8828 - val_accuracy: 1.0000\n",
            "Epoch 1239/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9050 - accuracy: 1.0000 - val_loss: 1.9421 - val_accuracy: 1.0000\n",
            "Epoch 1240/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.8922 - val_accuracy: 1.0000\n",
            "Epoch 1241/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.8824 - val_accuracy: 1.0000\n",
            "Epoch 1242/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9182 - accuracy: 1.0000 - val_loss: 1.8952 - val_accuracy: 1.0000\n",
            "Epoch 1243/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 1.9542 - val_accuracy: 1.0000\n",
            "Epoch 1244/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9056 - accuracy: 1.0000 - val_loss: 1.8948 - val_accuracy: 1.0000\n",
            "Epoch 1245/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.8876 - val_accuracy: 1.0000\n",
            "Epoch 1246/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9012 - accuracy: 1.0000 - val_loss: 2.1508 - val_accuracy: 1.0000\n",
            "Epoch 1247/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.9126 - val_accuracy: 1.0000\n",
            "Epoch 1248/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 1.9139 - val_accuracy: 1.0000\n",
            "Epoch 1249/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9107 - accuracy: 1.0000 - val_loss: 1.9024 - val_accuracy: 1.0000\n",
            "Epoch 1250/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 1.9047 - val_accuracy: 1.0000\n",
            "Epoch 1251/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9050 - accuracy: 1.0000 - val_loss: 1.9066 - val_accuracy: 1.0000\n",
            "Epoch 1252/2500\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.9141 - accuracy: 1.0000 - val_loss: 1.9788 - val_accuracy: 1.0000\n",
            "Epoch 1253/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9032 - accuracy: 1.0000 - val_loss: 1.9471 - val_accuracy: 1.0000\n",
            "Epoch 1254/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.8981 - val_accuracy: 1.0000\n",
            "Epoch 1255/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9026 - accuracy: 1.0000 - val_loss: 1.8859 - val_accuracy: 1.0000\n",
            "Epoch 1256/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9284 - accuracy: 1.0000 - val_loss: 1.9527 - val_accuracy: 1.0000\n",
            "Epoch 1257/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8988 - accuracy: 1.0000 - val_loss: 1.9493 - val_accuracy: 1.0000\n",
            "Epoch 1258/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9108 - accuracy: 1.0000 - val_loss: 1.9186 - val_accuracy: 1.0000\n",
            "Epoch 1259/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.9280 - val_accuracy: 1.0000\n",
            "Epoch 1260/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9196 - accuracy: 1.0000 - val_loss: 1.9463 - val_accuracy: 1.0000\n",
            "Epoch 1261/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9019 - accuracy: 1.0000 - val_loss: 1.8826 - val_accuracy: 1.0000\n",
            "Epoch 1262/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9108 - accuracy: 1.0000 - val_loss: 1.9010 - val_accuracy: 1.0000\n",
            "Epoch 1263/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9836 - val_accuracy: 1.0000\n",
            "Epoch 1264/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9076 - accuracy: 1.0000 - val_loss: 1.9210 - val_accuracy: 1.0000\n",
            "Epoch 1265/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.9687 - val_accuracy: 1.0000\n",
            "Epoch 1266/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.8897 - val_accuracy: 1.0000\n",
            "Epoch 1267/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9098 - accuracy: 1.0000 - val_loss: 1.8933 - val_accuracy: 1.0000\n",
            "Epoch 1268/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9054 - accuracy: 1.0000 - val_loss: 1.9256 - val_accuracy: 1.0000\n",
            "Epoch 1269/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 1.9074 - val_accuracy: 1.0000\n",
            "Epoch 1270/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.9114 - val_accuracy: 1.0000\n",
            "Epoch 1271/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9110 - accuracy: 1.0000 - val_loss: 1.9594 - val_accuracy: 1.0000\n",
            "Epoch 1272/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8977 - accuracy: 1.0000 - val_loss: 1.8869 - val_accuracy: 1.0000\n",
            "Epoch 1273/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9201 - accuracy: 1.0000 - val_loss: 1.8837 - val_accuracy: 1.0000\n",
            "Epoch 1274/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9023 - accuracy: 1.0000 - val_loss: 1.8943 - val_accuracy: 1.0000\n",
            "Epoch 1275/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9110 - accuracy: 1.0000 - val_loss: 1.8939 - val_accuracy: 1.0000\n",
            "Epoch 1276/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9081 - accuracy: 1.0000 - val_loss: 1.9591 - val_accuracy: 1.0000\n",
            "Epoch 1277/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.9089 - val_accuracy: 1.0000\n",
            "Epoch 1278/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.9358 - val_accuracy: 1.0000\n",
            "Epoch 1279/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.9124 - val_accuracy: 1.0000\n",
            "Epoch 1280/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 1.9509 - val_accuracy: 1.0000\n",
            "Epoch 1281/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9048 - accuracy: 1.0000 - val_loss: 1.8883 - val_accuracy: 1.0000\n",
            "Epoch 1282/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.8916 - val_accuracy: 1.0000\n",
            "Epoch 1283/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.8847 - val_accuracy: 1.0000\n",
            "Epoch 1284/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9124 - accuracy: 1.0000 - val_loss: 1.9105 - val_accuracy: 1.0000\n",
            "Epoch 1285/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9128 - accuracy: 1.0000 - val_loss: 1.8779 - val_accuracy: 1.0000\n",
            "Epoch 1286/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.8959 - val_accuracy: 1.0000\n",
            "Epoch 1287/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9005 - accuracy: 1.0000 - val_loss: 1.9079 - val_accuracy: 1.0000\n",
            "Epoch 1288/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.9039 - val_accuracy: 1.0000\n",
            "Epoch 1289/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.8923 - val_accuracy: 1.0000\n",
            "Epoch 1290/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9047 - accuracy: 1.0000 - val_loss: 1.8937 - val_accuracy: 1.0000\n",
            "Epoch 1291/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9109 - accuracy: 1.0000 - val_loss: 1.9057 - val_accuracy: 1.0000\n",
            "Epoch 1292/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9064 - accuracy: 1.0000 - val_loss: 1.8851 - val_accuracy: 1.0000\n",
            "Epoch 1293/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9107 - accuracy: 1.0000 - val_loss: 1.9255 - val_accuracy: 1.0000\n",
            "Epoch 1294/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9021 - accuracy: 1.0000 - val_loss: 1.8859 - val_accuracy: 1.0000\n",
            "Epoch 1295/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 1.9184 - val_accuracy: 1.0000\n",
            "Epoch 1296/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9167 - accuracy: 1.0000 - val_loss: 1.9195 - val_accuracy: 1.0000\n",
            "Epoch 1297/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.8857 - val_accuracy: 1.0000\n",
            "Epoch 1298/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9627 - val_accuracy: 1.0000\n",
            "Epoch 1299/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 1.8850 - val_accuracy: 1.0000\n",
            "Epoch 1300/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.9213 - val_accuracy: 1.0000\n",
            "Epoch 1301/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9180 - accuracy: 1.0000 - val_loss: 1.9230 - val_accuracy: 1.0000\n",
            "Epoch 1302/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9094 - accuracy: 1.0000 - val_loss: 1.8972 - val_accuracy: 1.0000\n",
            "Epoch 1303/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.9385 - val_accuracy: 1.0000\n",
            "Epoch 1304/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9127 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 1.0000\n",
            "Epoch 1305/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9044 - accuracy: 1.0000 - val_loss: 1.9326 - val_accuracy: 1.0000\n",
            "Epoch 1306/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9231 - val_accuracy: 1.0000\n",
            "Epoch 1307/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9102 - accuracy: 1.0000 - val_loss: 1.8913 - val_accuracy: 1.0000\n",
            "Epoch 1308/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9038 - accuracy: 1.0000 - val_loss: 1.9684 - val_accuracy: 1.0000\n",
            "Epoch 1309/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 1.9001 - val_accuracy: 1.0000\n",
            "Epoch 1310/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9561 - val_accuracy: 1.0000\n",
            "Epoch 1311/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9232 - accuracy: 1.0000 - val_loss: 2.3041 - val_accuracy: 1.0000\n",
            "Epoch 1312/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.9143 - val_accuracy: 1.0000\n",
            "Epoch 1313/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 2.1939 - val_accuracy: 1.0000\n",
            "Epoch 1314/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9139 - accuracy: 1.0000 - val_loss: 2.1056 - val_accuracy: 1.0000\n",
            "Epoch 1315/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9165 - accuracy: 1.0000 - val_loss: 1.8944 - val_accuracy: 1.0000\n",
            "Epoch 1316/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8974 - accuracy: 1.0000 - val_loss: 1.9418 - val_accuracy: 1.0000\n",
            "Epoch 1317/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.8935 - val_accuracy: 1.0000\n",
            "Epoch 1318/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9129 - accuracy: 1.0000 - val_loss: 1.8969 - val_accuracy: 1.0000\n",
            "Epoch 1319/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8984 - accuracy: 1.0000 - val_loss: 1.8870 - val_accuracy: 1.0000\n",
            "Epoch 1320/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9129 - accuracy: 1.0000 - val_loss: 2.0184 - val_accuracy: 1.0000\n",
            "Epoch 1321/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.9156 - val_accuracy: 1.0000\n",
            "Epoch 1322/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9091 - accuracy: 1.0000 - val_loss: 1.9163 - val_accuracy: 1.0000\n",
            "Epoch 1323/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 1.9009 - val_accuracy: 1.0000\n",
            "Epoch 1324/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8989 - accuracy: 1.0000 - val_loss: 1.9030 - val_accuracy: 1.0000\n",
            "Epoch 1325/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.8908 - val_accuracy: 1.0000\n",
            "Epoch 1326/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9032 - accuracy: 1.0000 - val_loss: 1.8823 - val_accuracy: 1.0000\n",
            "Epoch 1327/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.9324 - val_accuracy: 1.0000\n",
            "Epoch 1328/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9069 - val_accuracy: 1.0000\n",
            "Epoch 1329/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 1.9947 - val_accuracy: 1.0000\n",
            "Epoch 1330/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9043 - accuracy: 1.0000 - val_loss: 1.9174 - val_accuracy: 1.0000\n",
            "Epoch 1331/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9053 - val_accuracy: 1.0000\n",
            "Epoch 1332/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.8956 - val_accuracy: 1.0000\n",
            "Epoch 1333/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 1.9357 - val_accuracy: 1.0000\n",
            "Epoch 1334/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9129 - accuracy: 1.0000 - val_loss: 1.9130 - val_accuracy: 1.0000\n",
            "Epoch 1335/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9063 - accuracy: 1.0000 - val_loss: 1.9069 - val_accuracy: 1.0000\n",
            "Epoch 1336/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9094 - accuracy: 1.0000 - val_loss: 1.8788 - val_accuracy: 1.0000\n",
            "Epoch 1337/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9115 - accuracy: 1.0000 - val_loss: 1.8972 - val_accuracy: 1.0000\n",
            "Epoch 1338/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9017 - accuracy: 1.0000 - val_loss: 1.9052 - val_accuracy: 1.0000\n",
            "Epoch 1339/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9129 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 1.0000\n",
            "Epoch 1340/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.8974 - val_accuracy: 1.0000\n",
            "Epoch 1341/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9105 - accuracy: 1.0000 - val_loss: 1.9546 - val_accuracy: 1.0000\n",
            "Epoch 1342/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9227 - accuracy: 1.0000 - val_loss: 1.8971 - val_accuracy: 1.0000\n",
            "Epoch 1343/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8974 - accuracy: 1.0000 - val_loss: 1.9816 - val_accuracy: 1.0000\n",
            "Epoch 1344/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.9525 - val_accuracy: 1.0000\n",
            "Epoch 1345/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9146 - accuracy: 1.0000 - val_loss: 1.8987 - val_accuracy: 1.0000\n",
            "Epoch 1346/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9041 - accuracy: 1.0000 - val_loss: 3.9229 - val_accuracy: 1.0000\n",
            "Epoch 1347/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9712 - accuracy: 1.0000 - val_loss: 1.8928 - val_accuracy: 1.0000\n",
            "Epoch 1348/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9160 - accuracy: 1.0000 - val_loss: 1.9058 - val_accuracy: 1.0000\n",
            "Epoch 1349/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8941 - accuracy: 1.0000 - val_loss: 1.9027 - val_accuracy: 1.0000\n",
            "Epoch 1350/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9026 - accuracy: 1.0000 - val_loss: 1.9022 - val_accuracy: 1.0000\n",
            "Epoch 1351/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 1.9387 - val_accuracy: 1.0000\n",
            "Epoch 1352/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9162 - accuracy: 1.0000 - val_loss: 1.9637 - val_accuracy: 1.0000\n",
            "Epoch 1353/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9165 - accuracy: 1.0000 - val_loss: 1.9045 - val_accuracy: 1.0000\n",
            "Epoch 1354/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8952 - accuracy: 1.0000 - val_loss: 1.9204 - val_accuracy: 1.0000\n",
            "Epoch 1355/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9056 - val_accuracy: 1.0000\n",
            "Epoch 1356/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9028 - accuracy: 1.0000 - val_loss: 2.0208 - val_accuracy: 1.0000\n",
            "Epoch 1357/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 1.9664 - val_accuracy: 1.0000\n",
            "Epoch 1358/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9052 - accuracy: 1.0000 - val_loss: 1.9375 - val_accuracy: 1.0000\n",
            "Epoch 1359/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.8970 - val_accuracy: 1.0000\n",
            "Epoch 1360/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9004 - accuracy: 1.0000 - val_loss: 1.9718 - val_accuracy: 1.0000\n",
            "Epoch 1361/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9133 - accuracy: 1.0000 - val_loss: 1.9419 - val_accuracy: 1.0000\n",
            "Epoch 1362/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9050 - accuracy: 1.0000 - val_loss: 1.9109 - val_accuracy: 1.0000\n",
            "Epoch 1363/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.8981 - val_accuracy: 1.0000\n",
            "Epoch 1364/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8962 - accuracy: 1.0000 - val_loss: 1.9074 - val_accuracy: 1.0000\n",
            "Epoch 1365/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9105 - accuracy: 1.0000 - val_loss: 1.9635 - val_accuracy: 1.0000\n",
            "Epoch 1366/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9154 - accuracy: 1.0000 - val_loss: 1.8812 - val_accuracy: 1.0000\n",
            "Epoch 1367/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9071 - accuracy: 1.0000 - val_loss: 1.9369 - val_accuracy: 1.0000\n",
            "Epoch 1368/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9091 - accuracy: 1.0000 - val_loss: 1.9581 - val_accuracy: 1.0000\n",
            "Epoch 1369/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9036 - accuracy: 1.0000 - val_loss: 2.0430 - val_accuracy: 1.0000\n",
            "Epoch 1370/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9001 - val_accuracy: 1.0000\n",
            "Epoch 1371/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9148 - accuracy: 1.0000 - val_loss: 1.9077 - val_accuracy: 1.0000\n",
            "Epoch 1372/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9092 - accuracy: 1.0000 - val_loss: 1.9030 - val_accuracy: 1.0000\n",
            "Epoch 1373/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9036 - accuracy: 1.0000 - val_loss: 1.8842 - val_accuracy: 1.0000\n",
            "Epoch 1374/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.9420 - val_accuracy: 1.0000\n",
            "Epoch 1375/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9070 - accuracy: 1.0000 - val_loss: 1.9125 - val_accuracy: 1.0000\n",
            "Epoch 1376/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9013 - accuracy: 1.0000 - val_loss: 1.9273 - val_accuracy: 1.0000\n",
            "Epoch 1377/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9243 - accuracy: 1.0000 - val_loss: 1.9141 - val_accuracy: 1.0000\n",
            "Epoch 1378/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9108 - accuracy: 1.0000 - val_loss: 1.9450 - val_accuracy: 1.0000\n",
            "Epoch 1379/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.9475 - val_accuracy: 1.0000\n",
            "Epoch 1380/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 2.1687 - val_accuracy: 1.0000\n",
            "Epoch 1381/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9275 - accuracy: 1.0000 - val_loss: 1.9726 - val_accuracy: 1.0000\n",
            "Epoch 1382/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8957 - accuracy: 1.0000 - val_loss: 2.0174 - val_accuracy: 1.0000\n",
            "Epoch 1383/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9033 - accuracy: 1.0000 - val_loss: 1.8916 - val_accuracy: 1.0000\n",
            "Epoch 1384/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9127 - accuracy: 1.0000 - val_loss: 1.9146 - val_accuracy: 1.0000\n",
            "Epoch 1385/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.8938 - val_accuracy: 1.0000\n",
            "Epoch 1386/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8968 - accuracy: 1.0000 - val_loss: 1.9092 - val_accuracy: 1.0000\n",
            "Epoch 1387/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9265 - accuracy: 1.0000 - val_loss: 2.0695 - val_accuracy: 1.0000\n",
            "Epoch 1388/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.8881 - val_accuracy: 1.0000\n",
            "Epoch 1389/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9035 - accuracy: 1.0000 - val_loss: 1.8933 - val_accuracy: 1.0000\n",
            "Epoch 1390/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8998 - accuracy: 1.0000 - val_loss: 2.0821 - val_accuracy: 1.0000\n",
            "Epoch 1391/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9227 - accuracy: 1.0000 - val_loss: 1.8930 - val_accuracy: 1.0000\n",
            "Epoch 1392/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9040 - accuracy: 1.0000 - val_loss: 1.9049 - val_accuracy: 1.0000\n",
            "Epoch 1393/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9129 - accuracy: 1.0000 - val_loss: 2.0087 - val_accuracy: 1.0000\n",
            "Epoch 1394/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9253 - accuracy: 1.0000 - val_loss: 1.8965 - val_accuracy: 1.0000\n",
            "Epoch 1395/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8878 - accuracy: 1.0000 - val_loss: 1.9017 - val_accuracy: 1.0000\n",
            "Epoch 1396/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 2.0708 - val_accuracy: 1.0000\n",
            "Epoch 1397/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9145 - accuracy: 1.0000 - val_loss: 1.9557 - val_accuracy: 1.0000\n",
            "Epoch 1398/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 1.9365 - val_accuracy: 1.0000\n",
            "Epoch 1399/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9047 - accuracy: 1.0000 - val_loss: 1.9278 - val_accuracy: 1.0000\n",
            "Epoch 1400/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.8858 - val_accuracy: 1.0000\n",
            "Epoch 1401/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9074 - accuracy: 1.0000 - val_loss: 1.9366 - val_accuracy: 1.0000\n",
            "Epoch 1402/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9140 - accuracy: 1.0000 - val_loss: 1.9343 - val_accuracy: 1.0000\n",
            "Epoch 1403/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9028 - accuracy: 1.0000 - val_loss: 1.8947 - val_accuracy: 1.0000\n",
            "Epoch 1404/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9102 - accuracy: 1.0000 - val_loss: 1.9552 - val_accuracy: 1.0000\n",
            "Epoch 1405/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9083 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 1.0000\n",
            "Epoch 1406/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9066 - accuracy: 1.0000 - val_loss: 1.9035 - val_accuracy: 1.0000\n",
            "Epoch 1407/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 1.9346 - val_accuracy: 1.0000\n",
            "Epoch 1408/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 1.8960 - val_accuracy: 1.0000\n",
            "Epoch 1409/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.9394 - val_accuracy: 1.0000\n",
            "Epoch 1410/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9202 - accuracy: 1.0000 - val_loss: 1.9058 - val_accuracy: 1.0000\n",
            "Epoch 1411/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9089 - accuracy: 1.0000 - val_loss: 1.8883 - val_accuracy: 1.0000\n",
            "Epoch 1412/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9007 - accuracy: 1.0000 - val_loss: 1.9012 - val_accuracy: 1.0000\n",
            "Epoch 1413/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9012 - accuracy: 1.0000 - val_loss: 1.9478 - val_accuracy: 1.0000\n",
            "Epoch 1414/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9146 - accuracy: 1.0000 - val_loss: 1.8889 - val_accuracy: 1.0000\n",
            "Epoch 1415/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.9061 - val_accuracy: 1.0000\n",
            "Epoch 1416/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.8892 - val_accuracy: 1.0000\n",
            "Epoch 1417/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9096 - accuracy: 1.0000 - val_loss: 1.9816 - val_accuracy: 1.0000\n",
            "Epoch 1418/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9096 - accuracy: 1.0000 - val_loss: 2.0430 - val_accuracy: 1.0000\n",
            "Epoch 1419/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9102 - accuracy: 1.0000 - val_loss: 1.9713 - val_accuracy: 1.0000\n",
            "Epoch 1420/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9107 - accuracy: 1.0000 - val_loss: 1.9256 - val_accuracy: 1.0000\n",
            "Epoch 1421/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9010 - accuracy: 1.0000 - val_loss: 2.0634 - val_accuracy: 1.0000\n",
            "Epoch 1422/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9026 - accuracy: 1.0000 - val_loss: 1.8899 - val_accuracy: 1.0000\n",
            "Epoch 1423/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.8885 - val_accuracy: 1.0000\n",
            "Epoch 1424/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9318 - val_accuracy: 1.0000\n",
            "Epoch 1425/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9034 - accuracy: 1.0000 - val_loss: 1.9383 - val_accuracy: 1.0000\n",
            "Epoch 1426/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9021 - accuracy: 1.0000 - val_loss: 1.8968 - val_accuracy: 1.0000\n",
            "Epoch 1427/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9106 - accuracy: 1.0000 - val_loss: 2.0258 - val_accuracy: 1.0000\n",
            "Epoch 1428/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9031 - accuracy: 1.0000 - val_loss: 1.8945 - val_accuracy: 1.0000\n",
            "Epoch 1429/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.8797 - val_accuracy: 1.0000\n",
            "Epoch 1430/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 1.9285 - val_accuracy: 1.0000\n",
            "Epoch 1431/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9089 - accuracy: 1.0000 - val_loss: 1.9514 - val_accuracy: 1.0000\n",
            "Epoch 1432/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 1.9394 - val_accuracy: 1.0000\n",
            "Epoch 1433/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 1.9515 - val_accuracy: 1.0000\n",
            "Epoch 1434/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 2.1578 - val_accuracy: 1.0000\n",
            "Epoch 1435/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9243 - accuracy: 1.0000 - val_loss: 1.8899 - val_accuracy: 1.0000\n",
            "Epoch 1436/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9054 - accuracy: 1.0000 - val_loss: 1.9413 - val_accuracy: 1.0000\n",
            "Epoch 1437/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9062 - accuracy: 1.0000 - val_loss: 1.9189 - val_accuracy: 1.0000\n",
            "Epoch 1438/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8976 - accuracy: 1.0000 - val_loss: 1.9875 - val_accuracy: 1.0000\n",
            "Epoch 1439/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9115 - accuracy: 1.0000 - val_loss: 2.2939 - val_accuracy: 1.0000\n",
            "Epoch 1440/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9166 - accuracy: 1.0000 - val_loss: 1.8839 - val_accuracy: 1.0000\n",
            "Epoch 1441/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9165 - accuracy: 1.0000 - val_loss: 1.9170 - val_accuracy: 1.0000\n",
            "Epoch 1442/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 2.0039 - val_accuracy: 1.0000\n",
            "Epoch 1443/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.8964 - val_accuracy: 1.0000\n",
            "Epoch 1444/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8994 - accuracy: 1.0000 - val_loss: 1.8874 - val_accuracy: 1.0000\n",
            "Epoch 1445/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9098 - accuracy: 1.0000 - val_loss: 1.8921 - val_accuracy: 1.0000\n",
            "Epoch 1446/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 2.0058 - val_accuracy: 1.0000\n",
            "Epoch 1447/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.9085 - val_accuracy: 1.0000\n",
            "Epoch 1448/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9029 - accuracy: 1.0000 - val_loss: 1.8863 - val_accuracy: 1.0000\n",
            "Epoch 1449/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9070 - accuracy: 1.0000 - val_loss: 1.9576 - val_accuracy: 1.0000\n",
            "Epoch 1450/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9047 - accuracy: 1.0000 - val_loss: 1.8947 - val_accuracy: 1.0000\n",
            "Epoch 1451/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 1.9464 - val_accuracy: 1.0000\n",
            "Epoch 1452/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9145 - accuracy: 1.0000 - val_loss: 1.9356 - val_accuracy: 1.0000\n",
            "Epoch 1453/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9003 - accuracy: 1.0000 - val_loss: 1.9090 - val_accuracy: 1.0000\n",
            "Epoch 1454/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.9018 - val_accuracy: 1.0000\n",
            "Epoch 1455/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9044 - accuracy: 1.0000 - val_loss: 1.9097 - val_accuracy: 1.0000\n",
            "Epoch 1456/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9255 - val_accuracy: 1.0000\n",
            "Epoch 1457/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8997 - accuracy: 1.0000 - val_loss: 2.0034 - val_accuracy: 1.0000\n",
            "Epoch 1458/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.8950 - val_accuracy: 1.0000\n",
            "Epoch 1459/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9049 - accuracy: 1.0000 - val_loss: 2.0443 - val_accuracy: 1.0000\n",
            "Epoch 1460/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 1.9807 - val_accuracy: 1.0000\n",
            "Epoch 1461/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9174 - accuracy: 1.0000 - val_loss: 1.9040 - val_accuracy: 1.0000\n",
            "Epoch 1462/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8994 - accuracy: 1.0000 - val_loss: 1.9215 - val_accuracy: 1.0000\n",
            "Epoch 1463/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9224 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 1.0000\n",
            "Epoch 1464/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.9344 - val_accuracy: 1.0000\n",
            "Epoch 1465/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8971 - accuracy: 1.0000 - val_loss: 1.8820 - val_accuracy: 1.0000\n",
            "Epoch 1466/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.9126 - val_accuracy: 1.0000\n",
            "Epoch 1467/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9067 - accuracy: 1.0000 - val_loss: 1.9463 - val_accuracy: 1.0000\n",
            "Epoch 1468/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9078 - accuracy: 1.0000 - val_loss: 1.9158 - val_accuracy: 1.0000\n",
            "Epoch 1469/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.9407 - val_accuracy: 1.0000\n",
            "Epoch 1470/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9088 - accuracy: 1.0000 - val_loss: 1.9117 - val_accuracy: 1.0000\n",
            "Epoch 1471/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9025 - accuracy: 1.0000 - val_loss: 1.8879 - val_accuracy: 1.0000\n",
            "Epoch 1472/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9065 - accuracy: 1.0000 - val_loss: 1.9424 - val_accuracy: 1.0000\n",
            "Epoch 1473/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9040 - accuracy: 1.0000 - val_loss: 1.8810 - val_accuracy: 1.0000\n",
            "Epoch 1474/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9041 - accuracy: 1.0000 - val_loss: 1.9172 - val_accuracy: 1.0000\n",
            "Epoch 1475/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.8832 - val_accuracy: 1.0000\n",
            "Epoch 1476/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9035 - accuracy: 1.0000 - val_loss: 1.8946 - val_accuracy: 1.0000\n",
            "Epoch 1477/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9066 - accuracy: 1.0000 - val_loss: 1.9009 - val_accuracy: 1.0000\n",
            "Epoch 1478/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9116 - accuracy: 1.0000 - val_loss: 1.8902 - val_accuracy: 1.0000\n",
            "Epoch 1479/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.9001 - val_accuracy: 1.0000\n",
            "Epoch 1480/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8979 - accuracy: 1.0000 - val_loss: 1.9225 - val_accuracy: 1.0000\n",
            "Epoch 1481/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.8922 - val_accuracy: 1.0000\n",
            "Epoch 1482/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9080 - accuracy: 1.0000 - val_loss: 1.8835 - val_accuracy: 1.0000\n",
            "Epoch 1483/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9033 - accuracy: 1.0000 - val_loss: 1.8997 - val_accuracy: 1.0000\n",
            "Epoch 1484/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9269 - accuracy: 1.0000 - val_loss: 2.0313 - val_accuracy: 1.0000\n",
            "Epoch 1485/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8944 - accuracy: 1.0000 - val_loss: 1.9389 - val_accuracy: 1.0000\n",
            "Epoch 1486/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9229 - accuracy: 1.0000 - val_loss: 1.8854 - val_accuracy: 1.0000\n",
            "Epoch 1487/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8925 - accuracy: 1.0000 - val_loss: 2.0839 - val_accuracy: 1.0000\n",
            "Epoch 1488/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9078 - accuracy: 1.0000 - val_loss: 1.9305 - val_accuracy: 1.0000\n",
            "Epoch 1489/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9051 - accuracy: 1.0000 - val_loss: 1.9075 - val_accuracy: 1.0000\n",
            "Epoch 1490/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9083 - accuracy: 1.0000 - val_loss: 1.9369 - val_accuracy: 1.0000\n",
            "Epoch 1491/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9072 - accuracy: 1.0000 - val_loss: 1.8876 - val_accuracy: 1.0000\n",
            "Epoch 1492/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9063 - accuracy: 1.0000 - val_loss: 2.0015 - val_accuracy: 1.0000\n",
            "Epoch 1493/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9081 - accuracy: 1.0000 - val_loss: 1.9483 - val_accuracy: 1.0000\n",
            "Epoch 1494/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9002 - val_accuracy: 1.0000\n",
            "Epoch 1495/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9115 - accuracy: 1.0000 - val_loss: 1.8931 - val_accuracy: 1.0000\n",
            "Epoch 1496/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9026 - accuracy: 1.0000 - val_loss: 1.9649 - val_accuracy: 1.0000\n",
            "Epoch 1497/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9055 - accuracy: 1.0000 - val_loss: 1.9223 - val_accuracy: 1.0000\n",
            "Epoch 1498/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9088 - accuracy: 1.0000 - val_loss: 1.9047 - val_accuracy: 1.0000\n",
            "Epoch 1499/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.8854 - val_accuracy: 1.0000\n",
            "Epoch 1500/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 2.1666 - val_accuracy: 1.0000\n",
            "Epoch 1501/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9035 - accuracy: 1.0000 - val_loss: 1.8807 - val_accuracy: 1.0000\n",
            "Epoch 1502/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9240 - accuracy: 1.0000 - val_loss: 1.9178 - val_accuracy: 1.0000\n",
            "Epoch 1503/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.8909 - accuracy: 1.0000 - val_loss: 1.9675 - val_accuracy: 1.0000\n",
            "Epoch 1504/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9041 - accuracy: 1.0000 - val_loss: 2.7127 - val_accuracy: 1.0000\n",
            "Epoch 1505/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9347 - accuracy: 1.0000 - val_loss: 2.0627 - val_accuracy: 1.0000\n",
            "Epoch 1506/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 2.0043 - val_accuracy: 1.0000\n",
            "Epoch 1507/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9102 - accuracy: 1.0000 - val_loss: 1.9886 - val_accuracy: 1.0000\n",
            "Epoch 1508/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9054 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 1509/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 1.9079 - val_accuracy: 1.0000\n",
            "Epoch 1510/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9017 - accuracy: 1.0000 - val_loss: 1.9003 - val_accuracy: 1.0000\n",
            "Epoch 1511/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9080 - accuracy: 1.0000 - val_loss: 1.9916 - val_accuracy: 1.0000\n",
            "Epoch 1512/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.9508 - val_accuracy: 1.0000\n",
            "Epoch 1513/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9170 - accuracy: 1.0000 - val_loss: 1.9154 - val_accuracy: 1.0000\n",
            "Epoch 1514/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9009 - accuracy: 1.0000 - val_loss: 1.8979 - val_accuracy: 1.0000\n",
            "Epoch 1515/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9053 - accuracy: 1.0000 - val_loss: 1.9032 - val_accuracy: 1.0000\n",
            "Epoch 1516/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9045 - accuracy: 1.0000 - val_loss: 1.8835 - val_accuracy: 1.0000\n",
            "Epoch 1517/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9397 - accuracy: 1.0000 - val_loss: 1.8886 - val_accuracy: 1.0000\n",
            "Epoch 1518/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8863 - accuracy: 1.0000 - val_loss: 1.9507 - val_accuracy: 1.0000\n",
            "Epoch 1519/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9154 - val_accuracy: 1.0000\n",
            "Epoch 1520/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9051 - accuracy: 1.0000 - val_loss: 1.9372 - val_accuracy: 1.0000\n",
            "Epoch 1521/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9055 - accuracy: 1.0000 - val_loss: 1.9337 - val_accuracy: 1.0000\n",
            "Epoch 1522/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9012 - accuracy: 1.0000 - val_loss: 1.9675 - val_accuracy: 1.0000\n",
            "Epoch 1523/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9397 - val_accuracy: 1.0000\n",
            "Epoch 1524/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9118 - val_accuracy: 1.0000\n",
            "Epoch 1525/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9076 - accuracy: 1.0000 - val_loss: 2.0439 - val_accuracy: 1.0000\n",
            "Epoch 1526/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9043 - accuracy: 1.0000 - val_loss: 1.9275 - val_accuracy: 1.0000\n",
            "Epoch 1527/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9048 - accuracy: 1.0000 - val_loss: 1.9207 - val_accuracy: 1.0000\n",
            "Epoch 1528/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.9822 - val_accuracy: 1.0000\n",
            "Epoch 1529/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 2.1470 - val_accuracy: 1.0000\n",
            "Epoch 1530/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9047 - accuracy: 1.0000 - val_loss: 1.8998 - val_accuracy: 1.0000\n",
            "Epoch 1531/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9070 - accuracy: 1.0000 - val_loss: 1.9034 - val_accuracy: 1.0000\n",
            "Epoch 1532/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8993 - accuracy: 1.0000 - val_loss: 1.9835 - val_accuracy: 1.0000\n",
            "Epoch 1533/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 1.9426 - val_accuracy: 1.0000\n",
            "Epoch 1534/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.8872 - val_accuracy: 1.0000\n",
            "Epoch 1535/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.8984 - val_accuracy: 1.0000\n",
            "Epoch 1536/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8971 - accuracy: 1.0000 - val_loss: 1.8782 - val_accuracy: 1.0000\n",
            "Epoch 1537/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.9520 - val_accuracy: 1.0000\n",
            "Epoch 1538/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9013 - accuracy: 1.0000 - val_loss: 1.9186 - val_accuracy: 1.0000\n",
            "Epoch 1539/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.9028 - val_accuracy: 1.0000\n",
            "Epoch 1540/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 1.0000\n",
            "Epoch 1541/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9058 - accuracy: 1.0000 - val_loss: 1.9976 - val_accuracy: 1.0000\n",
            "Epoch 1542/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9079 - accuracy: 1.0000 - val_loss: 2.0285 - val_accuracy: 1.0000\n",
            "Epoch 1543/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9141 - val_accuracy: 1.0000\n",
            "Epoch 1544/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9003 - accuracy: 1.0000 - val_loss: 1.8835 - val_accuracy: 1.0000\n",
            "Epoch 1545/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9067 - accuracy: 1.0000 - val_loss: 1.9432 - val_accuracy: 1.0000\n",
            "Epoch 1546/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9024 - accuracy: 1.0000 - val_loss: 1.9071 - val_accuracy: 1.0000\n",
            "Epoch 1547/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9189 - val_accuracy: 1.0000\n",
            "Epoch 1548/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8933 - accuracy: 1.0000 - val_loss: 1.9190 - val_accuracy: 1.0000\n",
            "Epoch 1549/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9092 - accuracy: 1.0000 - val_loss: 1.9164 - val_accuracy: 1.0000\n",
            "Epoch 1550/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.9003 - val_accuracy: 1.0000\n",
            "Epoch 1551/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9139 - accuracy: 1.0000 - val_loss: 1.8852 - val_accuracy: 1.0000\n",
            "Epoch 1552/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9014 - accuracy: 1.0000 - val_loss: 1.9254 - val_accuracy: 1.0000\n",
            "Epoch 1553/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.8844 - val_accuracy: 1.0000\n",
            "Epoch 1554/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9033 - accuracy: 1.0000 - val_loss: 1.9510 - val_accuracy: 1.0000\n",
            "Epoch 1555/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9113 - accuracy: 1.0000 - val_loss: 1.8957 - val_accuracy: 1.0000\n",
            "Epoch 1556/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 1.9268 - val_accuracy: 1.0000\n",
            "Epoch 1557/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9031 - accuracy: 1.0000 - val_loss: 1.9311 - val_accuracy: 1.0000\n",
            "Epoch 1558/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.9105 - val_accuracy: 1.0000\n",
            "Epoch 1559/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9065 - accuracy: 1.0000 - val_loss: 1.8846 - val_accuracy: 1.0000\n",
            "Epoch 1560/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9033 - accuracy: 1.0000 - val_loss: 1.9411 - val_accuracy: 1.0000\n",
            "Epoch 1561/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 1.8863 - val_accuracy: 1.0000\n",
            "Epoch 1562/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.9023 - val_accuracy: 1.0000\n",
            "Epoch 1563/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8989 - accuracy: 1.0000 - val_loss: 1.9437 - val_accuracy: 1.0000\n",
            "Epoch 1564/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9072 - accuracy: 1.0000 - val_loss: 1.9491 - val_accuracy: 1.0000\n",
            "Epoch 1565/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.9444 - val_accuracy: 1.0000\n",
            "Epoch 1566/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9195 - accuracy: 1.0000 - val_loss: 1.9489 - val_accuracy: 1.0000\n",
            "Epoch 1567/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8923 - accuracy: 1.0000 - val_loss: 1.8895 - val_accuracy: 1.0000\n",
            "Epoch 1568/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9092 - accuracy: 1.0000 - val_loss: 1.9109 - val_accuracy: 1.0000\n",
            "Epoch 1569/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9032 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 1570/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9040 - accuracy: 1.0000 - val_loss: 1.9125 - val_accuracy: 1.0000\n",
            "Epoch 1571/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9041 - accuracy: 1.0000 - val_loss: 1.8834 - val_accuracy: 1.0000\n",
            "Epoch 1572/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9281 - val_accuracy: 1.0000\n",
            "Epoch 1573/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9028 - accuracy: 1.0000 - val_loss: 1.9176 - val_accuracy: 1.0000\n",
            "Epoch 1574/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.9173 - val_accuracy: 1.0000\n",
            "Epoch 1575/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8925 - accuracy: 1.0000 - val_loss: 1.8838 - val_accuracy: 1.0000\n",
            "Epoch 1576/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 1.9819 - val_accuracy: 1.0000\n",
            "Epoch 1577/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9098 - accuracy: 1.0000 - val_loss: 1.9251 - val_accuracy: 1.0000\n",
            "Epoch 1578/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9148 - val_accuracy: 1.0000\n",
            "Epoch 1579/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8985 - accuracy: 1.0000 - val_loss: 1.8936 - val_accuracy: 1.0000\n",
            "Epoch 1580/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9126 - accuracy: 1.0000 - val_loss: 2.0404 - val_accuracy: 1.0000\n",
            "Epoch 1581/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9362 - accuracy: 1.0000 - val_loss: 1.9176 - val_accuracy: 1.0000\n",
            "Epoch 1582/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8952 - accuracy: 1.0000 - val_loss: 1.9050 - val_accuracy: 1.0000\n",
            "Epoch 1583/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9054 - accuracy: 1.0000 - val_loss: 1.8983 - val_accuracy: 1.0000\n",
            "Epoch 1584/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 1.9605 - val_accuracy: 1.0000\n",
            "Epoch 1585/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8997 - accuracy: 1.0000 - val_loss: 1.9029 - val_accuracy: 1.0000\n",
            "Epoch 1586/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9079 - accuracy: 1.0000 - val_loss: 2.0767 - val_accuracy: 1.0000\n",
            "Epoch 1587/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.9337 - val_accuracy: 1.0000\n",
            "Epoch 1588/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 1.9035 - val_accuracy: 1.0000\n",
            "Epoch 1589/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9086 - accuracy: 1.0000 - val_loss: 1.8814 - val_accuracy: 1.0000\n",
            "Epoch 1590/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9071 - accuracy: 1.0000 - val_loss: 1.9154 - val_accuracy: 1.0000\n",
            "Epoch 1591/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9086 - accuracy: 1.0000 - val_loss: 1.9568 - val_accuracy: 1.0000\n",
            "Epoch 1592/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9024 - accuracy: 1.0000 - val_loss: 1.9457 - val_accuracy: 1.0000\n",
            "Epoch 1593/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.9270 - val_accuracy: 1.0000\n",
            "Epoch 1594/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.9266 - val_accuracy: 1.0000\n",
            "Epoch 1595/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9066 - accuracy: 1.0000 - val_loss: 1.8925 - val_accuracy: 1.0000\n",
            "Epoch 1596/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.8878 - val_accuracy: 1.0000\n",
            "Epoch 1597/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.8973 - val_accuracy: 1.0000\n",
            "Epoch 1598/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9002 - accuracy: 1.0000 - val_loss: 1.8789 - val_accuracy: 1.0000\n",
            "Epoch 1599/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9092 - accuracy: 1.0000 - val_loss: 1.8881 - val_accuracy: 1.0000\n",
            "Epoch 1600/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9064 - accuracy: 1.0000 - val_loss: 1.9275 - val_accuracy: 1.0000\n",
            "Epoch 1601/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9109 - accuracy: 1.0000 - val_loss: 1.9240 - val_accuracy: 1.0000\n",
            "Epoch 1602/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9054 - val_accuracy: 1.0000\n",
            "Epoch 1603/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.9221 - val_accuracy: 1.0000\n",
            "Epoch 1604/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.8887 - accuracy: 1.0000 - val_loss: 1.9639 - val_accuracy: 1.0000\n",
            "Epoch 1605/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9058 - accuracy: 1.0000 - val_loss: 1.8910 - val_accuracy: 1.0000\n",
            "Epoch 1606/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.9162 - val_accuracy: 1.0000\n",
            "Epoch 1607/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8939 - accuracy: 1.0000 - val_loss: 1.9808 - val_accuracy: 1.0000\n",
            "Epoch 1608/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9146 - accuracy: 1.0000 - val_loss: 1.8886 - val_accuracy: 1.0000\n",
            "Epoch 1609/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9033 - accuracy: 1.0000 - val_loss: 1.9617 - val_accuracy: 1.0000\n",
            "Epoch 1610/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 2.0683 - val_accuracy: 1.0000\n",
            "Epoch 1611/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.8855 - val_accuracy: 1.0000\n",
            "Epoch 1612/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9184 - accuracy: 1.0000 - val_loss: 1.9242 - val_accuracy: 1.0000\n",
            "Epoch 1613/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9070 - accuracy: 1.0000 - val_loss: 1.8845 - val_accuracy: 1.0000\n",
            "Epoch 1614/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8943 - accuracy: 1.0000 - val_loss: 4.2393 - val_accuracy: 1.0000\n",
            "Epoch 1615/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9919 - accuracy: 1.0000 - val_loss: 1.9359 - val_accuracy: 1.0000\n",
            "Epoch 1616/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9082 - accuracy: 1.0000 - val_loss: 1.9034 - val_accuracy: 1.0000\n",
            "Epoch 1617/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8978 - accuracy: 1.0000 - val_loss: 1.9097 - val_accuracy: 1.0000\n",
            "Epoch 1618/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8991 - accuracy: 1.0000 - val_loss: 1.9067 - val_accuracy: 1.0000\n",
            "Epoch 1619/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.8857 - val_accuracy: 1.0000\n",
            "Epoch 1620/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8936 - accuracy: 1.0000 - val_loss: 1.8922 - val_accuracy: 1.0000\n",
            "Epoch 1621/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9097 - accuracy: 1.0000 - val_loss: 1.8886 - val_accuracy: 1.0000\n",
            "Epoch 1622/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.8991 - accuracy: 1.0000 - val_loss: 1.9087 - val_accuracy: 1.0000\n",
            "Epoch 1623/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9108 - accuracy: 1.0000 - val_loss: 1.8810 - val_accuracy: 1.0000\n",
            "Epoch 1624/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9000 - accuracy: 1.0000 - val_loss: 1.9096 - val_accuracy: 1.0000\n",
            "Epoch 1625/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9048 - accuracy: 1.0000 - val_loss: 1.9909 - val_accuracy: 1.0000\n",
            "Epoch 1626/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9097 - accuracy: 1.0000 - val_loss: 1.9110 - val_accuracy: 1.0000\n",
            "Epoch 1627/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8994 - accuracy: 1.0000 - val_loss: 1.8963 - val_accuracy: 1.0000\n",
            "Epoch 1628/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9141 - accuracy: 1.0000 - val_loss: 1.9113 - val_accuracy: 1.0000\n",
            "Epoch 1629/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9036 - accuracy: 1.0000 - val_loss: 1.8939 - val_accuracy: 1.0000\n",
            "Epoch 1630/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.8860 - val_accuracy: 1.0000\n",
            "Epoch 1631/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8962 - accuracy: 1.0000 - val_loss: 1.8925 - val_accuracy: 1.0000\n",
            "Epoch 1632/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9054 - accuracy: 1.0000 - val_loss: 1.8934 - val_accuracy: 1.0000\n",
            "Epoch 1633/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9151 - accuracy: 1.0000 - val_loss: 1.9747 - val_accuracy: 1.0000\n",
            "Epoch 1634/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8996 - accuracy: 1.0000 - val_loss: 2.2024 - val_accuracy: 1.0000\n",
            "Epoch 1635/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9154 - accuracy: 1.0000 - val_loss: 1.9115 - val_accuracy: 1.0000\n",
            "Epoch 1636/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9170 - accuracy: 1.0000 - val_loss: 1.8897 - val_accuracy: 1.0000\n",
            "Epoch 1637/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8930 - accuracy: 1.0000 - val_loss: 1.9486 - val_accuracy: 1.0000\n",
            "Epoch 1638/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9039 - accuracy: 1.0000 - val_loss: 1.8881 - val_accuracy: 1.0000\n",
            "Epoch 1639/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.9849 - val_accuracy: 1.0000\n",
            "Epoch 1640/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9010 - val_accuracy: 1.0000\n",
            "Epoch 1641/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.9094 - val_accuracy: 1.0000\n",
            "Epoch 1642/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8998 - accuracy: 1.0000 - val_loss: 1.9728 - val_accuracy: 1.0000\n",
            "Epoch 1643/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.9455 - val_accuracy: 1.0000\n",
            "Epoch 1644/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.8986 - accuracy: 1.0000 - val_loss: 1.8934 - val_accuracy: 1.0000\n",
            "Epoch 1645/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9205 - accuracy: 1.0000 - val_loss: 1.9133 - val_accuracy: 1.0000\n",
            "Epoch 1646/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8877 - accuracy: 1.0000 - val_loss: 1.9439 - val_accuracy: 1.0000\n",
            "Epoch 1647/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 2.0293 - val_accuracy: 1.0000\n",
            "Epoch 1648/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9115 - accuracy: 1.0000 - val_loss: 1.9478 - val_accuracy: 1.0000\n",
            "Epoch 1649/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 1.9316 - val_accuracy: 1.0000\n",
            "Epoch 1650/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8909 - accuracy: 1.0000 - val_loss: 1.8989 - val_accuracy: 1.0000\n",
            "Epoch 1651/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9065 - accuracy: 1.0000 - val_loss: 1.8772 - val_accuracy: 1.0000\n",
            "Epoch 1652/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9054 - accuracy: 1.0000 - val_loss: 1.9182 - val_accuracy: 1.0000\n",
            "Epoch 1653/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.9024 - val_accuracy: 1.0000\n",
            "Epoch 1654/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8953 - accuracy: 1.0000 - val_loss: 1.9257 - val_accuracy: 1.0000\n",
            "Epoch 1655/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9150 - accuracy: 1.0000 - val_loss: 1.9725 - val_accuracy: 1.0000\n",
            "Epoch 1656/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9021 - accuracy: 1.0000 - val_loss: 1.9197 - val_accuracy: 1.0000\n",
            "Epoch 1657/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9246 - accuracy: 1.0000 - val_loss: 1.8932 - val_accuracy: 1.0000\n",
            "Epoch 1658/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9162 - val_accuracy: 1.0000\n",
            "Epoch 1659/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9031 - accuracy: 1.0000 - val_loss: 1.9517 - val_accuracy: 1.0000\n",
            "Epoch 1660/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9082 - accuracy: 1.0000 - val_loss: 1.9764 - val_accuracy: 1.0000\n",
            "Epoch 1661/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.8949 - val_accuracy: 1.0000\n",
            "Epoch 1662/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8965 - accuracy: 1.0000 - val_loss: 1.9170 - val_accuracy: 1.0000\n",
            "Epoch 1663/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8980 - accuracy: 1.0000 - val_loss: 1.8784 - val_accuracy: 1.0000\n",
            "Epoch 1664/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 1.0000\n",
            "Epoch 1665/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9042 - accuracy: 1.0000 - val_loss: 1.9731 - val_accuracy: 1.0000\n",
            "Epoch 1666/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.8888 - val_accuracy: 1.0000\n",
            "Epoch 1667/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8985 - accuracy: 1.0000 - val_loss: 1.9752 - val_accuracy: 1.0000\n",
            "Epoch 1668/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9169 - accuracy: 1.0000 - val_loss: 1.9367 - val_accuracy: 1.0000\n",
            "Epoch 1669/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8984 - accuracy: 1.0000 - val_loss: 1.9239 - val_accuracy: 1.0000\n",
            "Epoch 1670/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9053 - val_accuracy: 1.0000\n",
            "Epoch 1671/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9004 - accuracy: 1.0000 - val_loss: 1.9468 - val_accuracy: 1.0000\n",
            "Epoch 1672/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9073 - val_accuracy: 1.0000\n",
            "Epoch 1673/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9081 - accuracy: 1.0000 - val_loss: 1.9259 - val_accuracy: 1.0000\n",
            "Epoch 1674/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9006 - accuracy: 1.0000 - val_loss: 1.8995 - val_accuracy: 1.0000\n",
            "Epoch 1675/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9110 - accuracy: 1.0000 - val_loss: 1.8954 - val_accuracy: 1.0000\n",
            "Epoch 1676/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9075 - accuracy: 1.0000 - val_loss: 1.9127 - val_accuracy: 1.0000\n",
            "Epoch 1677/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.8849 - val_accuracy: 1.0000\n",
            "Epoch 1678/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9021 - accuracy: 1.0000 - val_loss: 1.8870 - val_accuracy: 1.0000\n",
            "Epoch 1679/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9375 - accuracy: 1.0000 - val_loss: 1.9641 - val_accuracy: 1.0000\n",
            "Epoch 1680/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.8920 - accuracy: 1.0000 - val_loss: 1.8896 - val_accuracy: 1.0000\n",
            "Epoch 1681/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8999 - accuracy: 1.0000 - val_loss: 1.9962 - val_accuracy: 1.0000\n",
            "Epoch 1682/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9002 - accuracy: 1.0000 - val_loss: 1.9193 - val_accuracy: 1.0000\n",
            "Epoch 1683/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9122 - accuracy: 1.0000 - val_loss: 2.0523 - val_accuracy: 1.0000\n",
            "Epoch 1684/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.8995 - accuracy: 1.0000 - val_loss: 1.9198 - val_accuracy: 1.0000\n",
            "Epoch 1685/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.8927 - val_accuracy: 1.0000\n",
            "Epoch 1686/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9179 - accuracy: 1.0000 - val_loss: 1.9818 - val_accuracy: 1.0000\n",
            "Epoch 1687/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.8945 - val_accuracy: 1.0000\n",
            "Epoch 1688/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9064 - accuracy: 1.0000 - val_loss: 1.8880 - val_accuracy: 1.0000\n",
            "Epoch 1689/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9021 - accuracy: 1.0000 - val_loss: 1.8900 - val_accuracy: 1.0000\n",
            "Epoch 1690/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9020 - accuracy: 1.0000 - val_loss: 1.9465 - val_accuracy: 1.0000\n",
            "Epoch 1691/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9048 - accuracy: 1.0000 - val_loss: 1.8994 - val_accuracy: 1.0000\n",
            "Epoch 1692/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.9391 - val_accuracy: 1.0000\n",
            "Epoch 1693/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9047 - accuracy: 1.0000 - val_loss: 1.8803 - val_accuracy: 1.0000\n",
            "Epoch 1694/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 2.0313 - val_accuracy: 1.0000\n",
            "Epoch 1695/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9064 - accuracy: 1.0000 - val_loss: 1.8886 - val_accuracy: 1.0000\n",
            "Epoch 1696/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9040 - accuracy: 1.0000 - val_loss: 1.9441 - val_accuracy: 1.0000\n",
            "Epoch 1697/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9109 - accuracy: 1.0000 - val_loss: 1.9285 - val_accuracy: 1.0000\n",
            "Epoch 1698/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9056 - val_accuracy: 1.0000\n",
            "Epoch 1699/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9045 - accuracy: 1.0000 - val_loss: 1.9508 - val_accuracy: 1.0000\n",
            "Epoch 1700/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9043 - accuracy: 1.0000 - val_loss: 1.9284 - val_accuracy: 1.0000\n",
            "Epoch 1701/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9033 - accuracy: 1.0000 - val_loss: 1.9008 - val_accuracy: 1.0000\n",
            "Epoch 1702/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9073 - accuracy: 1.0000 - val_loss: 1.9558 - val_accuracy: 1.0000\n",
            "Epoch 1703/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9013 - accuracy: 1.0000 - val_loss: 1.8930 - val_accuracy: 1.0000\n",
            "Epoch 1704/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9076 - accuracy: 1.0000 - val_loss: 1.9411 - val_accuracy: 1.0000\n",
            "Epoch 1705/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9106 - accuracy: 1.0000 - val_loss: 1.9714 - val_accuracy: 1.0000\n",
            "Epoch 1706/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8959 - accuracy: 1.0000 - val_loss: 4.7340 - val_accuracy: 1.0000\n",
            "Epoch 1707/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 2.2472 - accuracy: 1.0000 - val_loss: 2.1844 - val_accuracy: 1.0000\n",
            "Epoch 1708/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.1201 - accuracy: 1.0000 - val_loss: 2.1083 - val_accuracy: 1.0000\n",
            "Epoch 1709/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.1096 - accuracy: 1.0000 - val_loss: 2.0770 - val_accuracy: 1.0000\n",
            "Epoch 1710/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0785 - accuracy: 1.0000 - val_loss: 2.2604 - val_accuracy: 1.0000\n",
            "Epoch 1711/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0803 - accuracy: 1.0000 - val_loss: 2.0883 - val_accuracy: 1.0000\n",
            "Epoch 1712/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 2.0631 - accuracy: 1.0000 - val_loss: 2.0129 - val_accuracy: 1.0000\n",
            "Epoch 1713/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 2.0495 - accuracy: 1.0000 - val_loss: 2.0414 - val_accuracy: 1.0000\n",
            "Epoch 1714/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0582 - accuracy: 1.0000 - val_loss: 2.0462 - val_accuracy: 1.0000\n",
            "Epoch 1715/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0400 - accuracy: 1.0000 - val_loss: 2.0363 - val_accuracy: 1.0000\n",
            "Epoch 1716/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 2.0375 - accuracy: 1.0000 - val_loss: 2.2247 - val_accuracy: 1.0000\n",
            "Epoch 1717/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0417 - accuracy: 1.0000 - val_loss: 2.0400 - val_accuracy: 1.0000\n",
            "Epoch 1718/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0376 - accuracy: 1.0000 - val_loss: 2.0302 - val_accuracy: 1.0000\n",
            "Epoch 1719/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0321 - accuracy: 1.0000 - val_loss: 2.0522 - val_accuracy: 1.0000\n",
            "Epoch 1720/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0267 - accuracy: 1.0000 - val_loss: 2.3458 - val_accuracy: 1.0000\n",
            "Epoch 1721/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 2.0321 - accuracy: 1.0000 - val_loss: 2.0213 - val_accuracy: 1.0000\n",
            "Epoch 1722/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0232 - accuracy: 1.0000 - val_loss: 2.1595 - val_accuracy: 1.0000\n",
            "Epoch 1723/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0306 - accuracy: 1.0000 - val_loss: 2.0269 - val_accuracy: 1.0000\n",
            "Epoch 1724/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0178 - accuracy: 1.0000 - val_loss: 2.1627 - val_accuracy: 1.0000\n",
            "Epoch 1725/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 2.0254 - accuracy: 1.0000 - val_loss: 1.9987 - val_accuracy: 1.0000\n",
            "Epoch 1726/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 2.0156 - accuracy: 1.0000 - val_loss: 2.0996 - val_accuracy: 1.0000\n",
            "Epoch 1727/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 2.0209 - accuracy: 1.0000 - val_loss: 2.0134 - val_accuracy: 1.0000\n",
            "Epoch 1728/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0249 - accuracy: 1.0000 - val_loss: 2.0043 - val_accuracy: 1.0000\n",
            "Epoch 1729/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0203 - accuracy: 1.0000 - val_loss: 2.1802 - val_accuracy: 1.0000\n",
            "Epoch 1730/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 2.0253 - accuracy: 1.0000 - val_loss: 2.0803 - val_accuracy: 1.0000\n",
            "Epoch 1731/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0159 - accuracy: 1.0000 - val_loss: 2.0458 - val_accuracy: 1.0000\n",
            "Epoch 1732/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0142 - accuracy: 1.0000 - val_loss: 2.0244 - val_accuracy: 1.0000\n",
            "Epoch 1733/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0272 - accuracy: 1.0000 - val_loss: 2.0042 - val_accuracy: 1.0000\n",
            "Epoch 1734/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9944 - accuracy: 1.0000 - val_loss: 2.1163 - val_accuracy: 1.0000\n",
            "Epoch 1735/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 2.0096 - accuracy: 1.0000 - val_loss: 1.9938 - val_accuracy: 1.0000\n",
            "Epoch 1736/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0084 - accuracy: 1.0000 - val_loss: 1.9971 - val_accuracy: 1.0000\n",
            "Epoch 1737/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 2.0027 - accuracy: 1.0000 - val_loss: 2.0054 - val_accuracy: 1.0000\n",
            "Epoch 1738/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0039 - accuracy: 1.0000 - val_loss: 1.9840 - val_accuracy: 1.0000\n",
            "Epoch 1739/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0082 - accuracy: 1.0000 - val_loss: 1.9954 - val_accuracy: 1.0000\n",
            "Epoch 1740/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 2.0056 - accuracy: 1.0000 - val_loss: 2.0039 - val_accuracy: 1.0000\n",
            "Epoch 1741/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 2.0042 - accuracy: 1.0000 - val_loss: 1.9994 - val_accuracy: 1.0000\n",
            "Epoch 1742/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0060 - accuracy: 1.0000 - val_loss: 2.0949 - val_accuracy: 1.0000\n",
            "Epoch 1743/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0061 - accuracy: 1.0000 - val_loss: 1.9875 - val_accuracy: 1.0000\n",
            "Epoch 1744/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0081 - accuracy: 1.0000 - val_loss: 1.9829 - val_accuracy: 1.0000\n",
            "Epoch 1745/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9994 - accuracy: 1.0000 - val_loss: 2.0014 - val_accuracy: 1.0000\n",
            "Epoch 1746/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9990 - accuracy: 1.0000 - val_loss: 2.0862 - val_accuracy: 1.0000\n",
            "Epoch 1747/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9940 - accuracy: 1.0000 - val_loss: 1.9956 - val_accuracy: 1.0000\n",
            "Epoch 1748/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9998 - accuracy: 1.0000 - val_loss: 1.9747 - val_accuracy: 1.0000\n",
            "Epoch 1749/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9973 - accuracy: 1.0000 - val_loss: 2.0447 - val_accuracy: 1.0000\n",
            "Epoch 1750/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0025 - accuracy: 1.0000 - val_loss: 1.9811 - val_accuracy: 1.0000\n",
            "Epoch 1751/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9891 - accuracy: 1.0000 - val_loss: 1.9853 - val_accuracy: 1.0000\n",
            "Epoch 1752/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0000 - accuracy: 1.0000 - val_loss: 2.2414 - val_accuracy: 1.0000\n",
            "Epoch 1753/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9899 - accuracy: 1.0000 - val_loss: 2.0059 - val_accuracy: 1.0000\n",
            "Epoch 1754/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9994 - accuracy: 1.0000 - val_loss: 1.9906 - val_accuracy: 1.0000\n",
            "Epoch 1755/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9928 - accuracy: 1.0000 - val_loss: 1.9735 - val_accuracy: 1.0000\n",
            "Epoch 1756/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9942 - accuracy: 1.0000 - val_loss: 2.0004 - val_accuracy: 1.0000\n",
            "Epoch 1757/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0051 - accuracy: 1.0000 - val_loss: 2.0309 - val_accuracy: 1.0000\n",
            "Epoch 1758/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9858 - accuracy: 1.0000 - val_loss: 2.0724 - val_accuracy: 1.0000\n",
            "Epoch 1759/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 2.0054 - accuracy: 1.0000 - val_loss: 1.9930 - val_accuracy: 1.0000\n",
            "Epoch 1760/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9885 - accuracy: 1.0000 - val_loss: 2.0058 - val_accuracy: 1.0000\n",
            "Epoch 1761/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9941 - accuracy: 1.0000 - val_loss: 1.9997 - val_accuracy: 1.0000\n",
            "Epoch 1762/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9941 - accuracy: 1.0000 - val_loss: 2.0085 - val_accuracy: 1.0000\n",
            "Epoch 1763/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9881 - accuracy: 1.0000 - val_loss: 1.9674 - val_accuracy: 1.0000\n",
            "Epoch 1764/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9906 - accuracy: 1.0000 - val_loss: 1.9847 - val_accuracy: 1.0000\n",
            "Epoch 1765/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9940 - accuracy: 1.0000 - val_loss: 1.9825 - val_accuracy: 1.0000\n",
            "Epoch 1766/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9922 - accuracy: 1.0000 - val_loss: 1.9829 - val_accuracy: 1.0000\n",
            "Epoch 1767/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9937 - accuracy: 1.0000 - val_loss: 1.9991 - val_accuracy: 1.0000\n",
            "Epoch 1768/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9961 - accuracy: 1.0000 - val_loss: 2.1208 - val_accuracy: 1.0000\n",
            "Epoch 1769/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9962 - accuracy: 1.0000 - val_loss: 2.0091 - val_accuracy: 1.0000\n",
            "Epoch 1770/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9770 - accuracy: 1.0000 - val_loss: 2.0512 - val_accuracy: 1.0000\n",
            "Epoch 1771/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0073 - accuracy: 1.0000 - val_loss: 1.9687 - val_accuracy: 1.0000\n",
            "Epoch 1772/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9861 - accuracy: 1.0000 - val_loss: 1.9726 - val_accuracy: 1.0000\n",
            "Epoch 1773/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9899 - accuracy: 1.0000 - val_loss: 1.9879 - val_accuracy: 1.0000\n",
            "Epoch 1774/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9836 - accuracy: 1.0000 - val_loss: 2.0069 - val_accuracy: 1.0000\n",
            "Epoch 1775/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9959 - accuracy: 1.0000 - val_loss: 2.0731 - val_accuracy: 1.0000\n",
            "Epoch 1776/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9944 - accuracy: 1.0000 - val_loss: 1.9743 - val_accuracy: 1.0000\n",
            "Epoch 1777/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9928 - accuracy: 1.0000 - val_loss: 1.9812 - val_accuracy: 1.0000\n",
            "Epoch 1778/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9847 - accuracy: 1.0000 - val_loss: 2.0130 - val_accuracy: 1.0000\n",
            "Epoch 1779/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9876 - accuracy: 1.0000 - val_loss: 1.9881 - val_accuracy: 1.0000\n",
            "Epoch 1780/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9916 - accuracy: 1.0000 - val_loss: 2.0128 - val_accuracy: 1.0000\n",
            "Epoch 1781/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9845 - accuracy: 1.0000 - val_loss: 2.0146 - val_accuracy: 1.0000\n",
            "Epoch 1782/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9858 - accuracy: 1.0000 - val_loss: 1.9994 - val_accuracy: 1.0000\n",
            "Epoch 1783/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 2.0022 - accuracy: 1.0000 - val_loss: 1.9990 - val_accuracy: 1.0000\n",
            "Epoch 1784/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9740 - accuracy: 1.0000 - val_loss: 2.0237 - val_accuracy: 1.0000\n",
            "Epoch 1785/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9931 - accuracy: 1.0000 - val_loss: 1.9754 - val_accuracy: 1.0000\n",
            "Epoch 1786/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9846 - accuracy: 1.0000 - val_loss: 1.9756 - val_accuracy: 1.0000\n",
            "Epoch 1787/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9986 - accuracy: 1.0000 - val_loss: 2.0126 - val_accuracy: 1.0000\n",
            "Epoch 1788/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9833 - accuracy: 1.0000 - val_loss: 1.9921 - val_accuracy: 1.0000\n",
            "Epoch 1789/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9953 - accuracy: 1.0000 - val_loss: 2.1605 - val_accuracy: 1.0000\n",
            "Epoch 1790/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9764 - accuracy: 1.0000 - val_loss: 1.9669 - val_accuracy: 1.0000\n",
            "Epoch 1791/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0052 - accuracy: 1.0000 - val_loss: 1.9666 - val_accuracy: 1.0000\n",
            "Epoch 1792/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9672 - accuracy: 1.0000 - val_loss: 2.0643 - val_accuracy: 1.0000\n",
            "Epoch 1793/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9944 - accuracy: 1.0000 - val_loss: 1.9757 - val_accuracy: 1.0000\n",
            "Epoch 1794/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9804 - accuracy: 1.0000 - val_loss: 2.0873 - val_accuracy: 1.0000\n",
            "Epoch 1795/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9939 - accuracy: 1.0000 - val_loss: 1.9718 - val_accuracy: 1.0000\n",
            "Epoch 1796/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9941 - accuracy: 1.0000 - val_loss: 1.9658 - val_accuracy: 1.0000\n",
            "Epoch 1797/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9685 - accuracy: 1.0000 - val_loss: 2.0524 - val_accuracy: 1.0000\n",
            "Epoch 1798/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9915 - accuracy: 1.0000 - val_loss: 1.9647 - val_accuracy: 1.0000\n",
            "Epoch 1799/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9779 - accuracy: 1.0000 - val_loss: 1.9613 - val_accuracy: 1.0000\n",
            "Epoch 1800/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9977 - accuracy: 1.0000 - val_loss: 1.9933 - val_accuracy: 1.0000\n",
            "Epoch 1801/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9781 - accuracy: 1.0000 - val_loss: 1.9703 - val_accuracy: 1.0000\n",
            "Epoch 1802/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9799 - accuracy: 1.0000 - val_loss: 2.1700 - val_accuracy: 1.0000\n",
            "Epoch 1803/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9866 - accuracy: 1.0000 - val_loss: 2.0899 - val_accuracy: 1.0000\n",
            "Epoch 1804/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9842 - accuracy: 1.0000 - val_loss: 1.9796 - val_accuracy: 1.0000\n",
            "Epoch 1805/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 2.0073 - accuracy: 1.0000 - val_loss: 1.9938 - val_accuracy: 1.0000\n",
            "Epoch 1806/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9669 - accuracy: 1.0000 - val_loss: 1.9753 - val_accuracy: 1.0000\n",
            "Epoch 1807/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9854 - accuracy: 1.0000 - val_loss: 1.9892 - val_accuracy: 1.0000\n",
            "Epoch 1808/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9850 - accuracy: 1.0000 - val_loss: 1.9824 - val_accuracy: 1.0000\n",
            "Epoch 1809/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9800 - accuracy: 1.0000 - val_loss: 1.9835 - val_accuracy: 1.0000\n",
            "Epoch 1810/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9859 - accuracy: 1.0000 - val_loss: 1.9793 - val_accuracy: 1.0000\n",
            "Epoch 1811/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9988 - accuracy: 1.0000 - val_loss: 2.0532 - val_accuracy: 1.0000\n",
            "Epoch 1812/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9680 - accuracy: 1.0000 - val_loss: 2.0471 - val_accuracy: 1.0000\n",
            "Epoch 1813/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9938 - accuracy: 1.0000 - val_loss: 1.9707 - val_accuracy: 1.0000\n",
            "Epoch 1814/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9736 - accuracy: 1.0000 - val_loss: 1.9916 - val_accuracy: 1.0000\n",
            "Epoch 1815/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9763 - accuracy: 1.0000 - val_loss: 2.0447 - val_accuracy: 1.0000\n",
            "Epoch 1816/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9823 - accuracy: 1.0000 - val_loss: 1.9606 - val_accuracy: 1.0000\n",
            "Epoch 1817/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9833 - accuracy: 1.0000 - val_loss: 2.0335 - val_accuracy: 1.0000\n",
            "Epoch 1818/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9934 - accuracy: 1.0000 - val_loss: 1.9828 - val_accuracy: 1.0000\n",
            "Epoch 1819/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9851 - accuracy: 1.0000 - val_loss: 2.0213 - val_accuracy: 1.0000\n",
            "Epoch 1820/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9685 - accuracy: 1.0000 - val_loss: 2.0987 - val_accuracy: 1.0000\n",
            "Epoch 1821/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9856 - accuracy: 1.0000 - val_loss: 1.9913 - val_accuracy: 1.0000\n",
            "Epoch 1822/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9735 - accuracy: 1.0000 - val_loss: 2.0075 - val_accuracy: 1.0000\n",
            "Epoch 1823/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9874 - accuracy: 1.0000 - val_loss: 2.0492 - val_accuracy: 1.0000\n",
            "Epoch 1824/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9816 - accuracy: 1.0000 - val_loss: 1.9658 - val_accuracy: 1.0000\n",
            "Epoch 1825/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9843 - accuracy: 1.0000 - val_loss: 1.9975 - val_accuracy: 1.0000\n",
            "Epoch 1826/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9769 - accuracy: 1.0000 - val_loss: 1.9652 - val_accuracy: 1.0000\n",
            "Epoch 1827/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 2.0012 - accuracy: 1.0000 - val_loss: 1.9683 - val_accuracy: 1.0000\n",
            "Epoch 1828/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9655 - accuracy: 1.0000 - val_loss: 1.9864 - val_accuracy: 1.0000\n",
            "Epoch 1829/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9881 - accuracy: 1.0000 - val_loss: 1.9595 - val_accuracy: 1.0000\n",
            "Epoch 1830/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9790 - accuracy: 1.0000 - val_loss: 2.0668 - val_accuracy: 1.0000\n",
            "Epoch 1831/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9759 - accuracy: 1.0000 - val_loss: 1.9646 - val_accuracy: 1.0000\n",
            "Epoch 1832/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9748 - accuracy: 1.0000 - val_loss: 1.9615 - val_accuracy: 1.0000\n",
            "Epoch 1833/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 2.0047 - accuracy: 1.0000 - val_loss: 2.0056 - val_accuracy: 1.0000\n",
            "Epoch 1834/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9644 - accuracy: 1.0000 - val_loss: 1.9779 - val_accuracy: 1.0000\n",
            "Epoch 1835/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9902 - accuracy: 1.0000 - val_loss: 2.0323 - val_accuracy: 1.0000\n",
            "Epoch 1836/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9645 - accuracy: 1.0000 - val_loss: 2.2704 - val_accuracy: 1.0000\n",
            "Epoch 1837/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9862 - accuracy: 1.0000 - val_loss: 2.0047 - val_accuracy: 1.0000\n",
            "Epoch 1838/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9786 - accuracy: 1.0000 - val_loss: 1.9974 - val_accuracy: 1.0000\n",
            "Epoch 1839/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9871 - accuracy: 1.0000 - val_loss: 2.0496 - val_accuracy: 1.0000\n",
            "Epoch 1840/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9973 - accuracy: 1.0000 - val_loss: 1.9636 - val_accuracy: 1.0000\n",
            "Epoch 1841/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9566 - accuracy: 1.0000 - val_loss: 2.0078 - val_accuracy: 1.0000\n",
            "Epoch 1842/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9982 - accuracy: 1.0000 - val_loss: 1.9845 - val_accuracy: 1.0000\n",
            "Epoch 1843/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9636 - accuracy: 1.0000 - val_loss: 2.0060 - val_accuracy: 1.0000\n",
            "Epoch 1844/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9892 - accuracy: 1.0000 - val_loss: 1.9809 - val_accuracy: 1.0000\n",
            "Epoch 1845/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9825 - accuracy: 1.0000 - val_loss: 1.9776 - val_accuracy: 1.0000\n",
            "Epoch 1846/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9760 - accuracy: 1.0000 - val_loss: 1.9743 - val_accuracy: 1.0000\n",
            "Epoch 1847/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9718 - accuracy: 1.0000 - val_loss: 1.9544 - val_accuracy: 1.0000\n",
            "Epoch 1848/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9858 - accuracy: 1.0000 - val_loss: 1.9707 - val_accuracy: 1.0000\n",
            "Epoch 1849/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9749 - accuracy: 1.0000 - val_loss: 1.9744 - val_accuracy: 1.0000\n",
            "Epoch 1850/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9882 - accuracy: 1.0000 - val_loss: 1.9741 - val_accuracy: 1.0000\n",
            "Epoch 1851/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9677 - accuracy: 1.0000 - val_loss: 2.0202 - val_accuracy: 1.0000\n",
            "Epoch 1852/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9835 - accuracy: 1.0000 - val_loss: 2.1188 - val_accuracy: 1.0000\n",
            "Epoch 1853/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9780 - accuracy: 1.0000 - val_loss: 2.0059 - val_accuracy: 1.0000\n",
            "Epoch 1854/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9803 - accuracy: 1.0000 - val_loss: 1.9646 - val_accuracy: 1.0000\n",
            "Epoch 1855/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9740 - accuracy: 1.0000 - val_loss: 2.0357 - val_accuracy: 1.0000\n",
            "Epoch 1856/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9817 - accuracy: 1.0000 - val_loss: 2.0122 - val_accuracy: 1.0000\n",
            "Epoch 1857/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9737 - accuracy: 1.0000 - val_loss: 2.0156 - val_accuracy: 1.0000\n",
            "Epoch 1858/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9802 - accuracy: 1.0000 - val_loss: 1.9858 - val_accuracy: 1.0000\n",
            "Epoch 1859/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9824 - accuracy: 1.0000 - val_loss: 1.9887 - val_accuracy: 1.0000\n",
            "Epoch 1860/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9818 - accuracy: 1.0000 - val_loss: 2.0058 - val_accuracy: 1.0000\n",
            "Epoch 1861/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9829 - accuracy: 1.0000 - val_loss: 2.0716 - val_accuracy: 1.0000\n",
            "Epoch 1862/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9792 - accuracy: 1.0000 - val_loss: 2.0039 - val_accuracy: 1.0000\n",
            "Epoch 1863/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9758 - accuracy: 1.0000 - val_loss: 1.9659 - val_accuracy: 1.0000\n",
            "Epoch 1864/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9776 - accuracy: 1.0000 - val_loss: 1.9819 - val_accuracy: 1.0000\n",
            "Epoch 1865/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9723 - accuracy: 1.0000 - val_loss: 2.0328 - val_accuracy: 1.0000\n",
            "Epoch 1866/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9844 - accuracy: 1.0000 - val_loss: 1.9546 - val_accuracy: 1.0000\n",
            "Epoch 1867/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9730 - accuracy: 1.0000 - val_loss: 2.0272 - val_accuracy: 1.0000\n",
            "Epoch 1868/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9762 - accuracy: 1.0000 - val_loss: 2.1546 - val_accuracy: 1.0000\n",
            "Epoch 1869/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9743 - accuracy: 1.0000 - val_loss: 1.9937 - val_accuracy: 1.0000\n",
            "Epoch 1870/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9760 - accuracy: 1.0000 - val_loss: 1.9930 - val_accuracy: 1.0000\n",
            "Epoch 1871/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9859 - accuracy: 1.0000 - val_loss: 1.9678 - val_accuracy: 1.0000\n",
            "Epoch 1872/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9701 - accuracy: 1.0000 - val_loss: 2.0409 - val_accuracy: 1.0000\n",
            "Epoch 1873/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9885 - accuracy: 1.0000 - val_loss: 2.0781 - val_accuracy: 1.0000\n",
            "Epoch 1874/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9790 - accuracy: 1.0000 - val_loss: 1.9602 - val_accuracy: 1.0000\n",
            "Epoch 1875/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9804 - accuracy: 1.0000 - val_loss: 2.0136 - val_accuracy: 1.0000\n",
            "Epoch 1876/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9757 - accuracy: 1.0000 - val_loss: 1.9786 - val_accuracy: 1.0000\n",
            "Epoch 1877/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9749 - accuracy: 1.0000 - val_loss: 2.0062 - val_accuracy: 1.0000\n",
            "Epoch 1878/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9748 - accuracy: 1.0000 - val_loss: 2.1567 - val_accuracy: 1.0000\n",
            "Epoch 1879/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9902 - accuracy: 1.0000 - val_loss: 2.0576 - val_accuracy: 1.0000\n",
            "Epoch 1880/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9815 - accuracy: 1.0000 - val_loss: 2.0300 - val_accuracy: 1.0000\n",
            "Epoch 1881/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9775 - accuracy: 1.0000 - val_loss: 1.9764 - val_accuracy: 1.0000\n",
            "Epoch 1882/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9724 - accuracy: 1.0000 - val_loss: 1.9921 - val_accuracy: 1.0000\n",
            "Epoch 1883/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9806 - accuracy: 1.0000 - val_loss: 1.9809 - val_accuracy: 1.0000\n",
            "Epoch 1884/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9720 - accuracy: 1.0000 - val_loss: 2.1000 - val_accuracy: 1.0000\n",
            "Epoch 1885/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9803 - accuracy: 1.0000 - val_loss: 1.9672 - val_accuracy: 1.0000\n",
            "Epoch 1886/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9773 - accuracy: 1.0000 - val_loss: 2.0176 - val_accuracy: 1.0000\n",
            "Epoch 1887/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9771 - accuracy: 1.0000 - val_loss: 1.9644 - val_accuracy: 1.0000\n",
            "Epoch 1888/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9703 - accuracy: 1.0000 - val_loss: 1.9689 - val_accuracy: 1.0000\n",
            "Epoch 1889/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9755 - accuracy: 1.0000 - val_loss: 1.9525 - val_accuracy: 1.0000\n",
            "Epoch 1890/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9697 - accuracy: 1.0000 - val_loss: 1.9799 - val_accuracy: 1.0000\n",
            "Epoch 1891/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9818 - accuracy: 1.0000 - val_loss: 1.9949 - val_accuracy: 1.0000\n",
            "Epoch 1892/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9809 - accuracy: 1.0000 - val_loss: 1.9910 - val_accuracy: 1.0000\n",
            "Epoch 1893/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9628 - accuracy: 1.0000 - val_loss: 1.9851 - val_accuracy: 1.0000\n",
            "Epoch 1894/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9740 - accuracy: 1.0000 - val_loss: 2.0303 - val_accuracy: 1.0000\n",
            "Epoch 1895/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9667 - accuracy: 1.0000 - val_loss: 1.9960 - val_accuracy: 1.0000\n",
            "Epoch 1896/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9657 - accuracy: 1.0000 - val_loss: 1.9654 - val_accuracy: 1.0000\n",
            "Epoch 1897/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9720 - accuracy: 1.0000 - val_loss: 1.9567 - val_accuracy: 1.0000\n",
            "Epoch 1898/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9792 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 1.0000\n",
            "Epoch 1899/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9648 - accuracy: 1.0000 - val_loss: 2.0151 - val_accuracy: 1.0000\n",
            "Epoch 1900/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9662 - accuracy: 1.0000 - val_loss: 2.1188 - val_accuracy: 1.0000\n",
            "Epoch 1901/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9685 - accuracy: 1.0000 - val_loss: 2.0658 - val_accuracy: 1.0000\n",
            "Epoch 1902/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9663 - accuracy: 1.0000 - val_loss: 1.9855 - val_accuracy: 1.0000\n",
            "Epoch 1903/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9634 - accuracy: 1.0000 - val_loss: 1.9575 - val_accuracy: 1.0000\n",
            "Epoch 1904/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9714 - accuracy: 1.0000 - val_loss: 2.0133 - val_accuracy: 1.0000\n",
            "Epoch 1905/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9677 - accuracy: 1.0000 - val_loss: 2.0469 - val_accuracy: 1.0000\n",
            "Epoch 1906/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9662 - accuracy: 1.0000 - val_loss: 1.9645 - val_accuracy: 1.0000\n",
            "Epoch 1907/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9739 - accuracy: 1.0000 - val_loss: 1.9694 - val_accuracy: 1.0000\n",
            "Epoch 1908/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9591 - accuracy: 1.0000 - val_loss: 1.9695 - val_accuracy: 1.0000\n",
            "Epoch 1909/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9644 - accuracy: 1.0000 - val_loss: 2.0511 - val_accuracy: 1.0000\n",
            "Epoch 1910/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9588 - accuracy: 1.0000 - val_loss: 1.9433 - val_accuracy: 1.0000\n",
            "Epoch 1911/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9623 - accuracy: 1.0000 - val_loss: 1.9993 - val_accuracy: 1.0000\n",
            "Epoch 1912/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9659 - accuracy: 1.0000 - val_loss: 1.9634 - val_accuracy: 1.0000\n",
            "Epoch 1913/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9679 - accuracy: 1.0000 - val_loss: 2.0063 - val_accuracy: 1.0000\n",
            "Epoch 1914/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9695 - accuracy: 1.0000 - val_loss: 2.0590 - val_accuracy: 1.0000\n",
            "Epoch 1915/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9594 - accuracy: 1.0000 - val_loss: 2.0178 - val_accuracy: 1.0000\n",
            "Epoch 1916/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9627 - accuracy: 1.0000 - val_loss: 2.0691 - val_accuracy: 1.0000\n",
            "Epoch 1917/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9626 - accuracy: 1.0000 - val_loss: 2.0555 - val_accuracy: 1.0000\n",
            "Epoch 1918/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9686 - accuracy: 1.0000 - val_loss: 2.0234 - val_accuracy: 1.0000\n",
            "Epoch 1919/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9525 - accuracy: 1.0000 - val_loss: 1.9643 - val_accuracy: 1.0000\n",
            "Epoch 1920/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9602 - accuracy: 1.0000 - val_loss: 2.2924 - val_accuracy: 0.9999\n",
            "Epoch 1921/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9594 - accuracy: 1.0000 - val_loss: 2.0376 - val_accuracy: 1.0000\n",
            "Epoch 1922/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9594 - accuracy: 1.0000 - val_loss: 2.0018 - val_accuracy: 1.0000\n",
            "Epoch 1923/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9738 - accuracy: 1.0000 - val_loss: 1.9428 - val_accuracy: 1.0000\n",
            "Epoch 1924/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9562 - accuracy: 1.0000 - val_loss: 2.0630 - val_accuracy: 1.0000\n",
            "Epoch 1925/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9601 - accuracy: 1.0000 - val_loss: 1.9619 - val_accuracy: 1.0000\n",
            "Epoch 1926/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9638 - accuracy: 1.0000 - val_loss: 1.9421 - val_accuracy: 1.0000\n",
            "Epoch 1927/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9509 - accuracy: 1.0000 - val_loss: 1.9903 - val_accuracy: 1.0000\n",
            "Epoch 1928/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9671 - accuracy: 1.0000 - val_loss: 1.9982 - val_accuracy: 1.0000\n",
            "Epoch 1929/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9532 - accuracy: 1.0000 - val_loss: 1.9592 - val_accuracy: 1.0000\n",
            "Epoch 1930/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9553 - accuracy: 1.0000 - val_loss: 1.9644 - val_accuracy: 1.0000\n",
            "Epoch 1931/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9664 - accuracy: 1.0000 - val_loss: 1.9793 - val_accuracy: 1.0000\n",
            "Epoch 1932/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9506 - accuracy: 1.0000 - val_loss: 1.9524 - val_accuracy: 1.0000\n",
            "Epoch 1933/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9670 - accuracy: 1.0000 - val_loss: 1.9396 - val_accuracy: 1.0000\n",
            "Epoch 1934/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9508 - accuracy: 1.0000 - val_loss: 1.9499 - val_accuracy: 1.0000\n",
            "Epoch 1935/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9507 - accuracy: 1.0000 - val_loss: 1.9612 - val_accuracy: 1.0000\n",
            "Epoch 1936/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9606 - accuracy: 1.0000 - val_loss: 1.9355 - val_accuracy: 1.0000\n",
            "Epoch 1937/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9545 - accuracy: 1.0000 - val_loss: 1.9617 - val_accuracy: 1.0000\n",
            "Epoch 1938/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9673 - accuracy: 1.0000 - val_loss: 2.0206 - val_accuracy: 1.0000\n",
            "Epoch 1939/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9396 - accuracy: 1.0000 - val_loss: 1.9447 - val_accuracy: 1.0000\n",
            "Epoch 1940/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9615 - accuracy: 1.0000 - val_loss: 1.9454 - val_accuracy: 1.0000\n",
            "Epoch 1941/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9626 - accuracy: 1.0000 - val_loss: 1.9512 - val_accuracy: 1.0000\n",
            "Epoch 1942/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9413 - accuracy: 1.0000 - val_loss: 1.9415 - val_accuracy: 1.0000\n",
            "Epoch 1943/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9547 - accuracy: 1.0000 - val_loss: 1.9451 - val_accuracy: 1.0000\n",
            "Epoch 1944/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9507 - accuracy: 1.0000 - val_loss: 1.9588 - val_accuracy: 1.0000\n",
            "Epoch 1945/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9647 - accuracy: 1.0000 - val_loss: 1.9430 - val_accuracy: 1.0000\n",
            "Epoch 1946/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9442 - accuracy: 1.0000 - val_loss: 1.9479 - val_accuracy: 1.0000\n",
            "Epoch 1947/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9455 - accuracy: 1.0000 - val_loss: 1.9499 - val_accuracy: 1.0000\n",
            "Epoch 1948/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9499 - accuracy: 1.0000 - val_loss: 2.1594 - val_accuracy: 1.0000\n",
            "Epoch 1949/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9530 - accuracy: 1.0000 - val_loss: 2.0099 - val_accuracy: 1.0000\n",
            "Epoch 1950/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9510 - accuracy: 1.0000 - val_loss: 1.9945 - val_accuracy: 1.0000\n",
            "Epoch 1951/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9475 - accuracy: 1.0000 - val_loss: 1.9321 - val_accuracy: 1.0000\n",
            "Epoch 1952/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9575 - accuracy: 1.0000 - val_loss: 2.0103 - val_accuracy: 0.9999\n",
            "Epoch 1953/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9481 - accuracy: 1.0000 - val_loss: 1.9290 - val_accuracy: 1.0000\n",
            "Epoch 1954/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9632 - accuracy: 1.0000 - val_loss: 1.9479 - val_accuracy: 1.0000\n",
            "Epoch 1955/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9282 - accuracy: 1.0000 - val_loss: 2.0232 - val_accuracy: 1.0000\n",
            "Epoch 1956/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9610 - accuracy: 1.0000 - val_loss: 1.9459 - val_accuracy: 1.0000\n",
            "Epoch 1957/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9426 - accuracy: 1.0000 - val_loss: 2.0302 - val_accuracy: 1.0000\n",
            "Epoch 1958/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9507 - accuracy: 1.0000 - val_loss: 2.2786 - val_accuracy: 1.0000\n",
            "Epoch 1959/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9498 - accuracy: 1.0000 - val_loss: 1.9556 - val_accuracy: 1.0000\n",
            "Epoch 1960/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9392 - accuracy: 1.0000 - val_loss: 1.9574 - val_accuracy: 1.0000\n",
            "Epoch 1961/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9491 - accuracy: 1.0000 - val_loss: 1.9477 - val_accuracy: 1.0000\n",
            "Epoch 1962/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9459 - accuracy: 1.0000 - val_loss: 1.9388 - val_accuracy: 1.0000\n",
            "Epoch 1963/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9459 - accuracy: 1.0000 - val_loss: 1.9511 - val_accuracy: 1.0000\n",
            "Epoch 1964/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9441 - accuracy: 1.0000 - val_loss: 1.9956 - val_accuracy: 1.0000\n",
            "Epoch 1965/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9500 - accuracy: 1.0000 - val_loss: 1.9271 - val_accuracy: 1.0000\n",
            "Epoch 1966/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9485 - accuracy: 1.0000 - val_loss: 1.9449 - val_accuracy: 1.0000\n",
            "Epoch 1967/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9332 - accuracy: 1.0000 - val_loss: 1.9203 - val_accuracy: 1.0000\n",
            "Epoch 1968/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9417 - accuracy: 1.0000 - val_loss: 2.3614 - val_accuracy: 1.0000\n",
            "Epoch 1969/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9540 - accuracy: 1.0000 - val_loss: 1.9401 - val_accuracy: 1.0000\n",
            "Epoch 1970/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9273 - accuracy: 1.0000 - val_loss: 1.9155 - val_accuracy: 1.0000\n",
            "Epoch 1971/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9428 - accuracy: 1.0000 - val_loss: 1.9179 - val_accuracy: 1.0000\n",
            "Epoch 1972/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9409 - accuracy: 1.0000 - val_loss: 1.9401 - val_accuracy: 1.0000\n",
            "Epoch 1973/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9372 - accuracy: 1.0000 - val_loss: 1.9238 - val_accuracy: 1.0000\n",
            "Epoch 1974/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9383 - accuracy: 1.0000 - val_loss: 2.0431 - val_accuracy: 1.0000\n",
            "Epoch 1975/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9538 - accuracy: 1.0000 - val_loss: 1.9372 - val_accuracy: 1.0000\n",
            "Epoch 1976/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9292 - accuracy: 1.0000 - val_loss: 1.9323 - val_accuracy: 1.0000\n",
            "Epoch 1977/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9399 - accuracy: 1.0000 - val_loss: 1.9180 - val_accuracy: 1.0000\n",
            "Epoch 1978/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9382 - accuracy: 1.0000 - val_loss: 1.9912 - val_accuracy: 0.9999\n",
            "Epoch 1979/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9585 - accuracy: 1.0000 - val_loss: 1.9494 - val_accuracy: 1.0000\n",
            "Epoch 1980/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9322 - accuracy: 1.0000 - val_loss: 2.1096 - val_accuracy: 1.0000\n",
            "Epoch 1981/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9494 - accuracy: 1.0000 - val_loss: 1.9516 - val_accuracy: 1.0000\n",
            "Epoch 1982/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9467 - accuracy: 1.0000 - val_loss: 1.9739 - val_accuracy: 1.0000\n",
            "Epoch 1983/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9277 - accuracy: 1.0000 - val_loss: 2.0854 - val_accuracy: 1.0000\n",
            "Epoch 1984/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9392 - accuracy: 1.0000 - val_loss: 1.9694 - val_accuracy: 1.0000\n",
            "Epoch 1985/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9601 - accuracy: 1.0000 - val_loss: 1.9173 - val_accuracy: 1.0000\n",
            "Epoch 1986/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.9408 - val_accuracy: 1.0000\n",
            "Epoch 1987/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9462 - accuracy: 1.0000 - val_loss: 2.0453 - val_accuracy: 1.0000\n",
            "Epoch 1988/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9317 - accuracy: 1.0000 - val_loss: 1.9469 - val_accuracy: 1.0000\n",
            "Epoch 1989/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9395 - accuracy: 1.0000 - val_loss: 1.9683 - val_accuracy: 1.0000\n",
            "Epoch 1990/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9411 - accuracy: 1.0000 - val_loss: 1.9318 - val_accuracy: 1.0000\n",
            "Epoch 1991/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9276 - accuracy: 1.0000 - val_loss: 1.9432 - val_accuracy: 1.0000\n",
            "Epoch 1992/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9366 - accuracy: 1.0000 - val_loss: 1.9501 - val_accuracy: 1.0000\n",
            "Epoch 1993/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9382 - accuracy: 1.0000 - val_loss: 1.9376 - val_accuracy: 1.0000\n",
            "Epoch 1994/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9420 - accuracy: 1.0000 - val_loss: 1.9345 - val_accuracy: 1.0000\n",
            "Epoch 1995/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9238 - accuracy: 1.0000 - val_loss: 1.9409 - val_accuracy: 1.0000\n",
            "Epoch 1996/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9357 - accuracy: 1.0000 - val_loss: 1.9500 - val_accuracy: 1.0000\n",
            "Epoch 1997/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9545 - accuracy: 1.0000 - val_loss: 1.9277 - val_accuracy: 1.0000\n",
            "Epoch 1998/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9257 - accuracy: 1.0000 - val_loss: 1.9186 - val_accuracy: 1.0000\n",
            "Epoch 1999/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9347 - accuracy: 1.0000 - val_loss: 2.0154 - val_accuracy: 1.0000\n",
            "Epoch 2000/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9413 - accuracy: 1.0000 - val_loss: 1.9272 - val_accuracy: 1.0000\n",
            "Epoch 2001/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9313 - accuracy: 1.0000 - val_loss: 1.9240 - val_accuracy: 1.0000\n",
            "Epoch 2002/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9317 - accuracy: 1.0000 - val_loss: 1.9213 - val_accuracy: 1.0000\n",
            "Epoch 2003/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9317 - accuracy: 1.0000 - val_loss: 1.9305 - val_accuracy: 1.0000\n",
            "Epoch 2004/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9576 - accuracy: 1.0000 - val_loss: 1.9547 - val_accuracy: 1.0000\n",
            "Epoch 2005/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9244 - accuracy: 1.0000 - val_loss: 2.1458 - val_accuracy: 1.0000\n",
            "Epoch 2006/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9398 - accuracy: 1.0000 - val_loss: 1.9629 - val_accuracy: 1.0000\n",
            "Epoch 2007/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9251 - accuracy: 1.0000 - val_loss: 2.2251 - val_accuracy: 1.0000\n",
            "Epoch 2008/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9455 - accuracy: 1.0000 - val_loss: 1.9219 - val_accuracy: 1.0000\n",
            "Epoch 2009/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9244 - accuracy: 1.0000 - val_loss: 1.9113 - val_accuracy: 1.0000\n",
            "Epoch 2010/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9338 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 2011/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9375 - accuracy: 1.0000 - val_loss: 1.9293 - val_accuracy: 1.0000\n",
            "Epoch 2012/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9277 - accuracy: 1.0000 - val_loss: 1.9347 - val_accuracy: 1.0000\n",
            "Epoch 2013/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 1.9694 - val_accuracy: 1.0000\n",
            "Epoch 2014/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9355 - accuracy: 1.0000 - val_loss: 1.9297 - val_accuracy: 1.0000\n",
            "Epoch 2015/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9305 - accuracy: 1.0000 - val_loss: 1.9385 - val_accuracy: 1.0000\n",
            "Epoch 2016/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9388 - accuracy: 1.0000 - val_loss: 1.9052 - val_accuracy: 1.0000\n",
            "Epoch 2017/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9256 - accuracy: 1.0000 - val_loss: 1.9147 - val_accuracy: 1.0000\n",
            "Epoch 2018/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9258 - accuracy: 1.0000 - val_loss: 1.9595 - val_accuracy: 1.0000\n",
            "Epoch 2019/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9326 - accuracy: 1.0000 - val_loss: 1.9156 - val_accuracy: 1.0000\n",
            "Epoch 2020/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9277 - accuracy: 1.0000 - val_loss: 1.9093 - val_accuracy: 1.0000\n",
            "Epoch 2021/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9310 - accuracy: 1.0000 - val_loss: 1.9305 - val_accuracy: 1.0000\n",
            "Epoch 2022/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9271 - accuracy: 1.0000 - val_loss: 2.1462 - val_accuracy: 1.0000\n",
            "Epoch 2023/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9361 - accuracy: 1.0000 - val_loss: 1.9568 - val_accuracy: 1.0000\n",
            "Epoch 2024/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9315 - accuracy: 1.0000 - val_loss: 2.4151 - val_accuracy: 1.0000\n",
            "Epoch 2025/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9393 - accuracy: 1.0000 - val_loss: 1.9023 - val_accuracy: 1.0000\n",
            "Epoch 2026/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9243 - accuracy: 1.0000 - val_loss: 1.9767 - val_accuracy: 1.0000\n",
            "Epoch 2027/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 1.9385 - val_accuracy: 1.0000\n",
            "Epoch 2028/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9304 - accuracy: 1.0000 - val_loss: 1.9207 - val_accuracy: 1.0000\n",
            "Epoch 2029/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9348 - accuracy: 1.0000 - val_loss: 1.9192 - val_accuracy: 1.0000\n",
            "Epoch 2030/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9299 - accuracy: 1.0000 - val_loss: 1.9287 - val_accuracy: 1.0000\n",
            "Epoch 2031/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9416 - accuracy: 1.0000 - val_loss: 1.9038 - val_accuracy: 1.0000\n",
            "Epoch 2032/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9229 - accuracy: 1.0000 - val_loss: 1.9773 - val_accuracy: 1.0000\n",
            "Epoch 2033/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9276 - accuracy: 1.0000 - val_loss: 1.9635 - val_accuracy: 1.0000\n",
            "Epoch 2034/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9317 - accuracy: 1.0000 - val_loss: 1.9288 - val_accuracy: 1.0000\n",
            "Epoch 2035/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9235 - accuracy: 1.0000 - val_loss: 1.9306 - val_accuracy: 1.0000\n",
            "Epoch 2036/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9305 - accuracy: 1.0000 - val_loss: 1.9316 - val_accuracy: 1.0000\n",
            "Epoch 2037/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9238 - accuracy: 1.0000 - val_loss: 1.9114 - val_accuracy: 1.0000\n",
            "Epoch 2038/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9316 - accuracy: 1.0000 - val_loss: 1.9279 - val_accuracy: 1.0000\n",
            "Epoch 2039/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9305 - accuracy: 1.0000 - val_loss: 1.9133 - val_accuracy: 1.0000\n",
            "Epoch 2040/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.9026 - val_accuracy: 1.0000\n",
            "Epoch 2041/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9396 - accuracy: 1.0000 - val_loss: 1.9024 - val_accuracy: 1.0000\n",
            "Epoch 2042/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9495 - val_accuracy: 1.0000\n",
            "Epoch 2043/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9276 - accuracy: 1.0000 - val_loss: 1.9130 - val_accuracy: 1.0000\n",
            "Epoch 2044/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9244 - accuracy: 1.0000 - val_loss: 1.9208 - val_accuracy: 1.0000\n",
            "Epoch 2045/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9309 - accuracy: 1.0000 - val_loss: 1.9948 - val_accuracy: 1.0000\n",
            "Epoch 2046/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9296 - accuracy: 1.0000 - val_loss: 2.0031 - val_accuracy: 1.0000\n",
            "Epoch 2047/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9288 - accuracy: 1.0000 - val_loss: 1.9486 - val_accuracy: 1.0000\n",
            "Epoch 2048/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9211 - accuracy: 1.0000 - val_loss: 1.9759 - val_accuracy: 1.0000\n",
            "Epoch 2049/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9305 - accuracy: 1.0000 - val_loss: 1.9662 - val_accuracy: 1.0000\n",
            "Epoch 2050/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9273 - accuracy: 1.0000 - val_loss: 1.9980 - val_accuracy: 1.0000\n",
            "Epoch 2051/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9322 - accuracy: 1.0000 - val_loss: 2.0215 - val_accuracy: 1.0000\n",
            "Epoch 2052/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9212 - accuracy: 1.0000 - val_loss: 1.9006 - val_accuracy: 1.0000\n",
            "Epoch 2053/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9404 - accuracy: 1.0000 - val_loss: 1.9404 - val_accuracy: 1.0000\n",
            "Epoch 2054/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9164 - accuracy: 1.0000 - val_loss: 1.9700 - val_accuracy: 1.0000\n",
            "Epoch 2055/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9277 - accuracy: 1.0000 - val_loss: 1.9385 - val_accuracy: 1.0000\n",
            "Epoch 2056/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9270 - accuracy: 1.0000 - val_loss: 1.9070 - val_accuracy: 1.0000\n",
            "Epoch 2057/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9206 - accuracy: 1.0000 - val_loss: 1.8986 - val_accuracy: 1.0000\n",
            "Epoch 2058/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9226 - accuracy: 1.0000 - val_loss: 1.9321 - val_accuracy: 1.0000\n",
            "Epoch 2059/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9298 - accuracy: 1.0000 - val_loss: 1.9094 - val_accuracy: 1.0000\n",
            "Epoch 2060/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9246 - accuracy: 1.0000 - val_loss: 1.9266 - val_accuracy: 1.0000\n",
            "Epoch 2061/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9331 - accuracy: 1.0000 - val_loss: 2.1861 - val_accuracy: 1.0000\n",
            "Epoch 2062/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9290 - accuracy: 1.0000 - val_loss: 1.9223 - val_accuracy: 1.0000\n",
            "Epoch 2063/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9588 - accuracy: 1.0000 - val_loss: 1.9000 - val_accuracy: 1.0000\n",
            "Epoch 2064/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9078 - accuracy: 1.0000 - val_loss: 1.9856 - val_accuracy: 1.0000\n",
            "Epoch 2065/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9197 - accuracy: 1.0000 - val_loss: 1.9179 - val_accuracy: 1.0000\n",
            "Epoch 2066/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9268 - accuracy: 1.0000 - val_loss: 1.9062 - val_accuracy: 1.0000\n",
            "Epoch 2067/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9206 - accuracy: 1.0000 - val_loss: 1.9126 - val_accuracy: 1.0000\n",
            "Epoch 2068/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9383 - accuracy: 1.0000 - val_loss: 1.9092 - val_accuracy: 1.0000\n",
            "Epoch 2069/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 2.0276 - val_accuracy: 1.0000\n",
            "Epoch 2070/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9282 - accuracy: 1.0000 - val_loss: 1.9087 - val_accuracy: 1.0000\n",
            "Epoch 2071/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9256 - accuracy: 1.0000 - val_loss: 1.9468 - val_accuracy: 1.0000\n",
            "Epoch 2072/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9213 - accuracy: 1.0000 - val_loss: 1.9111 - val_accuracy: 1.0000\n",
            "Epoch 2073/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9207 - accuracy: 1.0000 - val_loss: 1.9721 - val_accuracy: 1.0000\n",
            "Epoch 2074/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9317 - accuracy: 1.0000 - val_loss: 1.9864 - val_accuracy: 1.0000\n",
            "Epoch 2075/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.9513 - val_accuracy: 1.0000\n",
            "Epoch 2076/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9423 - accuracy: 1.0000 - val_loss: 1.9078 - val_accuracy: 1.0000\n",
            "Epoch 2077/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9095 - accuracy: 1.0000 - val_loss: 1.9756 - val_accuracy: 1.0000\n",
            "Epoch 2078/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 2.0349 - val_accuracy: 1.0000\n",
            "Epoch 2079/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9330 - accuracy: 1.0000 - val_loss: 1.9256 - val_accuracy: 1.0000\n",
            "Epoch 2080/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9223 - accuracy: 1.0000 - val_loss: 1.8921 - val_accuracy: 1.0000\n",
            "Epoch 2081/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9237 - accuracy: 1.0000 - val_loss: 1.9040 - val_accuracy: 1.0000\n",
            "Epoch 2082/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9237 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 2083/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9423 - accuracy: 1.0000 - val_loss: 1.9626 - val_accuracy: 1.0000\n",
            "Epoch 2084/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9227 - accuracy: 1.0000 - val_loss: 1.9511 - val_accuracy: 1.0000\n",
            "Epoch 2085/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9247 - accuracy: 1.0000 - val_loss: 1.9446 - val_accuracy: 1.0000\n",
            "Epoch 2086/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 1.9026 - val_accuracy: 1.0000\n",
            "Epoch 2087/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.9205 - val_accuracy: 1.0000\n",
            "Epoch 2088/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9287 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 1.0000\n",
            "Epoch 2089/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9175 - accuracy: 1.0000 - val_loss: 1.9428 - val_accuracy: 1.0000\n",
            "Epoch 2090/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9350 - accuracy: 1.0000 - val_loss: 1.9007 - val_accuracy: 1.0000\n",
            "Epoch 2091/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9175 - accuracy: 1.0000 - val_loss: 1.9243 - val_accuracy: 1.0000\n",
            "Epoch 2092/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9231 - accuracy: 1.0000 - val_loss: 1.9069 - val_accuracy: 1.0000\n",
            "Epoch 2093/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9256 - accuracy: 1.0000 - val_loss: 2.0511 - val_accuracy: 1.0000\n",
            "Epoch 2094/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9474 - accuracy: 1.0000 - val_loss: 1.9005 - val_accuracy: 1.0000\n",
            "Epoch 2095/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9067 - accuracy: 1.0000 - val_loss: 1.9100 - val_accuracy: 1.0000\n",
            "Epoch 2096/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9205 - accuracy: 1.0000 - val_loss: 1.9369 - val_accuracy: 1.0000\n",
            "Epoch 2097/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9220 - accuracy: 1.0000 - val_loss: 1.9404 - val_accuracy: 1.0000\n",
            "Epoch 2098/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9305 - accuracy: 1.0000 - val_loss: 1.9596 - val_accuracy: 1.0000\n",
            "Epoch 2099/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9139 - accuracy: 1.0000 - val_loss: 1.9606 - val_accuracy: 1.0000\n",
            "Epoch 2100/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9334 - accuracy: 1.0000 - val_loss: 1.9689 - val_accuracy: 1.0000\n",
            "Epoch 2101/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9110 - accuracy: 1.0000 - val_loss: 1.9312 - val_accuracy: 1.0000\n",
            "Epoch 2102/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9295 - accuracy: 1.0000 - val_loss: 1.9612 - val_accuracy: 1.0000\n",
            "Epoch 2103/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9439 - accuracy: 1.0000 - val_loss: 1.9056 - val_accuracy: 1.0000\n",
            "Epoch 2104/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9104 - accuracy: 1.0000 - val_loss: 1.9255 - val_accuracy: 1.0000\n",
            "Epoch 2105/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9270 - accuracy: 1.0000 - val_loss: 1.9238 - val_accuracy: 1.0000\n",
            "Epoch 2106/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.9464 - val_accuracy: 1.0000\n",
            "Epoch 2107/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9238 - accuracy: 1.0000 - val_loss: 1.8950 - val_accuracy: 1.0000\n",
            "Epoch 2108/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9380 - accuracy: 1.0000 - val_loss: 2.0188 - val_accuracy: 1.0000\n",
            "Epoch 2109/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.9142 - val_accuracy: 1.0000\n",
            "Epoch 2110/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.9300 - val_accuracy: 1.0000\n",
            "Epoch 2111/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9230 - accuracy: 1.0000 - val_loss: 1.9010 - val_accuracy: 1.0000\n",
            "Epoch 2112/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9404 - accuracy: 1.0000 - val_loss: 1.9680 - val_accuracy: 1.0000\n",
            "Epoch 2113/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9086 - accuracy: 1.0000 - val_loss: 1.9073 - val_accuracy: 1.0000\n",
            "Epoch 2114/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9357 - accuracy: 1.0000 - val_loss: 1.8966 - val_accuracy: 1.0000\n",
            "Epoch 2115/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9224 - accuracy: 1.0000 - val_loss: 1.9279 - val_accuracy: 1.0000\n",
            "Epoch 2116/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9193 - accuracy: 1.0000 - val_loss: 1.8937 - val_accuracy: 1.0000\n",
            "Epoch 2117/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9193 - accuracy: 1.0000 - val_loss: 1.9162 - val_accuracy: 1.0000\n",
            "Epoch 2118/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.9237 - val_accuracy: 1.0000\n",
            "Epoch 2119/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.9566 - val_accuracy: 1.0000\n",
            "Epoch 2120/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9229 - accuracy: 1.0000 - val_loss: 1.9217 - val_accuracy: 1.0000\n",
            "Epoch 2121/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 1.9532 - val_accuracy: 1.0000\n",
            "Epoch 2122/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9248 - accuracy: 1.0000 - val_loss: 1.9170 - val_accuracy: 1.0000\n",
            "Epoch 2123/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9309 - accuracy: 1.0000 - val_loss: 1.9228 - val_accuracy: 1.0000\n",
            "Epoch 2124/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 1.9671 - val_accuracy: 1.0000\n",
            "Epoch 2125/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9257 - accuracy: 1.0000 - val_loss: 2.0286 - val_accuracy: 1.0000\n",
            "Epoch 2126/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9242 - accuracy: 1.0000 - val_loss: 1.9329 - val_accuracy: 1.0000\n",
            "Epoch 2127/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.9211 - val_accuracy: 1.0000\n",
            "Epoch 2128/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9187 - accuracy: 1.0000 - val_loss: 1.9089 - val_accuracy: 1.0000\n",
            "Epoch 2129/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9201 - accuracy: 1.0000 - val_loss: 1.8974 - val_accuracy: 1.0000\n",
            "Epoch 2130/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9327 - accuracy: 1.0000 - val_loss: 1.9154 - val_accuracy: 1.0000\n",
            "Epoch 2131/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9130 - accuracy: 1.0000 - val_loss: 1.9339 - val_accuracy: 1.0000\n",
            "Epoch 2132/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9293 - accuracy: 1.0000 - val_loss: 1.9052 - val_accuracy: 1.0000\n",
            "Epoch 2133/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9195 - accuracy: 1.0000 - val_loss: 1.8969 - val_accuracy: 1.0000\n",
            "Epoch 2134/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9140 - accuracy: 1.0000 - val_loss: 2.0465 - val_accuracy: 1.0000\n",
            "Epoch 2135/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9212 - accuracy: 1.0000 - val_loss: 1.9176 - val_accuracy: 1.0000\n",
            "Epoch 2136/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9189 - accuracy: 1.0000 - val_loss: 1.9024 - val_accuracy: 1.0000\n",
            "Epoch 2137/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9255 - accuracy: 1.0000 - val_loss: 1.9285 - val_accuracy: 1.0000\n",
            "Epoch 2138/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 1.9199 - val_accuracy: 1.0000\n",
            "Epoch 2139/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9183 - accuracy: 1.0000 - val_loss: 1.9548 - val_accuracy: 1.0000\n",
            "Epoch 2140/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9182 - accuracy: 1.0000 - val_loss: 1.9700 - val_accuracy: 1.0000\n",
            "Epoch 2141/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 1.9475 - val_accuracy: 1.0000\n",
            "Epoch 2142/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9070 - val_accuracy: 1.0000\n",
            "Epoch 2143/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9470 - accuracy: 1.0000 - val_loss: 1.9128 - val_accuracy: 1.0000\n",
            "Epoch 2144/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 1.9604 - val_accuracy: 1.0000\n",
            "Epoch 2145/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 2.0501 - val_accuracy: 1.0000\n",
            "Epoch 2146/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9178 - accuracy: 1.0000 - val_loss: 2.4590 - val_accuracy: 1.0000\n",
            "Epoch 2147/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9482 - accuracy: 1.0000 - val_loss: 1.9033 - val_accuracy: 1.0000\n",
            "Epoch 2148/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9031 - accuracy: 1.0000 - val_loss: 1.8981 - val_accuracy: 1.0000\n",
            "Epoch 2149/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9160 - accuracy: 1.0000 - val_loss: 1.9056 - val_accuracy: 1.0000\n",
            "Epoch 2150/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9232 - accuracy: 1.0000 - val_loss: 1.9104 - val_accuracy: 1.0000\n",
            "Epoch 2151/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 1.9127 - val_accuracy: 1.0000\n",
            "Epoch 2152/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9713 - val_accuracy: 1.0000\n",
            "Epoch 2153/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9300 - accuracy: 1.0000 - val_loss: 1.9544 - val_accuracy: 1.0000\n",
            "Epoch 2154/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.9050 - val_accuracy: 1.0000\n",
            "Epoch 2155/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9140 - accuracy: 1.0000 - val_loss: 2.0381 - val_accuracy: 1.0000\n",
            "Epoch 2156/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9360 - accuracy: 1.0000 - val_loss: 1.9837 - val_accuracy: 1.0000\n",
            "Epoch 2157/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.9203 - val_accuracy: 1.0000\n",
            "Epoch 2158/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9253 - accuracy: 1.0000 - val_loss: 1.9072 - val_accuracy: 1.0000\n",
            "Epoch 2159/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9269 - accuracy: 1.0000 - val_loss: 1.9277 - val_accuracy: 1.0000\n",
            "Epoch 2160/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.9047 - val_accuracy: 1.0000\n",
            "Epoch 2161/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9207 - accuracy: 1.0000 - val_loss: 1.9220 - val_accuracy: 1.0000\n",
            "Epoch 2162/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9124 - accuracy: 1.0000 - val_loss: 2.1891 - val_accuracy: 1.0000\n",
            "Epoch 2163/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9366 - accuracy: 1.0000 - val_loss: 1.9384 - val_accuracy: 1.0000\n",
            "Epoch 2164/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9291 - accuracy: 1.0000 - val_loss: 1.9090 - val_accuracy: 1.0000\n",
            "Epoch 2165/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9082 - accuracy: 1.0000 - val_loss: 1.8977 - val_accuracy: 1.0000\n",
            "Epoch 2166/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9016 - val_accuracy: 1.0000\n",
            "Epoch 2167/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9172 - accuracy: 1.0000 - val_loss: 1.9368 - val_accuracy: 1.0000\n",
            "Epoch 2168/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9234 - accuracy: 1.0000 - val_loss: 1.8965 - val_accuracy: 1.0000\n",
            "Epoch 2169/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.9559 - val_accuracy: 1.0000\n",
            "Epoch 2170/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 2.1321 - val_accuracy: 1.0000\n",
            "Epoch 2171/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9340 - accuracy: 1.0000 - val_loss: 1.9569 - val_accuracy: 1.0000\n",
            "Epoch 2172/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.8974 - val_accuracy: 1.0000\n",
            "Epoch 2173/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9208 - accuracy: 1.0000 - val_loss: 1.9410 - val_accuracy: 0.9999\n",
            "Epoch 2174/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9184 - accuracy: 1.0000 - val_loss: 1.9551 - val_accuracy: 1.0000\n",
            "Epoch 2175/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.9423 - val_accuracy: 1.0000\n",
            "Epoch 2176/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9265 - accuracy: 1.0000 - val_loss: 1.9033 - val_accuracy: 1.0000\n",
            "Epoch 2177/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9295 - accuracy: 1.0000 - val_loss: 1.8932 - val_accuracy: 1.0000\n",
            "Epoch 2178/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9003 - accuracy: 1.0000 - val_loss: 2.1335 - val_accuracy: 1.0000\n",
            "Epoch 2179/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9227 - accuracy: 1.0000 - val_loss: 1.9095 - val_accuracy: 1.0000\n",
            "Epoch 2180/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9205 - accuracy: 1.0000 - val_loss: 1.9637 - val_accuracy: 1.0000\n",
            "Epoch 2181/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9132 - accuracy: 1.0000 - val_loss: 1.9044 - val_accuracy: 1.0000\n",
            "Epoch 2182/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9227 - accuracy: 1.0000 - val_loss: 1.9373 - val_accuracy: 1.0000\n",
            "Epoch 2183/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9235 - accuracy: 1.0000 - val_loss: 1.9152 - val_accuracy: 1.0000\n",
            "Epoch 2184/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 1.9096 - val_accuracy: 1.0000\n",
            "Epoch 2185/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 1.8996 - val_accuracy: 1.0000\n",
            "Epoch 2186/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.9355 - val_accuracy: 1.0000\n",
            "Epoch 2187/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.9113 - val_accuracy: 1.0000\n",
            "Epoch 2188/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9159 - accuracy: 1.0000 - val_loss: 1.8888 - val_accuracy: 1.0000\n",
            "Epoch 2189/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9268 - accuracy: 1.0000 - val_loss: 2.0913 - val_accuracy: 1.0000\n",
            "Epoch 2190/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9141 - accuracy: 1.0000 - val_loss: 1.8884 - val_accuracy: 1.0000\n",
            "Epoch 2191/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9519 - val_accuracy: 1.0000\n",
            "Epoch 2192/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 1.9176 - val_accuracy: 1.0000\n",
            "Epoch 2193/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9198 - accuracy: 1.0000 - val_loss: 1.9189 - val_accuracy: 1.0000\n",
            "Epoch 2194/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 2.0683 - val_accuracy: 1.0000\n",
            "Epoch 2195/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9195 - accuracy: 1.0000 - val_loss: 1.9399 - val_accuracy: 1.0000\n",
            "Epoch 2196/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9276 - accuracy: 1.0000 - val_loss: 1.8993 - val_accuracy: 1.0000\n",
            "Epoch 2197/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9075 - accuracy: 1.0000 - val_loss: 1.8959 - val_accuracy: 1.0000\n",
            "Epoch 2198/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9240 - accuracy: 1.0000 - val_loss: 1.9236 - val_accuracy: 1.0000\n",
            "Epoch 2199/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9126 - accuracy: 1.0000 - val_loss: 2.0372 - val_accuracy: 1.0000\n",
            "Epoch 2200/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9216 - accuracy: 1.0000 - val_loss: 1.8989 - val_accuracy: 1.0000\n",
            "Epoch 2201/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9049 - accuracy: 1.0000 - val_loss: 1.9670 - val_accuracy: 1.0000\n",
            "Epoch 2202/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9199 - accuracy: 1.0000 - val_loss: 1.9409 - val_accuracy: 1.0000\n",
            "Epoch 2203/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9215 - accuracy: 1.0000 - val_loss: 1.9027 - val_accuracy: 1.0000\n",
            "Epoch 2204/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9066 - accuracy: 1.0000 - val_loss: 1.9108 - val_accuracy: 1.0000\n",
            "Epoch 2205/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9179 - accuracy: 1.0000 - val_loss: 1.9539 - val_accuracy: 1.0000\n",
            "Epoch 2206/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9261 - accuracy: 1.0000 - val_loss: 1.8913 - val_accuracy: 1.0000\n",
            "Epoch 2207/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.9020 - val_accuracy: 1.0000\n",
            "Epoch 2208/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9205 - accuracy: 1.0000 - val_loss: 2.0490 - val_accuracy: 1.0000\n",
            "Epoch 2209/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9239 - accuracy: 1.0000 - val_loss: 1.8938 - val_accuracy: 1.0000\n",
            "Epoch 2210/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9102 - accuracy: 1.0000 - val_loss: 1.9322 - val_accuracy: 1.0000\n",
            "Epoch 2211/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.9012 - val_accuracy: 1.0000\n",
            "Epoch 2212/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9231 - accuracy: 1.0000 - val_loss: 1.9411 - val_accuracy: 1.0000\n",
            "Epoch 2213/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9474 - accuracy: 1.0000 - val_loss: 1.9789 - val_accuracy: 0.9999\n",
            "Epoch 2214/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9013 - accuracy: 1.0000 - val_loss: 1.8934 - val_accuracy: 1.0000\n",
            "Epoch 2215/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.9727 - val_accuracy: 1.0000\n",
            "Epoch 2216/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8995 - accuracy: 1.0000 - val_loss: 1.9058 - val_accuracy: 1.0000\n",
            "Epoch 2217/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9210 - accuracy: 1.0000 - val_loss: 1.9139 - val_accuracy: 1.0000\n",
            "Epoch 2218/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9441 - accuracy: 1.0000 - val_loss: 1.8946 - val_accuracy: 1.0000\n",
            "Epoch 2219/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.8994 - accuracy: 1.0000 - val_loss: 1.9001 - val_accuracy: 1.0000\n",
            "Epoch 2220/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.8948 - val_accuracy: 1.0000\n",
            "Epoch 2221/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9145 - accuracy: 1.0000 - val_loss: 1.9192 - val_accuracy: 1.0000\n",
            "Epoch 2222/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9161 - accuracy: 1.0000 - val_loss: 1.9062 - val_accuracy: 1.0000\n",
            "Epoch 2223/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9106 - accuracy: 1.0000 - val_loss: 1.9590 - val_accuracy: 1.0000\n",
            "Epoch 2224/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9133 - accuracy: 1.0000 - val_loss: 2.0117 - val_accuracy: 1.0000\n",
            "Epoch 2225/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9245 - accuracy: 1.0000 - val_loss: 1.9176 - val_accuracy: 1.0000\n",
            "Epoch 2226/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9209 - accuracy: 1.0000 - val_loss: 1.9067 - val_accuracy: 1.0000\n",
            "Epoch 2227/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9121 - accuracy: 1.0000 - val_loss: 1.8878 - val_accuracy: 1.0000\n",
            "Epoch 2228/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9141 - accuracy: 1.0000 - val_loss: 1.9115 - val_accuracy: 1.0000\n",
            "Epoch 2229/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9200 - accuracy: 1.0000 - val_loss: 1.9543 - val_accuracy: 1.0000\n",
            "Epoch 2230/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.8973 - val_accuracy: 1.0000\n",
            "Epoch 2231/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9179 - accuracy: 1.0000 - val_loss: 1.9234 - val_accuracy: 1.0000\n",
            "Epoch 2232/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9225 - accuracy: 1.0000 - val_loss: 1.9689 - val_accuracy: 1.0000\n",
            "Epoch 2233/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9079 - val_accuracy: 1.0000\n",
            "Epoch 2234/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9279 - accuracy: 1.0000 - val_loss: 1.9420 - val_accuracy: 1.0000\n",
            "Epoch 2235/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9115 - accuracy: 1.0000 - val_loss: 1.9031 - val_accuracy: 1.0000\n",
            "Epoch 2236/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9051 - accuracy: 1.0000 - val_loss: 1.9322 - val_accuracy: 1.0000\n",
            "Epoch 2237/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 1.8986 - val_accuracy: 1.0000\n",
            "Epoch 2238/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.9840 - val_accuracy: 1.0000\n",
            "Epoch 2239/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9280 - accuracy: 1.0000 - val_loss: 1.9443 - val_accuracy: 1.0000\n",
            "Epoch 2240/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9074 - accuracy: 1.0000 - val_loss: 1.9037 - val_accuracy: 1.0000\n",
            "Epoch 2241/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9154 - accuracy: 1.0000 - val_loss: 2.0762 - val_accuracy: 1.0000\n",
            "Epoch 2242/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9171 - accuracy: 1.0000 - val_loss: 1.8970 - val_accuracy: 1.0000\n",
            "Epoch 2243/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 2.0219 - val_accuracy: 1.0000\n",
            "Epoch 2244/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9155 - accuracy: 1.0000 - val_loss: 1.9199 - val_accuracy: 1.0000\n",
            "Epoch 2245/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9229 - accuracy: 1.0000 - val_loss: 1.8984 - val_accuracy: 1.0000\n",
            "Epoch 2246/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9127 - accuracy: 1.0000 - val_loss: 1.9109 - val_accuracy: 1.0000\n",
            "Epoch 2247/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 1.8948 - val_accuracy: 1.0000\n",
            "Epoch 2248/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9139 - accuracy: 1.0000 - val_loss: 1.8896 - val_accuracy: 1.0000\n",
            "Epoch 2249/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.9087 - val_accuracy: 1.0000\n",
            "Epoch 2250/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9187 - accuracy: 1.0000 - val_loss: 1.8934 - val_accuracy: 1.0000\n",
            "Epoch 2251/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9601 - val_accuracy: 1.0000\n",
            "Epoch 2252/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9162 - accuracy: 1.0000 - val_loss: 2.0408 - val_accuracy: 1.0000\n",
            "Epoch 2253/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9977 - val_accuracy: 1.0000\n",
            "Epoch 2254/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9272 - accuracy: 1.0000 - val_loss: 1.9169 - val_accuracy: 1.0000\n",
            "Epoch 2255/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9072 - accuracy: 1.0000 - val_loss: 1.9448 - val_accuracy: 1.0000\n",
            "Epoch 2256/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9215 - accuracy: 1.0000 - val_loss: 1.9210 - val_accuracy: 1.0000\n",
            "Epoch 2257/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 1.9028 - val_accuracy: 1.0000\n",
            "Epoch 2258/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9078 - accuracy: 1.0000 - val_loss: 2.0693 - val_accuracy: 1.0000\n",
            "Epoch 2259/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9126 - accuracy: 1.0000 - val_loss: 2.0104 - val_accuracy: 1.0000\n",
            "Epoch 2260/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 1.9217 - val_accuracy: 1.0000\n",
            "Epoch 2261/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9094 - accuracy: 1.0000 - val_loss: 1.9267 - val_accuracy: 1.0000\n",
            "Epoch 2262/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 1.9240 - val_accuracy: 1.0000\n",
            "Epoch 2263/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9122 - accuracy: 1.0000 - val_loss: 1.9068 - val_accuracy: 1.0000\n",
            "Epoch 2264/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9169 - accuracy: 1.0000 - val_loss: 2.0216 - val_accuracy: 1.0000\n",
            "Epoch 2265/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9138 - accuracy: 1.0000 - val_loss: 2.1547 - val_accuracy: 1.0000\n",
            "Epoch 2266/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9042 - accuracy: 1.0000 - val_loss: 1.9231 - val_accuracy: 1.0000\n",
            "Epoch 2267/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9150 - accuracy: 1.0000 - val_loss: 1.8835 - val_accuracy: 1.0000\n",
            "Epoch 2268/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9142 - accuracy: 1.0000 - val_loss: 1.9120 - val_accuracy: 1.0000\n",
            "Epoch 2269/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.8970 - val_accuracy: 1.0000\n",
            "Epoch 2270/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9137 - accuracy: 1.0000 - val_loss: 1.9238 - val_accuracy: 1.0000\n",
            "Epoch 2271/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9316 - accuracy: 0.9999 - val_loss: 1.9202 - val_accuracy: 1.0000\n",
            "Epoch 2272/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.8978 - accuracy: 1.0000 - val_loss: 1.9186 - val_accuracy: 1.0000\n",
            "Epoch 2273/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9160 - accuracy: 1.0000 - val_loss: 1.8915 - val_accuracy: 1.0000\n",
            "Epoch 2274/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9222 - accuracy: 1.0000 - val_loss: 1.9094 - val_accuracy: 0.9999\n",
            "Epoch 2275/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9243 - accuracy: 1.0000 - val_loss: 1.9324 - val_accuracy: 1.0000\n",
            "Epoch 2276/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9007 - accuracy: 1.0000 - val_loss: 1.8838 - val_accuracy: 1.0000\n",
            "Epoch 2277/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 1.9041 - val_accuracy: 1.0000\n",
            "Epoch 2278/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.9271 - val_accuracy: 1.0000\n",
            "Epoch 2279/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9035 - accuracy: 1.0000 - val_loss: 1.9580 - val_accuracy: 1.0000\n",
            "Epoch 2280/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9168 - accuracy: 1.0000 - val_loss: 1.8915 - val_accuracy: 1.0000\n",
            "Epoch 2281/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9437 - val_accuracy: 1.0000\n",
            "Epoch 2282/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9173 - accuracy: 1.0000 - val_loss: 1.8916 - val_accuracy: 1.0000\n",
            "Epoch 2283/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9098 - accuracy: 1.0000 - val_loss: 1.9445 - val_accuracy: 1.0000\n",
            "Epoch 2284/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9145 - val_accuracy: 1.0000\n",
            "Epoch 2285/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9056 - accuracy: 1.0000 - val_loss: 1.9670 - val_accuracy: 1.0000\n",
            "Epoch 2286/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9123 - accuracy: 1.0000 - val_loss: 1.8871 - val_accuracy: 1.0000\n",
            "Epoch 2287/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9142 - accuracy: 1.0000 - val_loss: 1.9105 - val_accuracy: 1.0000\n",
            "Epoch 2288/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9105 - accuracy: 1.0000 - val_loss: 1.9368 - val_accuracy: 1.0000\n",
            "Epoch 2289/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9065 - accuracy: 1.0000 - val_loss: 1.9358 - val_accuracy: 1.0000\n",
            "Epoch 2290/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9111 - accuracy: 1.0000 - val_loss: 2.0616 - val_accuracy: 1.0000\n",
            "Epoch 2291/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.9079 - val_accuracy: 1.0000\n",
            "Epoch 2292/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9099 - accuracy: 1.0000 - val_loss: 1.8906 - val_accuracy: 1.0000\n",
            "Epoch 2293/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9081 - accuracy: 1.0000 - val_loss: 1.8980 - val_accuracy: 1.0000\n",
            "Epoch 2294/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9149 - accuracy: 1.0000 - val_loss: 1.8992 - val_accuracy: 1.0000\n",
            "Epoch 2295/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9281 - accuracy: 1.0000 - val_loss: 1.9005 - val_accuracy: 1.0000\n",
            "Epoch 2296/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8980 - accuracy: 1.0000 - val_loss: 2.0086 - val_accuracy: 1.0000\n",
            "Epoch 2297/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9088 - accuracy: 1.0000 - val_loss: 1.9439 - val_accuracy: 1.0000\n",
            "Epoch 2298/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.9075 - val_accuracy: 1.0000\n",
            "Epoch 2299/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9089 - accuracy: 1.0000 - val_loss: 1.9883 - val_accuracy: 1.0000\n",
            "Epoch 2300/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.9316 - val_accuracy: 1.0000\n",
            "Epoch 2301/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.8851 - val_accuracy: 1.0000\n",
            "Epoch 2302/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9103 - accuracy: 1.0000 - val_loss: 1.8929 - val_accuracy: 1.0000\n",
            "Epoch 2303/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 1.8999 - val_accuracy: 1.0000\n",
            "Epoch 2304/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9217 - accuracy: 1.0000 - val_loss: 1.9465 - val_accuracy: 1.0000\n",
            "Epoch 2305/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9091 - accuracy: 1.0000 - val_loss: 1.8897 - val_accuracy: 1.0000\n",
            "Epoch 2306/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9106 - accuracy: 1.0000 - val_loss: 1.8941 - val_accuracy: 1.0000\n",
            "Epoch 2307/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.8845 - val_accuracy: 1.0000\n",
            "Epoch 2308/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9521 - val_accuracy: 1.0000\n",
            "Epoch 2309/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.9331 - val_accuracy: 1.0000\n",
            "Epoch 2310/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.9495 - val_accuracy: 1.0000\n",
            "Epoch 2311/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 2.1435 - val_accuracy: 0.9999\n",
            "Epoch 2312/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.8888 - val_accuracy: 1.0000\n",
            "Epoch 2313/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9088 - accuracy: 1.0000 - val_loss: 1.8875 - val_accuracy: 1.0000\n",
            "Epoch 2314/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9060 - accuracy: 1.0000 - val_loss: 1.8945 - val_accuracy: 1.0000\n",
            "Epoch 2315/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9200 - accuracy: 1.0000 - val_loss: 1.9023 - val_accuracy: 0.9999\n",
            "Epoch 2316/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8974 - accuracy: 1.0000 - val_loss: 1.9558 - val_accuracy: 1.0000\n",
            "Epoch 2317/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9086 - accuracy: 1.0000 - val_loss: 1.8905 - val_accuracy: 1.0000\n",
            "Epoch 2318/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9105 - accuracy: 1.0000 - val_loss: 1.9162 - val_accuracy: 1.0000\n",
            "Epoch 2319/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9091 - accuracy: 1.0000 - val_loss: 1.8793 - val_accuracy: 1.0000\n",
            "Epoch 2320/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9044 - accuracy: 1.0000 - val_loss: 1.9321 - val_accuracy: 0.9999\n",
            "Epoch 2321/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9113 - accuracy: 1.0000 - val_loss: 1.9471 - val_accuracy: 1.0000\n",
            "Epoch 2322/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9105 - accuracy: 1.0000 - val_loss: 1.9012 - val_accuracy: 1.0000\n",
            "Epoch 2323/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9185 - accuracy: 1.0000 - val_loss: 2.7475 - val_accuracy: 1.0000\n",
            "Epoch 2324/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9254 - accuracy: 1.0000 - val_loss: 1.8867 - val_accuracy: 1.0000\n",
            "Epoch 2325/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9023 - accuracy: 1.0000 - val_loss: 1.9144 - val_accuracy: 1.0000\n",
            "Epoch 2326/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9066 - accuracy: 1.0000 - val_loss: 1.9205 - val_accuracy: 1.0000\n",
            "Epoch 2327/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9057 - accuracy: 1.0000 - val_loss: 1.8937 - val_accuracy: 1.0000\n",
            "Epoch 2328/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9115 - accuracy: 1.0000 - val_loss: 1.9464 - val_accuracy: 1.0000\n",
            "Epoch 2329/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9050 - accuracy: 1.0000 - val_loss: 1.8908 - val_accuracy: 0.9999\n",
            "Epoch 2330/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9158 - accuracy: 1.0000 - val_loss: 1.8902 - val_accuracy: 1.0000\n",
            "Epoch 2331/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9089 - accuracy: 1.0000 - val_loss: 1.9068 - val_accuracy: 1.0000\n",
            "Epoch 2332/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9156 - accuracy: 1.0000 - val_loss: 1.8852 - val_accuracy: 1.0000\n",
            "Epoch 2333/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9029 - accuracy: 1.0000 - val_loss: 1.8972 - val_accuracy: 0.9999\n",
            "Epoch 2334/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9064 - accuracy: 1.0000 - val_loss: 1.9010 - val_accuracy: 1.0000\n",
            "Epoch 2335/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9070 - accuracy: 1.0000 - val_loss: 1.9495 - val_accuracy: 1.0000\n",
            "Epoch 2336/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9136 - accuracy: 1.0000 - val_loss: 1.8853 - val_accuracy: 1.0000\n",
            "Epoch 2337/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9048 - accuracy: 1.0000 - val_loss: 1.9342 - val_accuracy: 1.0000\n",
            "Epoch 2338/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9041 - accuracy: 1.0000 - val_loss: 1.9967 - val_accuracy: 1.0000\n",
            "Epoch 2339/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9102 - accuracy: 1.0000 - val_loss: 1.9605 - val_accuracy: 1.0000\n",
            "Epoch 2340/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 1.9106 - val_accuracy: 1.0000\n",
            "Epoch 2341/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9019 - accuracy: 1.0000 - val_loss: 1.8912 - val_accuracy: 1.0000\n",
            "Epoch 2342/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9191 - accuracy: 1.0000 - val_loss: 1.8825 - val_accuracy: 1.0000\n",
            "Epoch 2343/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.8854 - val_accuracy: 1.0000\n",
            "Epoch 2344/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9186 - accuracy: 1.0000 - val_loss: 1.8824 - val_accuracy: 1.0000\n",
            "Epoch 2345/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8973 - accuracy: 1.0000 - val_loss: 1.8999 - val_accuracy: 1.0000\n",
            "Epoch 2346/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.9065 - val_accuracy: 1.0000\n",
            "Epoch 2347/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9152 - accuracy: 1.0000 - val_loss: 1.8948 - val_accuracy: 1.0000\n",
            "Epoch 2348/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.8910 - val_accuracy: 1.0000\n",
            "Epoch 2349/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9076 - accuracy: 1.0000 - val_loss: 1.8959 - val_accuracy: 1.0000\n",
            "Epoch 2350/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8924 - accuracy: 1.0000 - val_loss: 1.8915 - val_accuracy: 1.0000\n",
            "Epoch 2351/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9093 - accuracy: 1.0000 - val_loss: 1.9585 - val_accuracy: 1.0000\n",
            "Epoch 2352/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.9114 - val_accuracy: 1.0000\n",
            "Epoch 2353/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9026 - accuracy: 1.0000 - val_loss: 1.9288 - val_accuracy: 1.0000\n",
            "Epoch 2354/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9071 - accuracy: 1.0000 - val_loss: 2.0613 - val_accuracy: 1.0000\n",
            "Epoch 2355/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9076 - accuracy: 1.0000 - val_loss: 1.9396 - val_accuracy: 1.0000\n",
            "Epoch 2356/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 1.9220 - val_accuracy: 1.0000\n",
            "Epoch 2357/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9019 - accuracy: 1.0000 - val_loss: 1.9364 - val_accuracy: 1.0000\n",
            "Epoch 2358/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9105 - accuracy: 1.0000 - val_loss: 1.9106 - val_accuracy: 1.0000\n",
            "Epoch 2359/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9020 - accuracy: 1.0000 - val_loss: 1.8882 - val_accuracy: 1.0000\n",
            "Epoch 2360/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9107 - accuracy: 1.0000 - val_loss: 1.9489 - val_accuracy: 1.0000\n",
            "Epoch 2361/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8997 - accuracy: 1.0000 - val_loss: 1.9017 - val_accuracy: 1.0000\n",
            "Epoch 2362/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9039 - accuracy: 1.0000 - val_loss: 2.1036 - val_accuracy: 0.9999\n",
            "Epoch 2363/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9192 - accuracy: 1.0000 - val_loss: 1.8913 - val_accuracy: 1.0000\n",
            "Epoch 2364/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9001 - accuracy: 1.0000 - val_loss: 1.9136 - val_accuracy: 1.0000\n",
            "Epoch 2365/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9108 - accuracy: 1.0000 - val_loss: 1.8945 - val_accuracy: 1.0000\n",
            "Epoch 2366/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9199 - accuracy: 1.0000 - val_loss: 1.9077 - val_accuracy: 1.0000\n",
            "Epoch 2367/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8951 - accuracy: 1.0000 - val_loss: 1.9156 - val_accuracy: 1.0000\n",
            "Epoch 2368/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9157 - accuracy: 1.0000 - val_loss: 1.9428 - val_accuracy: 0.9999\n",
            "Epoch 2369/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9039 - accuracy: 1.0000 - val_loss: 1.8913 - val_accuracy: 1.0000\n",
            "Epoch 2370/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.9012 - accuracy: 1.0000 - val_loss: 1.9073 - val_accuracy: 0.9999\n",
            "Epoch 2371/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9228 - accuracy: 1.0000 - val_loss: 1.9113 - val_accuracy: 1.0000\n",
            "Epoch 2372/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8924 - accuracy: 1.0000 - val_loss: 1.9018 - val_accuracy: 1.0000\n",
            "Epoch 2373/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9085 - accuracy: 1.0000 - val_loss: 1.8983 - val_accuracy: 1.0000\n",
            "Epoch 2374/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9015 - accuracy: 1.0000 - val_loss: 2.0513 - val_accuracy: 1.0000\n",
            "Epoch 2375/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9316 - accuracy: 1.0000 - val_loss: 1.9174 - val_accuracy: 1.0000\n",
            "Epoch 2376/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8922 - accuracy: 1.0000 - val_loss: 1.8966 - val_accuracy: 1.0000\n",
            "Epoch 2377/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9077 - accuracy: 1.0000 - val_loss: 1.9443 - val_accuracy: 0.9999\n",
            "Epoch 2378/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9086 - accuracy: 1.0000 - val_loss: 1.8825 - val_accuracy: 1.0000\n",
            "Epoch 2379/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9055 - accuracy: 1.0000 - val_loss: 1.9150 - val_accuracy: 1.0000\n",
            "Epoch 2380/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9316 - accuracy: 1.0000 - val_loss: 1.8792 - val_accuracy: 1.0000\n",
            "Epoch 2381/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8899 - accuracy: 1.0000 - val_loss: 1.8987 - val_accuracy: 1.0000\n",
            "Epoch 2382/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9030 - accuracy: 1.0000 - val_loss: 1.9171 - val_accuracy: 1.0000\n",
            "Epoch 2383/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9163 - accuracy: 1.0000 - val_loss: 1.9538 - val_accuracy: 1.0000\n",
            "Epoch 2384/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9024 - accuracy: 1.0000 - val_loss: 1.8989 - val_accuracy: 1.0000\n",
            "Epoch 2385/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.9127 - val_accuracy: 1.0000\n",
            "Epoch 2386/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9014 - accuracy: 1.0000 - val_loss: 1.8834 - val_accuracy: 1.0000\n",
            "Epoch 2387/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9119 - accuracy: 1.0000 - val_loss: 1.9146 - val_accuracy: 1.0000\n",
            "Epoch 2388/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9027 - accuracy: 1.0000 - val_loss: 1.9193 - val_accuracy: 1.0000\n",
            "Epoch 2389/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 1.9148 - val_accuracy: 0.9999\n",
            "Epoch 2390/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9063 - accuracy: 1.0000 - val_loss: 1.9309 - val_accuracy: 1.0000\n",
            "Epoch 2391/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.8923 - val_accuracy: 1.0000\n",
            "Epoch 2392/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9007 - accuracy: 1.0000 - val_loss: 1.9258 - val_accuracy: 1.0000\n",
            "Epoch 2393/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9049 - accuracy: 1.0000 - val_loss: 1.9061 - val_accuracy: 1.0000\n",
            "Epoch 2394/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 2.2717 - val_accuracy: 1.0000\n",
            "Epoch 2395/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9091 - accuracy: 1.0000 - val_loss: 1.9305 - val_accuracy: 1.0000\n",
            "Epoch 2396/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9044 - accuracy: 1.0000 - val_loss: 2.0148 - val_accuracy: 1.0000\n",
            "Epoch 2397/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9190 - accuracy: 1.0000 - val_loss: 1.9052 - val_accuracy: 1.0000\n",
            "Epoch 2398/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9036 - accuracy: 1.0000 - val_loss: 2.0834 - val_accuracy: 1.0000\n",
            "Epoch 2399/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8995 - accuracy: 1.0000 - val_loss: 1.9907 - val_accuracy: 1.0000\n",
            "Epoch 2400/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9134 - accuracy: 1.0000 - val_loss: 1.8971 - val_accuracy: 1.0000\n",
            "Epoch 2401/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9061 - accuracy: 1.0000 - val_loss: 1.9240 - val_accuracy: 0.9999\n",
            "Epoch 2402/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9233 - accuracy: 1.0000 - val_loss: 1.8883 - val_accuracy: 0.9999\n",
            "Epoch 2403/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8918 - accuracy: 1.0000 - val_loss: 1.9264 - val_accuracy: 1.0000\n",
            "Epoch 2404/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9065 - accuracy: 1.0000 - val_loss: 1.9270 - val_accuracy: 0.9999\n",
            "Epoch 2405/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9143 - accuracy: 1.0000 - val_loss: 1.8874 - val_accuracy: 0.9999\n",
            "Epoch 2406/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8930 - accuracy: 1.0000 - val_loss: 1.8815 - val_accuracy: 1.0000\n",
            "Epoch 2407/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9135 - accuracy: 1.0000 - val_loss: 1.8866 - val_accuracy: 1.0000\n",
            "Epoch 2408/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9017 - accuracy: 1.0000 - val_loss: 1.9044 - val_accuracy: 1.0000\n",
            "Epoch 2409/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9154 - accuracy: 1.0000 - val_loss: 1.9095 - val_accuracy: 1.0000\n",
            "Epoch 2410/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8936 - accuracy: 1.0000 - val_loss: 1.8935 - val_accuracy: 1.0000\n",
            "Epoch 2411/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9074 - accuracy: 1.0000 - val_loss: 1.8848 - val_accuracy: 1.0000\n",
            "Epoch 2412/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9014 - accuracy: 1.0000 - val_loss: 1.9128 - val_accuracy: 1.0000\n",
            "Epoch 2413/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9101 - accuracy: 1.0000 - val_loss: 1.9185 - val_accuracy: 1.0000\n",
            "Epoch 2414/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9010 - accuracy: 1.0000 - val_loss: 1.8879 - val_accuracy: 1.0000\n",
            "Epoch 2415/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9090 - accuracy: 1.0000 - val_loss: 2.1606 - val_accuracy: 1.0000\n",
            "Epoch 2416/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9032 - accuracy: 1.0000 - val_loss: 1.9125 - val_accuracy: 1.0000\n",
            "Epoch 2417/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 1.9163 - val_accuracy: 1.0000\n",
            "Epoch 2418/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9068 - accuracy: 1.0000 - val_loss: 1.8808 - val_accuracy: 1.0000\n",
            "Epoch 2419/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9036 - accuracy: 1.0000 - val_loss: 1.9278 - val_accuracy: 1.0000\n",
            "Epoch 2420/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9096 - accuracy: 1.0000 - val_loss: 1.9013 - val_accuracy: 1.0000\n",
            "Epoch 2421/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.8994 - accuracy: 1.0000 - val_loss: 1.8833 - val_accuracy: 1.0000\n",
            "Epoch 2422/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9069 - accuracy: 1.0000 - val_loss: 1.8904 - val_accuracy: 1.0000\n",
            "Epoch 2423/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9217 - accuracy: 1.0000 - val_loss: 2.0128 - val_accuracy: 1.0000\n",
            "Epoch 2424/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8941 - accuracy: 1.0000 - val_loss: 1.8860 - val_accuracy: 1.0000\n",
            "Epoch 2425/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9162 - accuracy: 1.0000 - val_loss: 1.9497 - val_accuracy: 1.0000\n",
            "Epoch 2426/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.8966 - accuracy: 1.0000 - val_loss: 1.9150 - val_accuracy: 1.0000\n",
            "Epoch 2427/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9118 - accuracy: 1.0000 - val_loss: 2.0657 - val_accuracy: 1.0000\n",
            "Epoch 2428/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9063 - accuracy: 1.0000 - val_loss: 2.0580 - val_accuracy: 1.0000\n",
            "Epoch 2429/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9031 - accuracy: 0.9999 - val_loss: 1.9598 - val_accuracy: 1.0000\n",
            "Epoch 2430/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9112 - accuracy: 1.0000 - val_loss: 1.9733 - val_accuracy: 1.0000\n",
            "Epoch 2431/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8977 - accuracy: 1.0000 - val_loss: 1.8838 - val_accuracy: 1.0000\n",
            "Epoch 2432/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9084 - accuracy: 1.0000 - val_loss: 1.9577 - val_accuracy: 1.0000\n",
            "Epoch 2433/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9032 - accuracy: 1.0000 - val_loss: 1.9252 - val_accuracy: 1.0000\n",
            "Epoch 2434/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9000 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 1.0000\n",
            "Epoch 2435/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9016 - accuracy: 1.0000 - val_loss: 1.9142 - val_accuracy: 1.0000\n",
            "Epoch 2436/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9115 - accuracy: 1.0000 - val_loss: 1.8909 - val_accuracy: 1.0000\n",
            "Epoch 2437/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9108 - accuracy: 1.0000 - val_loss: 1.8892 - val_accuracy: 1.0000\n",
            "Epoch 2438/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9095 - accuracy: 1.0000 - val_loss: 1.9090 - val_accuracy: 1.0000\n",
            "Epoch 2439/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9044 - accuracy: 1.0000 - val_loss: 1.8821 - val_accuracy: 0.9999\n",
            "Epoch 2440/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9047 - accuracy: 1.0000 - val_loss: 1.9381 - val_accuracy: 1.0000\n",
            "Epoch 2441/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9055 - accuracy: 1.0000 - val_loss: 1.8807 - val_accuracy: 1.0000\n",
            "Epoch 2442/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8956 - accuracy: 1.0000 - val_loss: 1.8910 - val_accuracy: 1.0000\n",
            "Epoch 2443/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9064 - accuracy: 1.0000 - val_loss: 1.9016 - val_accuracy: 1.0000\n",
            "Epoch 2444/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9100 - accuracy: 1.0000 - val_loss: 1.9856 - val_accuracy: 1.0000\n",
            "Epoch 2445/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.8990 - accuracy: 1.0000 - val_loss: 1.8789 - val_accuracy: 0.9999\n",
            "Epoch 2446/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9147 - accuracy: 1.0000 - val_loss: 1.9131 - val_accuracy: 1.0000\n",
            "Epoch 2447/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8903 - accuracy: 1.0000 - val_loss: 1.8923 - val_accuracy: 1.0000\n",
            "Epoch 2448/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9153 - accuracy: 1.0000 - val_loss: 1.9263 - val_accuracy: 0.9999\n",
            "Epoch 2449/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9101 - accuracy: 0.9999 - val_loss: 1.8816 - val_accuracy: 0.9999\n",
            "Epoch 2450/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8993 - accuracy: 1.0000 - val_loss: 1.9025 - val_accuracy: 0.9999\n",
            "Epoch 2451/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9050 - accuracy: 1.0000 - val_loss: 1.9001 - val_accuracy: 1.0000\n",
            "Epoch 2452/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9023 - accuracy: 1.0000 - val_loss: 1.8976 - val_accuracy: 1.0000\n",
            "Epoch 2453/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9003 - accuracy: 1.0000 - val_loss: 1.8951 - val_accuracy: 1.0000\n",
            "Epoch 2454/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9131 - accuracy: 1.0000 - val_loss: 1.9360 - val_accuracy: 1.0000\n",
            "Epoch 2455/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9053 - accuracy: 0.9999 - val_loss: 1.8867 - val_accuracy: 1.0000\n",
            "Epoch 2456/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9049 - accuracy: 1.0000 - val_loss: 2.0445 - val_accuracy: 1.0000\n",
            "Epoch 2457/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8958 - accuracy: 1.0000 - val_loss: 1.8992 - val_accuracy: 1.0000\n",
            "Epoch 2458/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9064 - accuracy: 1.0000 - val_loss: 1.9656 - val_accuracy: 1.0000\n",
            "Epoch 2459/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9184 - accuracy: 1.0000 - val_loss: 1.8905 - val_accuracy: 1.0000\n",
            "Epoch 2460/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8967 - accuracy: 1.0000 - val_loss: 1.8946 - val_accuracy: 1.0000\n",
            "Epoch 2461/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9014 - accuracy: 1.0000 - val_loss: 1.9669 - val_accuracy: 1.0000\n",
            "Epoch 2462/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9177 - accuracy: 1.0000 - val_loss: 2.0321 - val_accuracy: 1.0000\n",
            "Epoch 2463/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8961 - accuracy: 1.0000 - val_loss: 1.9016 - val_accuracy: 1.0000\n",
            "Epoch 2464/2500\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.9066 - accuracy: 1.0000 - val_loss: 1.9579 - val_accuracy: 1.0000\n",
            "Epoch 2465/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9039 - accuracy: 1.0000 - val_loss: 1.8982 - val_accuracy: 1.0000\n",
            "Epoch 2466/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9084 - accuracy: 1.0000 - val_loss: 1.9323 - val_accuracy: 1.0000\n",
            "Epoch 2467/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9042 - accuracy: 1.0000 - val_loss: 1.8805 - val_accuracy: 1.0000\n",
            "Epoch 2468/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8998 - accuracy: 1.0000 - val_loss: 1.9342 - val_accuracy: 0.9999\n",
            "Epoch 2469/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9048 - accuracy: 0.9999 - val_loss: 1.8894 - val_accuracy: 1.0000\n",
            "Epoch 2470/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9041 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 1.0000\n",
            "Epoch 2471/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9074 - accuracy: 1.0000 - val_loss: 1.9328 - val_accuracy: 1.0000\n",
            "Epoch 2472/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9033 - accuracy: 1.0000 - val_loss: 1.9267 - val_accuracy: 1.0000\n",
            "Epoch 2473/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9120 - accuracy: 1.0000 - val_loss: 1.8896 - val_accuracy: 1.0000\n",
            "Epoch 2474/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8954 - accuracy: 1.0000 - val_loss: 1.9033 - val_accuracy: 1.0000\n",
            "Epoch 2475/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9020 - accuracy: 1.0000 - val_loss: 1.8800 - val_accuracy: 1.0000\n",
            "Epoch 2476/2500\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 1.8982 - accuracy: 1.0000 - val_loss: 1.9110 - val_accuracy: 1.0000\n",
            "Epoch 2477/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9144 - accuracy: 1.0000 - val_loss: 1.8818 - val_accuracy: 1.0000\n",
            "Epoch 2478/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8978 - accuracy: 1.0000 - val_loss: 1.9310 - val_accuracy: 1.0000\n",
            "Epoch 2479/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.8971 - accuracy: 1.0000 - val_loss: 1.8916 - val_accuracy: 1.0000\n",
            "Epoch 2480/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9059 - accuracy: 1.0000 - val_loss: 1.9240 - val_accuracy: 0.9999\n",
            "Epoch 2481/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9051 - accuracy: 1.0000 - val_loss: 1.8869 - val_accuracy: 1.0000\n",
            "Epoch 2482/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9049 - accuracy: 1.0000 - val_loss: 1.9303 - val_accuracy: 1.0000\n",
            "Epoch 2483/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9029 - accuracy: 1.0000 - val_loss: 2.0657 - val_accuracy: 1.0000\n",
            "Epoch 2484/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.9006 - accuracy: 1.0000 - val_loss: 1.8757 - val_accuracy: 1.0000\n",
            "Epoch 2485/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8983 - accuracy: 1.0000 - val_loss: 1.9124 - val_accuracy: 0.9999\n",
            "Epoch 2486/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9206 - accuracy: 1.0000 - val_loss: 1.9336 - val_accuracy: 1.0000\n",
            "Epoch 2487/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8873 - accuracy: 1.0000 - val_loss: 1.8864 - val_accuracy: 1.0000\n",
            "Epoch 2488/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8980 - accuracy: 1.0000 - val_loss: 1.8950 - val_accuracy: 1.0000\n",
            "Epoch 2489/2500\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 1.9117 - accuracy: 1.0000 - val_loss: 1.8919 - val_accuracy: 1.0000\n",
            "Epoch 2490/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9089 - accuracy: 1.0000 - val_loss: 1.8956 - val_accuracy: 0.9999\n",
            "Epoch 2491/2500\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 1.9046 - accuracy: 1.0000 - val_loss: 1.8983 - val_accuracy: 1.0000\n",
            "Epoch 2492/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9007 - accuracy: 1.0000 - val_loss: 2.0116 - val_accuracy: 1.0000\n",
            "Epoch 2493/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9008 - accuracy: 1.0000 - val_loss: 1.8889 - val_accuracy: 1.0000\n",
            "Epoch 2494/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9067 - accuracy: 1.0000 - val_loss: 1.8780 - val_accuracy: 1.0000\n",
            "Epoch 2495/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8943 - accuracy: 1.0000 - val_loss: 1.9881 - val_accuracy: 0.9999\n",
            "Epoch 2496/2500\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 1.9151 - accuracy: 0.9999 - val_loss: 1.8917 - val_accuracy: 0.9999\n",
            "Epoch 2497/2500\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 1.8896 - accuracy: 0.9999 - val_loss: 1.8973 - val_accuracy: 1.0000\n",
            "Epoch 2498/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9033 - accuracy: 1.0000 - val_loss: 1.9544 - val_accuracy: 1.0000\n",
            "Epoch 2499/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.9125 - accuracy: 1.0000 - val_loss: 1.8874 - val_accuracy: 1.0000\n",
            "Epoch 2500/2500\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 1.8929 - accuracy: 1.0000 - val_loss: 1.8870 - val_accuracy: 1.0000\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "n_inputs=13\n",
        "Mod(X_train,X_test,y_train,y_test,n_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PKbSbh_fpuT"
      },
      "source": [
        "### **Super Learner Classifier Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "IUnmzHD_Qnzp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "UF_wtQ0cb_ak"
      },
      "outputs": [],
      "source": [
        "models = list()\n",
        "meta = RandomForestClassifier()\n",
        "models.append(('LogR',LogisticRegression(solver='liblinear')))\n",
        "# models.append(('KNN',KNeighborsClassifier()))\n",
        "models.append(('PC',Perceptron()))\n",
        "models.append(('RIC',RidgeClassifier()))\n",
        "models.append(('PAC',PassiveAggressiveClassifier()))\n",
        "models.append(('BC',BaggingClassifier(meta,n_estimators=150)))\n",
        "models.append(('RC',RandomForestClassifier(n_estimators=150)))\n",
        "models.append(('EXTC',ExtraTreesClassifier(n_estimators=150)))\n",
        "models.append(('XGBC',XGBClassifier()))\n",
        "models.append(('MLPC',MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=250)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "3MoUVvPKcEa3"
      },
      "outputs": [],
      "source": [
        "# meta model\n",
        "meta = XGBClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9WF1WwNQv0V",
        "outputId": "634f77c9-c192-4141-d1a3-05e2c050b716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Train (8011, 13) (8011, 1) Test (2671, 13) (2671, 1)\n",
            "(8011, 7)\n",
            "(2671, 7)\n",
            "Model saved succesfully!!!\n"
          ]
        }
      ],
      "source": [
        "model =  StackingClassifier(estimators=models, final_estimator=meta, cv=25)\n",
        "encoder = load_model('STAE2-4-encoder.h5')\n",
        "print('Train', X_train.shape, y_train.shape, 'Test', X_test.shape, y_test.shape)\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "print(X_train_encode.shape)\n",
        "print(X_test_encode.shape)\n",
        "model.fit(X_train_encode,y_train)\n",
        "filename = 'SLMC.h5'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "print(\"Model saved succesfully!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'SLMC.h5'\n",
        "encoder = load_model('STAE2-4-encoder.h5')\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "X_val_encode = encoder.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4EVh0SS3mVu",
        "outputId": "9e3f206b-ebef-45b3-8ef4-222364e4413c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "print(\"Loaded Model Sucessfully\")"
      ],
      "metadata": {
        "id": "KJv6XO173_0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = loaded_model.predict(X_test_encode)\n",
        "print('Classifier Learner: %.3f' % (accuracy_score(y_test, yhat) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cPfOn8Q2cLJ",
        "outputId": "a8c694df-a490-4f6d-a0ef-03ab6d311a32"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Learner: 94.908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "hFi7aBO3RJHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae0dbfc-cb89-4631-b15a-7e680f284f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Model Sucessfully\n"
          ]
        }
      ],
      "source": [
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "print(\"Loaded Model Sucessfully\")\n",
        "yhat1 = loaded_model.predict(X_val_encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "U3kVI6oRROZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920ca253-a3df-4efc-e5ea-438a8c78c68b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4],\n",
              "       [3],\n",
              "       [4],\n",
              "       ...,\n",
              "       [4],\n",
              "       [1],\n",
              "       [6]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "6kSDNDdXRZgN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b2fb27-165b-4115-f79a-8b32b314ad16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4,  1,  4, ...,  6, 10,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "yhat1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryyY3GhFfutu"
      },
      "source": [
        "### **Model Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Q_YGzF0IRctt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "7YI9OaFzRalQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ba643d-e0ed-449b-aa88-3529899fd342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Score Metrics:\n",
            "\n",
            " Accuracy score: 95\n",
            "\n",
            " Balanced Accuracy score 75\n",
            "\n",
            " Cohen Kappa score 93\n",
            "\n",
            " F1 Score (Macro): 76\n",
            "\n",
            " F1 Score (Micro): 95\n",
            "\n",
            " F1 Score (Weighted): 95\n",
            "\n",
            " Jaccard Score (Macro): 73\n",
            "\n",
            " Jaccard Score (Micro): 90\n",
            "\n",
            " Jaccard Score (Weighted): 90\n",
            "\n",
            " Loss Metrics:\n",
            "\n",
            " Hamming Loss : 0.05091725945338824\n",
            "\n",
            " Confusion matrix\n",
            "\n",
            "\n",
            "[[ 89   0   0   1   0   0   0   0   0   0]\n",
            " [  0 401   0   3  31   0   1   0   2   1]\n",
            " [  0   0  39   1   1   0   1   0   0   0]\n",
            " [  1   3   0 459   3   0  10   0   3   1]\n",
            " [  0   7   0   7 977   0   4   0   1   2]\n",
            " [  0   0   0   0   1   0   0   0   0   0]\n",
            " [  0  11   0   8  10   0 274   0   0   0]\n",
            " [  0   0   0   0   0   0   3   0   0   0]\n",
            " [  0   0   0   4   4   0   0   0 193   0]\n",
            " [  0   1   0   0   9   0   1   0   0 103]]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Score Metrics:\")\n",
        "print('\\n Accuracy score:', round(accuracy_score(y_test, yhat) * 100))\n",
        "print('\\n Balanced Accuracy score' ,round(balanced_accuracy_score(y_test, yhat) * 100))\n",
        "print('\\n Cohen Kappa score' ,round(cohen_kappa_score(y_test, yhat) * 100))\n",
        "print('\\n F1 Score (Macro):' ,round(f1_score(y_test, yhat,average='macro') * 100))\n",
        "print('\\n F1 Score (Micro):' ,round(f1_score(y_test, yhat,average='micro') * 100))\n",
        "print('\\n F1 Score (Weighted):' ,round(f1_score(y_test, yhat,average='weighted') * 100))\n",
        "print('\\n Jaccard Score (Macro):' ,round(jaccard_score(y_test, yhat,average='macro') * 100))\n",
        "print('\\n Jaccard Score (Micro):' ,round(jaccard_score(y_test, yhat,average='micro') * 100))\n",
        "print('\\n Jaccard Score (Weighted):' ,round(jaccard_score(y_test, yhat,average='weighted') * 100))\n",
        "print(\"\\n Loss Metrics:\")\n",
        "print('\\n Hamming Loss :' ,hamming_loss(y_test, yhat))\n",
        "print(\"\\n Confusion matrix\")\n",
        "print('\\n')\n",
        "print(confusion_matrix(y_test,yhat))\n",
        "print('\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FLP&AP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}